\documentclass[11pt]{scrartcl}
\usepackage[sexy]{evan}
\usepackage{graphicx}

 %Sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb C}

%From Topology
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cU}{\mathcal{U}}
\renewcommand{\P}{\mathbb{P}}


\usepackage{answers}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\declaretheorem[style=thmbluebox,name={Theorem}]{thm}


\begin{document}
\title{Math 218a}
\author{Vishal Raman}
\thispagestyle{empty}
$ $
\vfill
\begin{center}

\centerline{\huge \textbf{Math 218a Lecture Notes, Fall 2020}}
\centerline{\Large \textbf{Probability Theory} } 
\centerline{Professor: Shirshendu Ganguly}
\centerline{Vishal Raman}
\end{center}
\vfill
$ $
\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
%\maketitle
\section{August 27th, 2020}
\subsection{Introduction}
Consider a \textbf{random experiment} - this involves a state space $\Omega$ and some "probability" on it.  The outcome of an experiment would be $\omega \in \Omega$.

\begin{example}[Fair Coin Toss] $\Omega = \{0, 1\}, P(0) = 1/2, P(1) = 1/2$ models a fair coin toss.  The outcomes are $\omega \in \Omega, \omega = 0$ or $\omega = 1$.
\end{example}
\begin{example}[Continuous State Space] $\Omega = [0, 1]$, $X$ is the outcome of a random experiment.  Suppose $X$ is uniformly distributed random variable.  $P(X \in [0, \frac{1}{2}]) = 1/2$.  Take $A = \Q \cap [0, 1]$.  $P(x \in A) = 0$, since $A$ has no "volume".  Similarly, taking $A_1 = \R\setminus\Q \cap [0, 1]$, then $P(x \in A_1) = 1 - P(x \in A) = 1$.  Finally, take $E \subset [0, 1]$.  $P(x \in E) = $"volume" of $E$.   
\end{example}
The issue: we need to define some notion of volume.  Some properties we would like are the following:
\begin{itemize}
\item Translation Invariance
\item Countable Additivity: $A_1, A_2, \dots$ disjoint with $A = \bigcup A_i$, then $P(A) = \sum_{i=1}^{\infty}P(A_i)$.
\end{itemize}
\subsection{Nonmeasurable Sets}
Take $I = [-1, 2]$, and define $x \sim y$ iff $x-y \in \Q$. [Exercise: check that $\sim$ is an equivalence relation.]  This decomposes $I$ into equivalence classes $I/\sim$.  Note that the equivalence classes are countable, since any class is $x + A, A \subset Q$.  

For each equivalence class $B$, pick $x_B \in B\cap [0, 1]$.  Define $E = \{x_B \}$ over all the equivalence classes.  Note that $x_B$ is a representative of $B$ in $E$, so $B = \{x_b + q : x_b + q \in I, q \in \Q \}.$  

Now, consider the set $[0, 1] \subset \bigcup_{q \in [-1, 1]}E + q \subset [-1, 2]$.  Equality doesn't hold, because there can be $B$ s. t. $x_b$ is close to $0$.  Then $E + (\Q \cap [-1, 1])$ will only recover elements of $B$ near $1$ and will not go up to $2$.

\begin{proposition} We claim that $E+q$ are disjoint for different values of $q$.
\end{proposition}
\begin{proof}
Suppose $E+q_1 \cap E+q_2 \ne \emptyset$ for some $q_1, q_2$.  Then, there exists $x, y \in E$ such that $x+q_1 = y + q_2$.  This implies that $x-y = q_2 - q_1 \in \Q$, so $x \sim y$, but by definition, there is exactly one member of each equivalence class in $E$.  
\end{proof}

The big question: What is $P(E)$?  Suppose $P(E) > 0$.  Then $\bigcup_{q \in [-1, 1]} E + Q \subset [-1, 2]$ and $P(E + q_1) = P(E + q_2) = P(E)$ for all $q_1, q_2$.  Furthermore, by countable additivity,
$$1 \ge P(\bigcup_{q \in [-1, 1] E + q}) = \sum_{q \in [-1, 1]} P(E + q) = \infty \cdot P(E).$$
This would imply that $P(E) = 0$.  However, $$[0, 1] \subseteq \bigcup_{q \in [-1, 1]}E + q \Rightarrow P([0, 1]) = 1/3 \le \sum_{q \in[-1, 1]} P(E+q) = 0.$$  Hence, $P(E)$ cannot be defined.

The issue is the step where we pick $x_B$, since we need to pick $x_B$ from uncountably many points, which assumes the axiom of choice.  It was proved by Robert M. Solovay that all models of set theory excluding the axiom of choice have the property that all sets are Lebesgue measurable.

Our goal is thus to come up with a general framework where things can be consistently defined for a large class of sets.  
\subsection{Measure Theory Beginnings}  For the definitions, we take $\Omega$ to be the state space.
\begin{definition}[Sigma-Algebra]  Suppose $\Sigma$ follows the following properties:
\begin{enumerate}
\item $\emptyset \in \Sigma$
\item $A \in \Sigma \Rightarrow A^c \in \Sigma$
\item $A_1, A_2, \dots \in \Sigma$, then $\bigcup A_i \in \Sigma$
\end{enumerate}
Note that $2$ and $3$ imply $1$ since $(A \cup A^c)^c = \emptyset$.  Then $\Sigma$ is a sigma-algebra.
\end{definition}
Note that we also have countable intersections(this is an easy exercise).

\newpage
\section{September 1st, 2020}
Last time:
\begin{itemize}
 \item We discussed the notation of a $\Sigma$-algebra, a reasonable class of sets on which we will define measures.
 \item Properties: $\emptyset \in \SA, A \in \SA, A^c \in \SA, \bigcup A_i \in \SA$.
 \end{itemize} 
\subsection{Measures}
We are working in a space $(\Omega, \Sigma)$.  
\begin{definition}[Measure] A measure is a function $\mu: \Sigma \rightarrow [0, \infty]$ with the following properties:
\begin{itemize}
\item $\mu(\emptyset) = 0$
\item "Countable Additivity": $\mu(\bigcup A)i) = \sum \mu(A_i)$ for disjoint $A_i \in \Sigma$.
\end{itemize}
\end{definition}
\begin{example} If $\Omega$ is finite, ${1, 2, \dots, n}$, $\Sigma = 2^{\Omega}$, then all possible measures on $(\Omega, \Sigma)$ are given by fixing $a_1, a_2, \dots, a_n \in [0, \infty]$ and $\mu(A) = \sum_{i \in A} a_i$.
\end{example}
Properties of measures:
\begin{itemize}
\item Monotonicity: $A \subset B$, then $\mu(A) \le \mu(B)$. 
\begin{proof}
$B = A \cup (B\setminus A)$ and $B\setminus A \in \Sigma$, so $$\mu(B) = \mu(A) + \mu(B\setminus A) \ge \mu(A).$$
\end{proof}
\item Countable Subadditivity:  $A \subseteq \bigcup_{i=1}^\infty B_i$, then $\mu(A) \le \sum \mu(B_i)$.
\begin{proof}
We disjointify the $B_i$:  Define $C_1 = B_1, C_i = B_i \setminus B_{i-1}$.  Then
$$\mu(A) \le \mu(\bigcup C_i) = \sum \mu(C_i) \le \sum \mu(B_i).$$
\end{proof}
\item: Continuity from below:  If $A_i \uparrow A$, then $\mu(A_i) \uparrow \mu(A)$.
\begin{proof}
$A = A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus A_2) \dots$, so by countable additivity
$$\mu(A) = \sum_{i=1}^{\infty} \mu(C_i) = \lim_{n\rightarrow \infty} \sum_{i=1}^n \mu(C_i) = \lim_{n \rightarrow \infty} \mu(A_n).$$
\end{proof}
\item Continuity from above, if $A_i \downarrow A$,  and $\mu(A_1) < \infty$, then $\mu(A_i) \rightarrow \mu(A)$ 
\begin{proof}
We need the condition $\mu(A_i) < \infty$.  Take $A_i = [i, \infty)$ as a counterexample if we don't have that condition.

Define $A_1 \setminus A_i = B_i$, so $B_i \uparrow A_1 \setminus A$.  Then, use the continuity from below.  
\end{proof}
\end{itemize}
\subsection{Sigma algebras}
\begin{fact} For any $A \subset 2^{\Omega}$, define $$\Sigma(A) = \bigcap_{A \in \Sigma}\Sigma.$$ Then, $\Sigma(A)$ is a sigma-algebra.
\end{fact}
Note that $\Sigma(A)$ is the smallest sigma-algebra containing $A$.  For this reason, we call it the sigma-algebra \textbf{generated} by $A$.

\begin{example} Take $X, Y \subset 2^{\Omega}$.  We want to prove $\Sigma(X) = \Sigma(Y)$.  It suffices to show $X \subseteq \Sigma(Y)$ and $Y \subseteq \Sigma(X)$.  
\end{example}

\begin{definition}[Borel Sigma-Algebra] $(\Omega, \cU)$, a topological space with a family of open sets.  The \textbf{Borel Sigma-Algebra} is $\SB = \Sigma(\cU)$. 
\end{definition}

\begin{example} For $\Omega = \R$, $\SB$ is the sigma algebra generated by open sets in $\R$.  We also have $\SB$ is the sigma-algebra generated by open intervals in $\R$, which follows from the fact that any open set can be written as a countable union of open intervals.  Furthermore,
$$\Sigma((a, b): a, b\in \Q, \R) = \Sigma([a, b]: a, b \in \Q, \R),$$
since $[a, b] = \bigcap (a-1/n, b+1/n)$ and $(a, b) = \bigcup [a+1/n, b-1/n]$.
\end{example}

\subsection{Uniform Measure on the Borel Sets}
We will attempt to define the uniform measure on Borel sets of $\R$.  Broadly, we do it as follows:
\begin{enumerate}
\item Define it on a semi-algebra containing the intervals.
\item Extend the definition to an algebra.
\item Extend it to a sigma-algebra.
\end{enumerate}
\begin{definition}[Semi-algebra] $\Sigma \subset 2^\Omega$ is a semi-algebra if
\begin{itemize}
\item $A_1, A_2 \in \Sigma$ implies $A_1\cap A_2 \in \Sigma$
\item $A_1 \in \Sigma$ implies that $A_1^c = \bigcup_{i=1}^n B_i$ for $B_i \in \Sigma$.
\end{itemize}
\end{definition}
Note:  The set of intervals $\{(a, b): a, b \in \R\}$ is not a semi-algebra.  If $(a, b)^c = [b, \infty)$ which is not finitely coverable by disjoint open sets. Similarly, $\{[a, b]: a, b \in \R\}$ is not a semi-algebra.  

Claim: $\Sigma = \{(a, b]: a, b \in \R\}$ is a semi-algebra.  [This is left as an exercise].

Now, $\mu((a, b]) = b-a$. The proof that $\mu$ is countable additive on $\Sigma$.  If $A = \bigcup_{i=1}^{\infty} B_i$, $B_i$ disjoint, $A, B_i \in \Sigma$, then $\mu(A) = \sum_{i=1}^{\infty} \mu(B_i).$
\begin{proof}
We first show that $\mu(A) \ge \sum_{i=1}^{\infty} \mu(B_i)$.  This is an easy exercise, show $\mu(A) \ge \sum_{i=1}^{n}\mu(B_i)$, and we pass to the limit.

It suffices to show $\mu(A) \le \sum_{i=1}^{\infty} \mu(B_i)$.  We do this by exploiting compactness.

Let $A = (a, b] \supset [a+1/n, b] = A;$, take $B_i = (c_i, d] \subset c_i, d + \frac{\epsilon}{2^i} = B_i'$.  Note that $$A' \subset \bigcup_{i=1}^{\infty}B_i',$$
so there exists a finite subcover $A' \subset \bigcup_{j=1}^{k} B_{i_j}'.$  It is easy to show that $b - (a+1/n) \le \sum_{j=1}^{k} (d_{i_j}' - c_{i_j}')$. But note that 
$$\sum_{j=1}^{k} (d_{i_j}' - c_{i_j}') \le \sum_{j=1}^k d_{i_j} - c_{i_j} + \epsilon,$$
which implies that $$\mu(A) - 1/n \le \sum_{i=1}^{\infty}\mu(B_i) + \epsilon \Rightarrow \mu(A) \le \sum_{i=1}^{\infty} \mu(B_i).$$ 
\end{proof}
\begin{definition} $\SA$ is an algebra if 
\begin{itemize}
\item $\emptyset \in \SA$
\item $A_1 \in \SA$ implies $A^c \in \SA$
\item $A_1, \dots, A_n \in \SA$, then $\bigcup_{i=1}^n A_i \in A$.
\end{itemize}
The algebra generated by a semi-algebra is given by taking all possible disjoint finite unions.
\end{definition}
Claim: $\Sigma_a = \{\bigcup_{i=1}^n A_i\}$ for disjoint $A_i$ semialgebras is an algebra.
\begin{proof}
We show $A, B \in \Sigma_a \Rightarrow A\cup B \in \Sigma_a$ and $A^c \in \Sigma_a$.  
Note that $A = \bigcup_{i=1}^n C_i, B = \bigcup_{j=1}^k D_j$, so $$A \cap B = \bigcup_{i=1}^n \bigcup_{j=1}^k C_i \cap D_j,$$
and $C_i \cap D_j $ are disjoint.  Then $C_i, D_j \in \Sigma$ implies $C_i \cap D_j \in \Sigma$.

Then, if $A = \bigcup_{i=1}^k C_i$, then $A^c = \bigcap_{i=1}^k C_i^c$, and $C_i^c = \bigcup_{j=1}^\ell E_j \in \Sigma_a$.
\end{proof}

We extend $\mu$ to an algebra by $\mu(A) = \sum_{i=1}^k \mu(C_i)$, where $A = \bigcup C_i$ in the semi-algebra.
\pagebreak
\section{September 3rd, 2020}
Recall that we are aiming to define the uniform measure on $(\R, \SB)$.  

Last time:
\begin{enumerate}
\item We defined a \textbf{premeasure} on a semi-algebra, which $\Sigma_{semi} = \{(a, b]: -\infty \le a \le b \le \infty\}$, $\mu((a, b]) = b-a$ was countably additive.  
\item Extend $\mu$ to an algebra $\Sigma_a = $ disjoint union of elements of $\Sigma_{semi}$.  
\item For $A = \bigcup_{i=1}^k C_i \in \Sigma_a$, $$\mu(A) = \sum_{i=1}^k \mu(C_i).$$
\end{enumerate}
\subsection{Uniform Measure on the Borel Sets}

We first need show show $\mu$ is well defined.  Suppose $A = \bigcup_{i=1}^l C_i, \bigcup_{j=1}^\ell B_j$ for $C_i, B_i \in \Sigma_{semi}$.  We want
$$\mu(A) = \sum_{i=1}^k \mu(C_i) = \sum_{j=1}^{\ell} \mu(B_j).$$

Note that $C_i = \bigcup_{j=1}^{\ell} (C_i \cap B_j)$, which are all disjoint.  Similarly, $B_j = \bigcup_{i=1}^k (B_j \cap C_i)$, disjoint.  Thus, from the finite additivity of $\Sigma_{semi}$, we have
$$\sum_{i=1}^k \mu(C_i) = \sum_{i=1}^k \sum_{j=1}^\ell \mu(C_i \cap B_k) = \sum_{j=1}^\ell \mu(B_j),$$
as desired.

We will next show that $\mu$ is finitely additive additive.  First, if we have $A_1, A_2, \dots, A_n \in \Sigma_a$ disjoint, we show $\mu\left (\bigcup A_i\right ) = \sum \mu(A_i)$.

Note that each $A_i = \bigcup_{j=1}^{m_i} C_j^i$, which are disjoint, so 
$$\mu\left (\bigcup A_i\right ) = \mu \left ( \bigcup_i \bigcup_j C_j^i\right ) = \sum_{i=1}^n \sum_{j=1}^{m_i} \mu(C_j^i) = \sum_{i=1}^n \mu(A_i).$$

Next, we show $\mu$ is monotonic.  For $A, B \in \Sigma_a, A \subseteq B$, $B = A \cup (B \setminus A)$, so $$\mu(B) = \mu(A) + \mu(B \setminus A) \ge \mu(A).$$

We show countably additivity: Let $A = \bigcup_{i=1}^{\infty} \mu(A_i)$.  We need to show $\mu(A) = \sum_{i=1}^{\infty} \mu(A_i)$.  

We first show $\mu(A) \ge \sum_{i=1}^{\infty} \mu(A_i)$.  It suffices to show $\mu(A) \ge \sum_{i=1}^n \mu(A_i)$.  Since $\bigcup_{i=1}^n A_i \subseteq A$, monotonicity gives $\mu\left (\bigcup_{i=1}^n A_i\right ) \le \mu(A)$.

Next, we show $\mu(A) \le \sum_{i=1}^{\infty} \mu(A_i)$.  
First $A = \bigcup_{j=1}^k C_j$ for $C_j\in \Sigma_{semi}$, 
$A_i = \bigcup_{\ell}^{m_i} C_{\ell}^i$ for $C_{\ell}^j \in \Sigma_{semi}$.

Thus, $$\mu(A) = \sum_{j=1}^k \mu(C_j).$$
Hence, it suffices to show $\mu(C_j) \le \sum_{i=1}^{\infty} \mu(C_j \cap A_i),$ since 
$$\mu(A) = \sum \mu(C_j) \le \sum_{j=1}^k \sum_{i=1}^{\infty} \mu(C_j \cap A_i) = \sum_{i=1}^{\infty}\sum_{j=1}^k \mu(C_j \cap A_i).$$

Note that $C_j = \bigcup_{i=1}^\infty \bigcup_{\ell}^{m_i}C_j \cap C_\ell^i$, and we finish by using countable additivity for $\Sigma_{semi}$.

It suffices extend to $\Sigma(\Sigma_a)$ which is the sigma-algebra generated by $\Sigma_a$.  

\begin{thm}[Caratheodory's Extension Theorem] We have the following:
\begin{itemize}
\item Given a countably additive measure $\mu$ on an algebra $\Sigma_a$, it can be extended to a measure on $\Sigma(\Sigma_a)$.
\item If $\mu$ is $\sigma$-finite on $\Sigma_a$, the extension is unique.
\end{itemize}
A measure $\mu$ is $\sigma$-finite on $\Sigma_a$ if there exists $A_1 \subseteq A_2 \subseteq \dots \in \Sigma_a$ so that $\bigcup A_i = \Omega$, $\mu(A_i) \le \infty$ for all $i$.
\end{thm}
\begin{proof}
For example, consider $\Sigma_{semi} = \{(a, b] \cap \Q\}$.  Then $\Sigma = 2^\Q$.  The cardinality of every element in $\Sigma_{semi}$ is either $\infty$ or $0$.  Define $\mu(A) = \infty$ if $|A| = \infty$, else $0$.  One can check $\mu$ is a measure on $\Sigma_{semi}$.  We can also take the counting measure $\nu$.  This agrees on $\Sigma_{semi}$, but not on $\Sigma$.  We can check that $\nu$ is not sigma-finite.

We now show uniqueness, given $\sigma$-finiteness.  For simplicity, assume $\mu(\Omega) < \infty$.   If we have two measures $\mu_1, \mu_2$ on $\Sigma$ with $\mu_1(A) = \mu_2(A)$ for all $A \in \Sigma_a$, then we show $\mu_1(B) = \mu_2(B)$ for all $B\in \Sigma$.

A general strategy to show some property is true for a sigma-algebra is to show that the sets satisfying those properties must be closed under some natural operations and that any such family of sets must contain a sigma-algebra.
\begin{thm}[$\pi$ - $\lambda$] A class of sets $P$ is said to be a $\pi$-system if $A, B \in P$ implies $A \cap B \in P$.  A class of sets $G$ is said to be a $\lambda$-system if $\Omega \in G$, $A \subset B, A, B \in G$, then $B \setminus A \in G$, and $A_i \in G$ , $A_i \uparrow A \rightarrow A \in G$.  If $P$ is a $\pi$ system contained in $G$, a $\lambda$-system, then $\Sigma(P) \subset G$.
\end{thm}

Note that a semi-algebra is a $\pi$ system.  It suffices to consider the set $\mathscr{C} = \{A : \mu_1(A) = \mu_2(A)\}.$  We know that $\Sigma_{semi} \subset \mathscr{C}$.  To show $\Sigma \subset \mathscr{C}$, it suffices to show that $\mathscr{C}$ is a $\lambda$-system.  

Note that a sigma-algebra is a $\lambda$-system, so given any $\pi$-system $P$, $\Sigma(P)$ is the smallest $\lambda$-system containing $P$.

We have already verified $\Sigma_a \subset \mathscr{C}$. Furthermore, $\Omega \in \mathscr{C}$ because $\Omega$ is an algebra.  Finally, suppose we have $A \subset B$ , $A, B \in \mathscr{C}$.  We need $B \setminus A \in \mathscr{C}$.
$\mu_1(A) = \mu_2(A), \mu_1(B) = \mu_2(B)$ and $\mu(\Omega) < \infty$.  Since $\mu_1(\Omega) = \mu(\Omega) = \mu_2(\Omega) < \infty$,
$$\mu_1(B \setminus A) = \mu_1(B) - \mu_1(A) = \mu_2(B) - \mu_2(A) = \mu_2(B\setminus A).$$

For $A_i \uparrow A$, by continuity from below, $\mu_1(A_i) \rightarrow \mu_1(A), \mu_2(A_i) \rightarrow \mu_2(A)$, so $A \in \mathscr C$.
\end{proof}
An easy exercise is to modify the above prove to include the sigma-finite case.  The proof of the $\pi-\lambda$ theorem involves some set theory.  

We'll sketch the existence proof.  Suppose we have $\mu$ on $\Sigma_a$.  For example, take $B \subset \R$.  How do we define $\mu(B)$?  We could try to approximate $B$ by the union of intervals.

Define the outer measure, $\mu_*(B) = \inf \sum_{i=1}^{\infty}\mu(A_i)$ defined over covers of $B$.  Some properties are $\mu_*(B_1) \le \mu_*(B_2)$ if $B_1 \subseteq B_2$, $\mu_*(\emptyset) = 0$, and $\mu_*(\bigcup C_i) \le \sum_{i=1}^{\infty} \mu_*(C_i)$.

Define $\SA = \{A : \mu_*(E) = \mu_*(E \cap A) + \mu_*(E \cap A^C) \forall E\}.$  $\SA$ is a sigma algebra containing $\Sigma_a$ and $\mu_*$ is a measure when restricted to $\SA$.  
\pagebreak
\section{September 8th, 2020}
Last time, we completed the construction of the uniform measure on the Borel sets.  
\subsection{The Outer Measure}
On an algebra $\Sigma_a$,  let $\Sigma_a^\sigma$ be the elements formed by taking countable unions of elements of $\Sigma_a$.  Let $\Sigma_a^{\sigma \delta}$ contain countable intersections of elements of $\Sigma_a^\sigma$.  Notice that from the definition of an outer measure, for any set $B$, there exists a set $B' \in \Sigma_a^{\sigma \delta}$ such that $$B \subset B', \mu_*(B) = \mu_*(B').$$

This implies that $\mu_*(B' \setminus B) = 0$ and for every $N$ such that $\mu_*(N) = 0$, we can check that $N$ belongs to $\SA$.  Remark:  The construction defines the measure $\mu_*$ on sets of the form 
$A \cup B$, where $A$ is a Borel set and $\mu_*(B) = 0$.  It is easy to check that $\mu_*(B) = 0$ implies that there exists a Borel set $C$ such that $\mu_*(C) = 0$ and $B \subseteq C$.  Thus, we call it the completion of Borel sets.  This is a strictly larger sigma-algebra than the Borel sets, which follows from comparing cardinalities.  Namely, the cardinality of the Borel sets is $2^{\N_0}$.  Observe the Lebesgue sigma algebra contains $2^\text{Cantor Set} = 2^C$.

Meausres on the real line are characterized by distribution functions, which are non-decreasing right continuous functions $F$.  One can adopt the same strategy to define a measure on the real line by defining $\mu((a, b]) = F(b) - F(a)$.  Similarly, given $\mu$ on $(R, B(R))$, we can check that $F(b) = \mu((-\infty, b])$ is a distribution function. 

We can also consider $(\R^d, B(\R^d))$, the Borel sets on $\R^d$.  We claim that this is
$$\Sigma ((a_1, b_1) \times (a_2, b_2) \times \dots \times (a_d, b_d)) = \Sigma (B_1, B_2, \dots, B_d: B_i \in B(R)).$$

For distribution functions on $\R^d$, consider the lexigraphical partial order.  We would like them to satisfy,
\begin{itemize}
\item $F(x)$ is non-decreasing
\item $F(x)$ is right continuous: If $x_i \downarrow x$, then $F(x_i) \rightarrow F(x)$.
\item $F(x) \rightarrow 0$ as $x \downarrow -\infty$, $F(x) \rightarrow 1$ as $x \rightarrow \infty$.
\end{itemize}
The properties above are not actually enough to define a measure. (Consider the semibox in $\R^2$).  

However, for any $F$ such that $F(A) \ge 0$ for any $A \in \Sigma_{semi}$, the strategy to build a measure on $B(\R^d)$ from $\Sigma_{semi}\rightarrow \Sigma_a \rightarrow B(R^d)$ works.

\subsection{Functions Between Measure Spaces}
Suppose we have two measure spaces $(\Omega_1, \Sigma_1), (\Omega_2, \Sigma_2)$ and a function $f:\Omega_1 \rightarrow \Omega_2$.
\begin{definition}[Measurable Function] $f$ is said to be \textbf{measurable} if $f^{-1}(A_2) \in \Sigma_1$ for all $A \in \Sigma_2$.  If $(\Omega_2, \Sigma_2) = (\R, B(\R))$, then $f$ will be called a \textbf{random variable}.
\end{definition}

\begin{proposition} If $\Sigma_2 = \Sigma(\SA)$, then to check $f$ is measurable, it suffices to check $f^{-1}(B) \in \Sigma_1$ for all $B \in \SA$.
\end{proposition}
\begin{proof}
$$\Sigma' = \{B : f^{-1}(B) \in \Sigma_1\}$$ is a sigma-algebra: If $B \in \Sigma'$, then $B^c \in \Sigma'$, since $f^{-1}(B^c) = (f^{-1}(B))^c$.  $\Omega \in \Sigma'$ since $f^{-1}(\Omega_2) = \Omega_1$.  It is easy to check countable unions.  $\Sigma'$ is a sigma algebra containing $\SA$, so $\Sigma'$ contains $\Sigma(\SA).$  
\end{proof}

\begin{fact} If we have $f: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (\Omega_2, \Sigma_2)$, $f$ induces a measure $\mu_2$ on $(\Omega_2, \Sigma_2)$ where $\mu_2(B) = \mu(f^{-1}(B))$ for all $B \in \Sigma_2$.
\end{fact}

Some properties:
\begin{itemize}
\item If we have $f: (\Omega_1, \Sigma_1) \rightarrow  (\Omega_2, \Sigma_2), g:  (\Omega_2, \Sigma_2)\rightarrow (\Omega_3, \Sigma_3)$, then $h = g \circ f$ is measurable. 
\item If $X_1, X_2$ are two random variables, then $(X_1, X_2)$ is a measurable function into $(\R^2, B(\R^2))$.

We know that $B(\R^2) = \Sigma(I_1 \times I_2)$ for intervals.  Finally, $$(X_1, X_2)^{-1}(I_1 \times I_2) = X_1^{-1}(I_1) \cap X_2^{-1}(I_2)$$, so$ (X_1, X_2)^{-1}(I_1 \times I_2)$ is measurable.
\item Suppose $F$ is a continuous function from $(\Omega_1, B(\Omega_1)) \rightarrow (\Omega_2, B(\Omega_2))$. Then $F$ is measurable, since the preimage of open sets is open.
\item if $X_1, X_2, \dots, X_d$ is a random variable, then $X_1 + X_2 + \dots + X_d$ is a random variable.
\item If $f_n$ are random variables and $f_n \rightarrow f$ pointwise.  Then, $f$ is measurable.
\begin{proof}
Consider the set $\{f > x\} = \bigcup_{n=1}^{\infty}\bigcap_{m = n}^{\infty} \{f_m > x\}$ is measurable.  Then $(x, \infty)_{x \in \R}$ is a generating set.
\end{proof}
\item$ X: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (R, B(R))$ induces a measure $\mu$ on $(R, B(R))$, where $\mu(B) = \mu_1(X^{-1}(B))$ for all $B \in B(R)$.  It also induces a distribution function $F$, which is nondecreasing, right continuous, and $F(x) \uparrow 1$ and $x \rightarrow \infty$, $F(x) \downarrow 0$ as $x \rightarrow -\infty$.  
\end{itemize}

Given a distribution function, is there a random variable?  Given a distribution function, we can construct a measure on $\R$ by the Caratheodory Extension Theorem.  Let $I: (R, B(R), \mu) \rightarrow (R, B(R))$.  We could also take $X: ([0, 1], B([0, 1], \mu) \rightarrow (R, B(R))$, where $\mu$ is uniform.  Suppose $F$ is continuous and strictly monotone.  We want $X$ to induce the distribution $F$, so it suffices to show $X^{-1}(\infty, y) = [0, F(y)]$.  If we define $X(F(y)) = y,$, we get the above, but that's not always well defined.  For general distributions, one can come up with various definitions of an "inverse" which induces the desired properties.  One particular choice is
$$w \in (0, 1), X(\omega) = \sup\{y : F(y) < \omega\}.$$

It is an exercise to check that $\{\omega: X(\omega) < x\} = \{\omega : \omega \le F(x) \}$, which implies that $X$ is measureable.
\section{September 10th, 2020}
\subsection{Integration}
We work in $(\Omega, \Sigma, \mu)$ a sigma-finite measure space.  Often, we take it to be a probability measure.  The goal is to define a notion of integration for measurable functions and the behavior of integration with limits.  Consider a function $f: \Omega \rightarrow \R$ measurable with the Borel Sigma-algebra.  

Our strategy is as follows:
\begin{enumerate}
\item Consider simple functions.
\item Extend to bounded functions.
\item Extend to general functions.
\end{enumerate}
\subsection{Simple Functions}
\begin{definition}[Simple Function] Consider $f = 1_E$, where $\mu(E) < \infty$, an indicator function.  A \textbf{simple function} is a linear combination of indicator functions,
$$f = \sum_{i=1}^k c_i 1_{A_i}, \mu(A_i) < \infty,$$
where $A_i$ are disjoint.  

We'll define the integral of a simple function as 
$$\int f d\mu = \sum_{i=1}^k c_i \mu(A_i).$$
\end{definition}
First, note that any $f = \sum_{i=1}^k d_i 1_{B_i}$ can be represented as $\sum_{i=1}^k c_i 1_{A_i}$, where they are disjoint.  We verify that our definition is well defined.  Suppose
$$f = \sum_{i=1}^k c_i 1_{A_i} = \sum_{j=1}^m e_j 1_{F_j}.$$
Then, observe that for $i, j$ such that $A_i \cap F_j \ne \emptyset$, then $c_i = e_j$.  So, we have $f = \sum_{i, j} d_{i, j} 1_{A_i \cap F_j}$, and we can check that $d_{ij} = c_i = e_j$. 
Thus,
$$\sum_{i=1}^k c_i \mu(A_i) = \sum_{i, j} d_{ij}\mu(A_i \cap F_j) = \sum_{j=1}^m e_j \mu(F_j).$$

Some properties:
\begin{itemize}
\item If $f \ge 0$ then $\int f \ge 0$.
\item $\int af = a \int f$.
\item $\int (f+g) = \int f + \int g$.
\item If $g \le f$ then $\int g \le \int f$.
\item if $g = f$, $\int g = \int f$.
\item $|\int f| \le \int |f|.$ 
\end{itemize}
\subsection{Bounded Functions}
Suppose $|f| \le M$ and $f$ vanishes outside $E$ and $\mu(E) < \infty$.

We can approximate from above or below:
$$\sup \int_{g \le f} g \le \inf \int_{h \ge f} h$$
To prove equality, it suffices to show that there exists $g, h$ such that $\int h - \int g \le \epsilon$.  It suffices to construct $h$ such that $h-f < \epsilon$ and $f-g < \epsilon$.  Then $\int h - \int g \le 2 \epsilon \mu(E)$.

Note that the range of $f$ is $[-M, M]$, so we can discretize the interval into smaller intervals $I_1, \dots, I_k$ of size $\epsilon$.   

Then, define $A_1 = f^{-1}(I_1) \cap E$, and $f^{-1}(I_j) \cap E = A_j$.  Then.
 $$h = \sum ((j+1) \epsilon - M) 1_{A_j}, g = \sum j\epsilon - M)1_{A_j}.$$
 
 Thus, $\int f = \sup \int g = \inf \int h.$
 
 Observe that the new definition agrees with the old definition when $f$ is simple.
 
 As an exercise, we'll verify $\int f+g = \int f + \int g$.  Take Suppose $\int h_1 \ge \int f \ge \int g_1$ with $\int h_1 - g_1 < \epsilon$, and $\int h_2 \ge \int g \ge \int g_2$ with $\int h_2 - g_2 < \epsilon$.
 
 Then
 $$h_1 + h_2 \ge f+g \ge g_1 + g_2,$$
 and $$\int h_1 + \int h_2 < \int g_1 + g_2 + 2\epsilon = \int g_1 + \int g_2 + 2\epsilon.$$
 \subsection{General Functions}
 Assume $f \ge 0$.  Note that we can no longer approximate from above, so we approximate from below:
 $$\int f = \sup\{\int h, h \le f, \text{ bounded }\}.$$
 
Clearly, the definition agrees with the old one for bounded functions with finite support.

As an exercise, we'll prove that $\int f+g = \int f + \int g$.  If we have a bounded $h_1 \le f, h_2 \le g$, then $h_1 + h_2 \le f+g$, which implies that $\int h_1 + h_2 = \int h_1 + \int h_2 \le \int f+g$.  and $\sup \int h_1 + \sup \int h_2 = \int f+\int g$, so $\int f + \int g \le \int f+g$.

\begin{lemma} Suppose $E_n \uparrow \Omega$, $\mu(E_n)< \infty$.  Now, consider $(f \wedge n)1_{E_n}$, where $f\wedge n$ is the minimum of $f, n$.  Then, the function is bounded and has finite support.  Note that $(f \wedge n)1_{E_n}$.  We claim that
$$\int (f \wedge n)1_{E_n} \uparrow \int f.$$
\end{lemma}
\begin{proof}
It is clear that $$\lim \int (f \wedge n) 1_{E_n} \le \int f,$$
since $h_n = (f \wedge n)1_{E_n}$ is contained in the set of bounded functions for which the supremum is $\int f$.  

If suffices to show that $\lim \int (f \wedge n)1_{E_n} \ge \int f$.  Take $h$ bounded such that $\int f < \int g + \epsilon$.  There is a set $E$ such that $g \le M$ on $E$ and $g$ is $0$ on $E^c$.  

$g \le f$, so for any $n \ge M$, $h_n \ge g$ on $E_n \cap E$.
We claim that $$\int h_n \ge \int g - M \mu(E \setminus E_n).$$  Then $E_n \uparrow \Omega$, so $\mu(E \setminus E_n) \rightarrow 0$.
\end{proof}
Now, we conclude the original proof.  Note that 
$\int f+g = \lim \int((f+g)\wedge n)1_{E_n}$,
so $$\int ((f+g)\wedge n)1_{E_n} \le \int (f\wedge n) 1_{E_n} + \int (g \wedge n)1_{E_n}.$$
Taking limits gives the desired result.
\subsection{Arbitrary Measurable Functions}
Define $\int f$ only when $\int |f| < \infty$.  Define $$f = f^+ - f^-,$$
where $f^+ = f \vee 0, f^- = f \wedge 0$.

Then $$\int f = \int f^+ - \int f^-.$$
\begin{lemma} If $f_1, f_2$ nonnegative and $f = f_1 - f_2$, then
$$\int f = \int f_1 - \int f_2.$$
\end{lemma}
\begin{proof}
$f = f^+ - f^- = f_1 - f_2$, so $$f^+ + f_2 = f_1 + f^-,$$
then $$\int f^+ + \int f_2 = \int f_1 + \int f^-,$$
so $$\int f = \int f^+ - \int f^- = \int f_1 - \int f_2.$$
\end{proof}
\pagebreak
\section{September 15th, 2020}
\subsection{Properties of Integrals with Limits}
We assume for simplicity that our measure space $(\Omega, \Sigma, \mu)$ is finite.  Last time, we defined integrals for measurable functions starting with indicators, to simple functions, to non-negative functions, and finally to general functions.

Observe that if $f$ is $0$ almost surely, then $\int f = 0$.  Suppose $\{f_n\}$ is a set of measurable functions and $f_n \rightarrow f$ pointwise almost everywhere, then $\lim f_n$ is measurable.  In other words, there exists a set $E$ such that $\mu(E) = 0$ and $f_n$ converges on $E^c$.  Define $f$ to be $0$ on $E$ and $\lim f_n$ on $E^c$.  Note that $f$ is measurable.  Suppose $f_n$ "converge" to $f$.  When can one expect $\int f_n$ to converge to $\int f$?

\begin{definition}[Convergence in Measure] We say $f_n \rightarrow f$ in measure if given $\epsilon > 0$, $$\lim_{n \rightarrow \infty} \mu\left (|f_n - f| > \epsilon\right ) = 0$$  We denote this by $f_n \xrightarrow{\mu} f$.
\end{definition}
\begin{exercise} If $\mu(\Omega) < \infty$, then $f_n \rightarrow f$ almost everywhere implies that $f_n \xrightarrow{\mu} f$.
\end{exercise}
\begin{example} Suppose $f_n = 1_{[-n, n]}$ over $\R$.  Then $f_n \rightarrow f = 1_{\R}$, but $\mu(|f_n - f| > \epsilon) = \infty$.

Recall continuity of measure from below: If $A_n \uparrow A$ then $\mu(A_n) \uparrow \mu(A)$, but if $\mu(A) = \infty$, then $\mu(A) - \mu(A_n) = \infty$ for all $n$.  This doesn't happen for $\mu(\Omega) < \infty$.
\end{example}
\begin{example} If $f_n \xrightarrow{\mu} f$, then does $f_n \rightarrow f$ almost everywhere?  No:  Take $\Omega = [0, 1]$, $f = 0$ and $f_1 = 1_{[0, 1/n]}, f_2 = 1_{[1/n, 2/n]}, ..., f_n = 1_{[n-1/n, 1]}, f_{n+1} = 1_{[0, 1/(n+1)]}, ...$

There is always some interval where $f_n = 1$, so it does not converge pointwise to $0$.
\end{example}
\begin{example} If $f_n \xrightarrow{\mu} f$, then does $\int f_n \rightarrow \int f$?  No.
Take $f_n = \frac{1}{n}1_{[0, n]}$.  
\end{example}
\begin{thm}[Bounded Convergence Theorem] Suppose $\mu$ is finite and $f_n$ are supposed on $E$ such that $\mu(E)  < \infty$.

If $|f_n| < M$ and $f_n \xrightarrow{\mu} f$, then $\int f_n \rightarrow \int f$.
\end{thm}
\begin{proof}
$f$ must be $0$ almost everywhere outside $E$.  Define $F = \{|f_n - f| < \epsilon\}$.  Note that 
$$\left |\int_E f_n - \int_E f\right | \le \int_{F \cap E} |f_n - f| + \int_{E\cap F^c} |f_n - f|$$
$$\le \epsilon \mu(E) + 2M\mu(F^c \cap E) \xrightarrow{\epsilon \rightarrow 0} 0.$$
\end{proof}
\begin{thm}[Fatou's Lemma] If $f_n \ge 0$ then
$$\liminf_{n \rightarrow \infty} \int f_n d\mu \ge \int \left (\liminf_{n \rightarrow \infty} f_n\right )d\mu.$$
\end{thm}
\begin{proof}
Let $g_n(x) = \inf_{m\ge n}f_m(x)$.  Then $g_n(x) \uparrow g = \liminf f_n$.  

We know that $f_n \ge g_n$.  This implies that $\int f_n \ge \int g_n$.  We have that
$$\liminf \int f_n \ge \liminf \int g_n = \lim \int g_n.$$

Hence, It suffices to show that 
$$\lim \int g_n \ge \int g.$$

Recall that $g_n \uparrow g$ so $\lim g_n = g$.  Consider $g_n \wedge m$, a bounded function. Note that $g_n \wedge m \uparrow g \wedge m$, so by the Bounded Convergence Theorem,
$$\int (g_n \wedge m) \uparrow \int (g \wedge m).$$
Furthermore, we have 
$$\int g_n \ge \int (g_n \wedge m),$$
so $$\lim \int g_n \ge \lim \int (g_n \wedge m) \uparrow \int (g \wedge m) \uparrow \int g,$$
where the last inequality comes from approximation by bounded functions of finite support.
\end{proof}
\begin{thm}[Monotone Convergence Theorem] If $f_n \ge 0$ and $f_n \uparrow f$, then $\int f_n \uparrow \int f$.
\end{thm}
\begin{proof}
Note that $\int f \ge \lim_{n \rightarrow \infty} \int f_n$ since $\int f \ge \int f_n$.  Then $\int f \le \lim_{n \rightarrow} \int f_n$ by Fatou's lemma.
\end{proof}
\begin{thm}[Dominated Convergence Theorem] If $|f_n| \le g$ where $\int g \le \infty$ and $f_n \rightarrow f$ pointwise, then
$$\int f_n \rightarrow \int f.$$
\end{thm}
\begin{proof}
Note that $f_n + g \ge 0$, and $f_n + g \rightarrow f+g$ so by Fatou's lemma,
$$\liminf_{n \rightarrow \infty} \int f_n + g  \ge \int f+g d\mu,$$
which implies that $\liminf_{n \rightarrow \infty} \int f_n \ge \int f.$

Then, applying the result to $g-f_n$, we have $$\liminf_{n \rightarrow \infty} -f_n\ge \int -f \Rightarrow \limsup_{n \rightarrow \infty} f_n \le \int f,$$
which implies that $\lim \int f_n = \int f$, as desired.
\end{proof}

\subsection{Expected Value}
We have been discussing measurable functions, but these can easily be translated into the language of random variables.  Namely, if $X$ is a random variable and $\int |X| < \infty$, then $\int X = E(X)$, the expectation of $X$.

\begin{itemize}
\item $X_n \ge 0$, then $X_n \uparrow X \rightarrow E(X_n) \rightarrow E(X)$.
\item $|X_n| < Y$, $X_n \rightarrow X$, $E(Y) < \infty$, then $E(X_n) \rightarrow E(X)$.
\end{itemize}
\subsection{Change of measure for Integrals}
We have a random measurable map $$X: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (\Omega_2, \Sigma_2) \xrightarrow{f} (\R, B(\R)).$$  
Then $f \circ X: (\Omega_1, \Sigma_1) \rightarrow (\R, B(\R))$, and if $\int |f \circ X| < \infty$, then $X$ induces a measure $\mu_2$ on $(\Omega_2, \Sigma_2)$ with $\mu_2(A) = \mu_1(X^{-1}(A)).$
Hence, we can discuss
$$\int_{\Omega_2}f d\mu_2.$$
\begin{thm}[Change of Measure]
$$\int_{\Omega_1} f \circ X d\mu_1 = \int_{\Omega_2} f d\mu_2.$$
\end{thm}
\begin{proof}
Let $f = 1_{E}$ for $E \in \Sigma_2$.
$$\int_{\Omega_2} f d\mu_2 = \mu_2(E) = \mu_1(X^{-1}(E)).$$
Then $f \circ X = 1(X^{-1}(E))$, so 
$$\int f \circ X = \mu_1(X^{-1}(E)).$$

For simple functions, we can use linearity of integrals for the result.  For nonnegative functions, we construct a monotone sequence of functions which increase to $f$.  One possible choice is 
$$f_n = \frac{\floor{2^n f}}{2^n} \wedge n.$$

We know that $f_n \uparrow f$ and $\int_{\Omega_1} f_n \circ X = \int_{\Omega_2} f_n$, and $f_n \circ X \uparrow f \circ X$, so by the monotone convergence theorem,
$$\int_{\Omega_1} f_n \circ X \rightarrow \int_{\Omega_1} f\circ X,$$
and $$\int_{\Omega_2} f_n \rightarrow \int_{\Omega_2} f,$$
so it follows that $\int_{\Omega_1}f \circ X = \int_{\Omega_2} f,$ as desired.
\end{proof}
\subsection{Product Measures}
We will relate high dimensional integrals with low dimensional ones with the notion of Product Measures.

Let $(\Omega_1, \Sigma_1, \mu_1), (\Omega_2, \Sigma_2, \mu_2)$ be measure spaces.  Consider $(\Omega_1 \times \Omega_2, \Sigma(\Sigma_1 \times \Sigma_2))$.  Note that $\Sigma_1 \times \Sigma_2$ is a semialgebra.  From here, we construct the product measure.
\begin{thm} There exists a unique measure on $\Sigma_p = \Sigma_1 \times \Sigma_2$, $\mu$ such that 
$$\mu(A \times B) = \mu_1(A) \mu_2(B)$$
for all $A \times B \in \Sigma_p$.  We will call this the product measure.
\end{thm}
\begin{proof}
Given a countable additive sigma-finite measure on a semi-algebra, it admits a unique extension to the generated sigma-algebra by Caratheodory's extension theorem.  To prove the existence and uniqueness, it suffices to check countable additivity on $\Sigma_1 \times \Sigma_2$.  

If we have $A \times B = \bigcup A_i \times B_i$, we want $$\mu(A \times B) = \mu_1(A)\mu_2(B) = \sum \mu_1(A_i)\mu_2(B_i).$$

Our strategy is to product to one dimension less. Fix $x \in A$.  Note that $B = \{y : (x, y) \in A \times B\}$.  But $A \times B = \bigcup A_i \times B_i.$  Consider all $A_i$'s that contain $x$.  Then, the corresponding $B_i$'s form a disjoint partition of $B$.  We know that $\mu_2(B) = \sum_{x \in A_i} \mu_2(B_i)$ for all $x \in A$, so in particular
$$1_A\mu_2(B) = \sum 1_A \mu_2(B_i).$$
We claim that the two functions are pointwise same on $\Omega_1$.  Then, we integrate (uses MCT) with respect to $\mu_1$ to get
$$\mu_1(A)\mu_2(B) = \lim_{n \rightarrow \infty} \sum_{i=1}^n \mu_1(A_i)\mu_2(B_i).$$
\end{proof}
\pagebreak
\section{September 17th, 2020}
\subsection{Product Measures, Continued}
We have a semialgebra $\Sigma_1 \times \Sigma_2$ and we denote $\Sigma_{1 \times 2} = \Sigma(\Sigma_1 \times \Sigma_2)$.  Let $E \in \Sigma_{1 \times 2}$.
\begin{lemma} For any $X \in \Omega_1$, the set
$$E_x = \{y \in \Sigma_2 : (x, y) \in E\}$$
is measurable.
\end{lemma}
\begin{proof}
If $E = E_1 \times E_2$, then either $E_x = \emptyset$ or $E_x = E_2$.  We show that the set of $E$ with this property forms a sigma-algebra.  If $E \in \mathcal{A}$, then $(E^c)_x = (E_x)^c$ so $E^c \in \mathcal{A}$.  

If $E_1, E_2, \dots \in \mathcal A$, then
$$\left (\bigcup_{i=1}^\infty E_i\right )_x = \bigcup_{i=1}^\infty (E_i)_x.$$

Hence, $\mathcal{A}$ is a sigma-algebra containing $\Sigma_1 \times \Sigma_2$, so $\mathcal{A} = \Sigma_{1 \times 2}$.
\end{proof}

\begin{thm} For any $E \in \Sigma_{1 \times 2}$, 
$$\mu_1 \times \mu_2(E) = \mu(E) = \int_{\Omega_1} \mu_2(E_x)d\mu_1,$$
and $\mu_2(E_x)$ is a measurable function from $\Omega_1 \rightarrow \R$.
\end{thm}
\begin{proof}
The result is clear for rectangles.  If $E_1, E_2 \in \mathcal{A}$, then $\mu_2(E_x) = \mu_2(E_{1x}) + \mu_2( E_{2x}) - \mu((E_{1} \cap E_{2})_x)$  We use the $\pi-\lambda$ theorem.  It suffices to show that $\mathcal A$ is a $\lambda$-system and we have that $A \supset \Sigma_1 \times \Sigma_2$, which is a $\pi
$-system.

It is clear that $\Omega_1 \times \Omega_2 \in \mathcal A$.  We claim that if $E_n \in \mathcal A$, then $E_n \uparrow E$ implies $E \in \mathcal A$.  
Note that $(E_n)_x \uparrow E_x$ for all $x$ then $\mu_2(E_{nx}) \uparrow \mu_2(E_x)$ and if we define $\mu_2(E_{nx}) = f_n(x)$, then $f_n(x) \uparrow E_x$ is measurable, as desired.  
Finally, we show that if $E_1 \supset E_2$ and $E_1, E_2 \in \mathcal A$, then $E_1 \setminus E_2 \in \mathcal A$.  This is clear since $(E_1 \setminus E_2)_x = E_{1x} \setminus E_{2x}$ so $\mu((E_1 \setminus E_2)_x) = \mu_2(E_{1x}) - \mu_2(E_{2x})$ is measurable as the difference of finite measurable functions.

The same argument shows that $\mathcal A$ is a $\lambda$-system if we define $\mathcal A$ to be all $E$ so that both conclusions of the theorem hold. 

\end{proof}
\begin{thm}[Fubini] Let $f \ge 0$ or $\|f\|_1 < \infty$.  Then 
\begin{enumerate}
\item For all $x$, $f(x, \cdot)$ is measurable on $\Sigma_2$.
\item $\int f(x, \cdot)$ is measurable on $\Sigma_1$.
\item $\int \int f(x, \cdot) = \int f$.
\end{enumerate}
\end{thm}
\begin{proof}
We have verified this for $f = 1_E$.  Suppose $f \ge 0$.  By linearity of integrals and the fact that the sum of measurable functions is measurable, the claim holds for simple functions.  For general $f \ge 0$, take a sequence of simple functions $f_n \uparrow f$.  

Then,
$$\int \left( \int f_n(x, \cdot) d\mu_2 \right )d\mu_1 = \int f_n d(\mu_1\times \mu_2),$$
so the result follows from the monotone convergence theorem.

For general $f \in L^1(\R)$, $f = f^+ - f^-$, so we use the above to conclude.
\end{proof}
\begin{example}[Not-integrable Function] Let $\Omega_1 = \Omega_2 = \N$.  Suppose $\mu_1, \mu_2$ are counting measures. Let $f(m, m) = 1$, $f(m+1, m) = -1$ and $f(m, n) = 0$.  Then 
$$\sum_m \sum_n f(m, n) = 1, \sum_n \sum_m f(m, n) = 0.$$
The failure is that $f \not \in \ell^1$.
\end{example}
\begin{example}[Not $\sigma$-finite] Let $\Omega_1 = \Omega_2 = (0, 1)$.  Let $\mu_1$ be the uniform measure and $\mu_2$ be the counting measure.  Let $E = \{(x, x): x \in (0, 1)\}$.   Then $\int \int \mu_2(E_x) d\mu_1 = 1$, but $\int \int \mu_1(E_y) d\mu_2 = 0$. 
\end{example}
\subsection{Independence}
\begin{definition}[Naive Independence] If $X_1, X_2$ are random variables, then $X_1$ and $X_2$ are independent if $$P(X_1 \in E, X_2 \in F) = P(X_1 \in E)P(X_2 \in F).$$
\end{definition}
We will generalize this notion.
\begin{definition}[Independence] For a $(\Omega, \Sigma, \mu)$, if $\Sigma_1, \Sigma_2, \dots, \Sigma_k \subset \Sigma$ are said to be 
\textbf{mutually independent} if for any subset $\{i_1, i_2, \dots, i_\ell\} \subset \{1, \dots, k\}$ and sets $A_{i_1}, A_{i_2}, \dots, A_{i_\ell}$, $A_{i_j} \in \Sigma_{i_j}$ 
$$\mu(A_{i_1} \cap \dots \cap A_{i_\ell}) = \prod \mu(A_{i_j}).$$

This is the same as the condition $$\mu(A_1 \cap A_2 \cap \dots A_k) = \prod_{i=1}^k \mu(A_i),$$
since we take some of the $A_i = \Omega$.
\end{definition}
\begin{definition}[Independent Random Variables] $X_1, \dots, X_k$ are mutually independent of $\{\Sigma(X_i)\}$ are mutually independent.
\end{definition}
\begin{thm} Suppose $A_1, A_2, \dots \subset \Sigma$ are mutually independent $\pi$-systems.  Then $\Sigma(A_i)$ are also mutually independent.
\end{thm}
\begin{proof}
Wlog, we can assume $\Omega \in A_i$ for all $i$.  Fix $B_2 \in A_2, \dots, B_\ell \in A_\ell$.  For $B_1 \in \Sigma(A_1)$, define the two measures $\mu', \mu''$ as 
$$\mu'(B_1) = \mu(B_1 \cap B_2 \cap \dots\cap B_\ell),$$
$$\mu''(B_1) = \mu(B_1) \prod_{i=2}^\ell \mu(B_i).$$  

We claim that $\mu' = \mu''$.  Observe that $\mu'$ and $\mu''$ agree on $A_1$ by hypothesis, so the claim holds by the uniqueness part of the Caratheodory Extension theorem on $\Sigma(A_1)$.

$\Sigma(A_1), A_2, \dots, A_\ell$ are mutually independent $\pi$ systems.  We iterate to get that $\Sigma(A_i)$ are mutually independent.
\end{proof}
\begin{example}[Pairwise Independent $\ne$ Mutually Independent]  Take $X_1, X_2, X_3 \in \{0, 1\}$,  Pick $(X_1, X_2, X_3)$ uniformly from all triples $(x_1, x_2, x_3)$ such that $x_1 + x_2 + x_3 = 0 \pmod 2$.  Note that $P(X_i = 1) = P(X_i = 0) = 1/2$.  It is clear that $(X_i, X_j)$ are independent, but $(X_1, X_2, X_3)$ are not independent since $P((X_1, X_2, X_3) = (1, 1, 1)) = 0 \ne (1/2)^3$.
 \end{example}
\begin{thm}[Kolmogorov's 0-1 Law]
 Suppose $X_1, X_2, \dots$ are independent random variables.  Consider $$T_n = \sigma(X_n, X_{n+1}, \dots),$$
 and let $$T = \cap_{n=1}^{\infty} T_n$$
 (this is known as a tail-sigma algebra).  Then $T$ is a $\mu$-trivial sigma algebra: for all $E \in T$, $\mu(E) = 0$ or $1$.
 \end{thm}
 \begin{proof}
 The idea is $E$ is independent of $X_1, \dots, X_{n-1}$, so $E$ is independent of $\sigma(X_1), \sigma(X_2), \dots, \sigma(X_{n-1})$.  Hence $E$ is independent of $\bigcap_{i=1}^{n-1}\sigma(X_i)$, so $E$ is independent of $\bigcap_{i=1}^{\infty} \sigma(X_i)$, so $E$ is independent of $\Sigma(X_1, X_2, \dots)$.  But $E \in T \subset \Sigma(X_1, \dots)$, so $P(E \cap E) = P(E)P(E) = P(E)$, so $P(E) = 0$ or $1$.
 
 Claim: If $A_{ij}$ for $j=1, \dots, m_i$ such that $A_{ij}$ are all $\pi$-systems containing $\Omega$ are mutually independent, then $\Sigma(A_{i1}, A_{i2}, \dots, A_{im_i})$ are also mutually independent.  To prove this, let $A_i = \{B_1 \cap B_2 \cap \dots \cap B_{m_i}:B_j \in A_{ij}\}$.
 
 We know that $\Sigma(X_1), \dots $ are independent $\pi$ systems,so $\Sigma(X_1, \dots, X_n)$ and $\Sigma(X_{n+1}, \dots)$ are independent.  Hence $E$ is independent of $\bigcap_{i=1}^\infty \Sigma(X_i)$, so $E$ is independent of $T$, which gives the result. 
 \end{proof}
\begin{thm}[Kolmogorov Extension] Take $(\R^n, B(\R^n), \mu_n)$ a consistent family of measures on $\R^n$: for $A \in B(\R^n)$
$$\mu_{n+1}(A \times \R) = \mu_n(A).$$
Then there exists a measure $\mu$ on $(\R^\N, B(\R^\N))$ such that $\mu$ agrees with $\mu_n$ on $\R^n \times \R \times \R \times \dots$.

\end{thm}
\pagebreak
\section{September 22nd, 2020}
Last time, we discussed product measures, independent random variables/sigma algebras, and how to construct infinitely many independent random variables.  We also proved the $0-1$ law for tail-sigma algebras.

If we have $(\Omega, \Sigma, \P)$ and random variables $X_1, X_2, \dots$, $T_n = \Sigma(X_n, X_{n+1}, \dots)$ and $T_\infty = \bigcap T_n$ is a sigma algebra that is P-trivial.

Any event that does not depend on any finite set of $X_i$'s is in the tail-sigma algebra.  For example, let $Y = \limsup X_i$ and $E = \{Y < t\}$.  Note that $Y$ does not depend on finitely many $X_i$'s.  Another example is $S_n = \sum_{i=1}^n x_i$ and we define $Y = \limsup \frac{S_n}{n}$.

When does $\frac{S_n}{n}$ have a limit?

\subsection{Law of Large Numbers}
We have $X_1, X_2, \dots$ independent random variables.  What is the asymptotic behavior of $\frac{S_n}{n}$?


Suppose $X_1, X_2, \dots$ have $E(X_i^2) < C$, $E(X_iX_j) = 0$ and $E(X_i) = 0$.  Then, 
$$\frac{S_n}{n} \xrightarrow{\P} 0 \Leftrightarrow \P\left (|\frac{S_n}{n}| > \epsilon \right ) \rightarrow 0.$$ 
\begin{proof}
We first note Markov's Inequality: Suppose $X$ is a nonnegative random variable.  For any positive $c$, 
$$P(X > c) \le \frac{E(X)}{c}.$$

Furthermore, note that $$\left \{ \left| \frac{S_n}{n}\right| > \epsilon\right \} = \left \{\left (\frac{S_n}{n}\right )^2 > \epsilon^2\right \}.$$

By Markov's Inequality,
$$\P((\frac{S_n}{n})^2 > \epsilon^2) \le \frac{1}{n^2\epsilon^2} E(S_n^2),$$
and finally,
$$E(S_n)^2 = E((X_1 + \dots X_n)^2)=\sum EX_i^2 + \sum E(X_iX_j) \le nC$$
so 
$$\frac{1}{n^2\epsilon^2} E(S_n^2) \le \frac{nC}{n^2\epsilon^2} = \frac{C}{n\epsilon^2} \rightarrow 0.$$
\end{proof}
\begin{corollary}If $X_1, X_2, \dots$ are independent with the same distribution and $E(X_i) = \mu$ $E(X_i^2) = \sigma^2$, then 
$$\frac{S_n}{n} \xrightarrow{\P} \mu.$$
\end{corollary}
\begin{proof}
Note that $E(\overline{X_i}\overline{X_j}) = E(\overline{X_i})E(\overline{X_j}) = 0$ by Fubini's theorem.  Hence we apply the previous theorem to $\bar{X_i} = X_i - \mu$.
\end{proof}
\begin{fact} Chebyshev's Inequality: For any RV X, 
$$P(|X| > t) \le \frac{E(X^2)}{t^2}.$$
\end{fact}
\begin{example}[Polynomial Approximation]
Task: Given $f:[0, 1] \rightarrow \R$ continuous, and $\epsilon > 0$, find a polynomial $f_n(x)$ such that 
$$|f_n(x) - f(x)| < \epsilon$$
for all $x \in [0, 1]$.

Let $$f_n(x) = \sum_{m=0}^n \binom{n}{m}x^m(1-x)^{n-m}f\left (\frac{m}{n}\right ).$$
We expect $f_n(x) \approx f(x)$ by the Binomial Theorem.  Precisely,
$$f_n(x) = E\left (f\left (\frac{S_n}{n}\right )\right )$$
where $S_n \sim Bin(n, x)$ with $S_n = \sum_{i=1}^n X_i$ for $X_i \sim Ber(x)$.  It suffices to show that $\frac{S_n}{n} \approx x$.

By the Law of Large Numbers,
$$P\left (\left |\frac{S_n}{n} - x\right | > \epsilon\right ) \rightarrow 0.$$

Since $f$ is continuous on $[0, 1]$, it is uniformly continuous, so that given $\delta$, there exists $\epsilon$ such that for all $x, y$ with $|x - y| < \epsilon$, $|f(x)- f(y)| < \delta$.  If we let the event above be $A^c$, then, 
\begin{align*}
E(f(S_n/n)) &= E(f(S_n/n)1_A) + E(f(S_n/n)1_{A^c}) \\
&= f(x)P(A) + E(f(S_n/n) - f(x))1_A) + E(f(S_n/n))1_{A^c} \\
& \le f(x)P(A) + \delta P(A) + \sup_{x \in [0, 1]} f(x)P(A^c)
&\rightarrow f(x).
\end{align*}

Note that $$P(A^c) \le \frac{\text{Var}(S_n)}{n^2 \epsilon^2} \le \frac{1}{n\epsilon^2}$$
since $\text{Var}(X_i) \le 1$ for $X_i \in [0, 1]$.

Hence, for any $x \in [0, 1]$, $f_n(x) \rightarrow f(x)$ uniformly as $n \rightarrow \infty$.
 \end{example}
 
Now, our goal is to prove the law of Large Numbers without the second moment assumption.  Namely, for $X_1, X_2, \dots$ iid with $E|X_i| < \infty$, $EX_i = 0$,
$$\frac{S_n}{n} \xrightarrow{P} 0.$$
Our strategy is truncation.  
\begin{definition} For any random variable $X$, we will consider the random variable from $X_M = X1_{|X| < M}$. Note that we have $E(X_M^2) < \infty$ for all $M$ even if $E(X^2) = \infty$.
\end{definition}
\begin{thm} Suppose that for each $n$ there exists a constant $b_n$ such that 
$$\sum_{i=1}^n P(|X_{n_i}| > b_n) \rightarrow 0$$
and $$\sum_{i=1}^n\frac{ E(\overline{X_{n_i}})^2}{b_n^2} \rightarrow 0.$$

Then
$$\sum_{i=1}^n\frac{ X_{n_i} - E(\overline{X_{n_i}})}{b_{n}} \rightarrow 0.$$
\end{thm}
\begin{proof}
We first prove that 
$$Y = \sum_{i=1}^n \frac{\overline{X_{n_i}} - E(\overline{X_{n_i}})}{b_n} \rightarrow 0.$$
This follows from Chebyshev, since $E(Y) = 0$ and 
$$\text{Var}(Y) \le \sum_{i=1}^n\frac{E(\overline{X_{n_i}}^2)}{b_n^2}.$$

Then $\sum_{i=1}^n P(|X_{n_i} > bn|) \rightarrow 0$ so if $X_{n_i} < b_n$, $X_{n_i} = \overline{X_{n_i}}$.

Let $B = \{X_{n_i} \ne \overline{X_{n_i}} : i \in \{1, \dots, n\}\}.$  Then $$P(B) \le \sum_{i=1}^n P(|X_{n_i}| > b_n) \rightarrow 0,$$
so it follows that 
$$\sum_{i=1}^n\frac{ X_{n_i} - E(\overline{X_{n_i}})}{b_{n}} \rightarrow 0.$$
\end{proof}

\begin{lemma}
Suppose $X_1, X_2, \dots$ are iid.  Suppose that $$KP(|X_1| > K) \rightarrow 0.$$

Then
$$\frac{\sum_{i=1}^n X_i - n E(X_1 1\{|X_1| < n\})}{n} \rightarrow 0$$
in measure.
\end{lemma}
\begin{proof}
Note that this does not imply $E(X_1) < \infty$.  Form a triangular sequence from the $X_i$'s and let $b_n = n$. 
 We show that $\sum_{i=1}^n P(|X_i| > n) \rightarrow 0$ and $\sum_{i=1}^n E(\overline{X_i}^2) \rightarrow 0$.

For 2, it suffices to show
$$E(\overline{X_i}^2)/n \rightarrow 0.$$

Note that $|\overline{X_i}|= |X_i| 1\{|X_i| < n\}.$  Suppose $X$ is a non-negative random variable. 
Note that 
$$E(X) \approx \sum_{n=1}^{\infty} P(X > n).$$

Similarly,
$$E(X^2) \approx \sum_{n=1}^{\infty} nP(X > n).$$

Then 
$$E(\overline{X_i}^2) \approx \sum_{K = 1}^n KP(X_1 > K).$$

It suffices to show that 
$$\frac{\sum_{k=1}^n kP(|X_1| > k)}{n} \rightarrow 0,$$
which follows from the fact that $kP(|X_1| > k) \rightarrow 0$.
\end{proof}

\begin{thm}[Law of Large Numbers] If $X_1, X_2, \dots$ iid and $E(|X_1|) < \infty$ and $E(X_1)  = 0$, then $S_n/n \rightarrow 0$ in measure.
\end{thm}
\begin{proof}
Note that $kP(|X_1| > k)  \le E(|X_1| 1\{|X_1| > k\}) \rightarrow 0$, by the dominated convergence theorem. By the lemma,
$$\frac{S_n}{n} - E(\overline{X_1}) \rightarrow 0,$$
and note that $E(\overline{X_1}) \rightarrow E(X_1) = 0$ by the dominated convergence theorem.
\end{proof}
\pagebreak
\section{September 24th, 2020}
\subsection{Law of Large Numbers, continued}
Last time, we began discussing the Law of large numbers.  Recall:
\begin{itemize}
\item Markov's Inequality: $$P(|X| > c) \le \frac{E(|X|)}{c}.$$
\item With $X_1, X_2, \dots$ iid, $E(X_i) = 0$.  When $E(X_1^2) < c$, 
$$S_n/n \xrightarrow{P} 0.$$
\item Under 1st Moment condition, we used truncation to make thinks bounded and have second moments.  We discussed triangular arrays and saw a theorem which proves a LLN type of statement for truncated variables.
\item We showed that the truncation has no limiting effect.  Then, we considered 
$$\sum X_i 1_{|X_i| < n}/n \rightarrow 0,$$
which implied the law of large numbers.
\end{itemize}
\begin{example} Let $X_1, X_2, \dots$ be iid with $X_i \ge 0$.  Suppose $E(X_1) = \infty$.  Then
$$\frac{\sum_{i=1}^n X_i}{n} = ?$$
Let $Y_i \sim X_i 1_{|X_i| < M}$.  Then $S_n'/n \sum_{i=1}^n Y_i/n \rightarrow E(Y_i)$ by the weak law of large numbers.  But by nonnegativity, $S_n/n > S_n'/n \rightarrow E(Y_i)$, but $E(Y_i)$ can be made arbitrarily large by choosing $M$ very large.  

For any $c$,
$$P\left (\frac{S_n}{n} > c\right ) \rightarrow 1,$$
so $S_n/n \rightarrow \infty$. 
\end{example}
\begin{example} Let $X= 2^i$ with probability $1/2^i$ for $i \ge 1$.  Note $E(X) = \infty$.

Let $X_1, X_2, \dots$ be iid $X$.  What is the growth rate of $S_n$?   One expects to see some $X_i$'s take value comparable to $n$ since $P(X_1 = n) = \frac{1}{n}$.  

We will control the growth with truncation.  Let $\alpha_n = \log n + k(n)$, $b_n = 2^{\alpha_n}$.  We need to show that 
$$\sum_{i=1}^n P(X_i > b_n) \rightarrow 0,$$
and $$\frac{\sum_{i=1} ^nE(X_i^2 1_{X_i < b_n})}{b_n^2} \rightarrow 0.$$

Note that $$P(X_i > b_n) \approx \frac{1}{b_n} = \frac{1}{n2^{k(n)}},$$
so $$\sum P(X_i > b_n) = \frac{1}{2^{k(n)}} \rightarrow 0.$$

Then,
$$E(X_i^2 1_{X_i < b_n})\approx \sum_{i=1}^{\alpha(n)}2^{2i}/2^i = \sum_{i=1}^{\alpha(n)}2^i \approx 2^{\alpha(n)} = b_n.$$

Then,
$$\frac{\sum_{i=1} ^nE(X_i^2 1_{X_i < b_n})}{b_n^2} \approx \frac{nb_n}{b_n^2} = \frac{1}{2^{k(n)})} \rightarrow 0.$$

Therefore,
$$\frac{S_n - nE(\overline{X_i})}{b_n} \rightarrow 0.$$

Note that $E(\overline{X_i}) = \alpha(n)$, so 
$$\frac{S_n - n(\log n +k(n))}{n2^{k(n)}}$$

If we choose $\log \log n$, then
$$\frac{S_n - n(\log n + \log \log n)}{n \log n} \rightarrow 0,$$
so $$\frac{S_n}{n\log n} \rightarrow 1 \Longrightarrow S_n = \Theta(n\log n).$$
\end{example}
\subsection{Almost Sure Convergence}
Let $X_1, X_2, \dots$ iid, $E(X_i) = 0$, $E(X_i^2)< C$.

We know that $$\frac{S_n}{n} \xrightarrow{P} 0,$$
but do we have $$\frac{S_n}{n} \rightarrow 0,$$
almost surely?

\begin{lemma}[Borel-Cantelli] If events $E_i$ satisfy $\sum_{i=1}^{\infty} P(E_i) < \infty$, then $P(E_i \text{ infinitely often}) = 0$.
\end{lemma}
\begin{example}
Let $\epsilon > 0$.  We want
$$P\left (\left |\frac{S_n}{n}\right | > \epsilon,  i. o. \right )  = 0.$$

In order to apply BC, we have to show 
$$\sum P\left (\left |\frac{S_n}{n}\right | > \epsilon \right )<\infty,$$
but 
$$\sum P\left (\left |\frac{S_n}{n}\right | > \epsilon \right ) \approx \frac{1}{\epsilon^2 n} \rightarrow \infty.$$

We try to get around this by assuming a higher moment.  Suppose $E(X^4) < \infty$.  

Then,
$$\frac{E(S_n^4)}{n^4} = \frac{E((\sum_{i=1}^n X_i)^4)}{n^4}=\frac{nE(X_1^4) + n^2 E(X_1^2X_2^2)}{n^4}\approx \frac{1}{n^2}.$$
So 
$$P\left (\left |\frac{S_n}{n}\right| > \epsilon\right ) \le \frac{1}{\epsilon^4}E((S_n/n)^4) \approx \frac{1}{\epsilon^4 n^2},$$
which gives that 
$$\sum P\left (\left |\frac{S_n}{n}\right| > \epsilon\right ) < \infty.$$
\end{example}

Can one use naive Markov to show a subsequence converges?  

If we let $K(n) = n^2$, 
$$P(|\frac{S_{k(n)}}{k(n)} |> \epsilon) \approx \frac{1}{n^2}$$
so we can take the infinite sum and it approaches $0$. 

Define $$D(n) = \sup_{k(n) \le i \le k(n+1)} |S_i - S_{k(n)}|.$$

It suffices to show that $$\frac{D(n)}{k(n)} \rightarrow 0.$$

We know that 
$$P(|D_n/k(n)| > \epsilon) \le \sum_{i=k(n)}^{k(n+1)}P\left (\frac{|S_i - S_{k(n)}|}{k(n)}> \epsilon\right ),$$
by subadditivity.  By Chebyshev,
$$\sum_{i=k(n)}^{k(n+1)}P\left (\frac{|S_i - S_{k(n)}|}{k(n)}> \epsilon\right ) \le \sum\frac{i-k(n)}{k(n)^2 \epsilon^2} \le \frac{(k(n+1) - k(n))^2}{2k(n)^2} \approx  \frac{1}{n^2} ,$$
so $\frac{D_n}{k(n)} \rightarrow 0$ almost surely by BC.

\end{document}
