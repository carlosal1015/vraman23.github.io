\documentclass[11pt]{scrartcl}
\usepackage[sexy]{evan}
\usepackage{graphicx}

 %Sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb C}

%From Topology
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cU}{\mathcal{U}}
\renewcommand{\P}{\mathbb{P}}

\let \mc \mathcal
\let \ov \overline

\usepackage{answers}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\declaretheorem[style=thmbluebox,name={Theorem}]{thm}


\begin{document}
\title{Math 218a}
\author{Vishal Raman}
\thispagestyle{empty}
$ $
\vfill
\begin{center}

\centerline{\huge \textbf{Math 218a Lecture Notes, Fall 2020}}
\centerline{\Large \textbf{Probability Theory} } 
\centerline{Professor: Shirshendu Ganguly}
\centerline{Vishal Raman}
\end{center}
\vfill
$ $
\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
%\maketitle
\section{August 27th, 2020}
\subsection{Introduction}
Consider a \textbf{random experiment} - this involves a state space $\Omega$ and some "probability" on it.  The outcome of an experiment would be $\omega \in \Omega$.

\begin{example}[Fair Coin Toss] $\Omega = \{0, 1\}, P(0) = 1/2, P(1) = 1/2$ models a fair coin toss.  The outcomes are $\omega \in \Omega, \omega = 0$ or $\omega = 1$.
\end{example}
\begin{example}[Continuous State Space] $\Omega = [0, 1]$, $X$ is the outcome of a random experiment.  Suppose $X$ is uniformly distributed random variable.  $P(X \in [0, \frac{1}{2}]) = 1/2$.  Take $A = \Q \cap [0, 1]$.  $P(x \in A) = 0$, since $A$ has no "volume".  Similarly, taking $A_1 = \R\setminus\Q \cap [0, 1]$, then $P(x \in A_1) = 1 - P(x \in A) = 1$.  Finally, take $E \subset [0, 1]$.  $P(x \in E) = $"volume" of $E$.   
\end{example}
The issue: we need to define some notion of volume.  Some properties we would like are the following:
\begin{itemize}
\item Translation Invariance
\item Countable Additivity: $A_1, A_2, \dots$ disjoint with $A = \bigcup A_i$, then $P(A) = \sum_{i=1}^{\infty}P(A_i)$.
\end{itemize}
\subsection{Nonmeasurable Sets}
Take $I = [-1, 2]$, and define $x \sim y$ iff $x-y \in \Q$. [Exercise: check that $\sim$ is an equivalence relation.]  This decomposes $I$ into equivalence classes $I/\sim$.  Note that the equivalence classes are countable, since any class is $x + A, A \subset Q$.  

For each equivalence class $B$, pick $x_B \in B\cap [0, 1]$.  Define $E = \{x_B \}$ over all the equivalence classes.  Note that $x_B$ is a representative of $B$ in $E$, so $B = \{x_b + q : x_b + q \in I, q \in \Q \}.$  

Now, consider the set $[0, 1] \subset \bigcup_{q \in [-1, 1]}E + q \subset [-1, 2]$.  Equality doesn't hold, because there can be $B$ s. t. $x_b$ is close to $0$.  Then $E + (\Q \cap [-1, 1])$ will only recover elements of $B$ near $1$ and will not go up to $2$.

\begin{proposition} We claim that $E+q$ are disjoint for different values of $q$.
\end{proposition}
\begin{proof}
Suppose $E+q_1 \cap E+q_2 \ne \emptyset$ for some $q_1, q_2$.  Then, there exists $x, y \in E$ such that $x+q_1 = y + q_2$.  This implies that $x-y = q_2 - q_1 \in \Q$, so $x \sim y$, but by definition, there is exactly one member of each equivalence class in $E$.  
\end{proof}

The big question: What is $P(E)$?  Suppose $P(E) > 0$.  Then $\bigcup_{q \in [-1, 1]} E + Q \subset [-1, 2]$ and $P(E + q_1) = P(E + q_2) = P(E)$ for all $q_1, q_2$.  Furthermore, by countable additivity,
$$1 \ge P(\bigcup_{q \in [-1, 1] E + q}) = \sum_{q \in [-1, 1]} P(E + q) = \infty \cdot P(E).$$
This would imply that $P(E) = 0$.  However, $$[0, 1] \subseteq \bigcup_{q \in [-1, 1]}E + q \Rightarrow P([0, 1]) = 1/3 \le \sum_{q \in[-1, 1]} P(E+q) = 0.$$  Hence, $P(E)$ cannot be defined.

The issue is the step where we pick $x_B$, since we need to pick $x_B$ from uncountably many points, which assumes the axiom of choice.  It was proved by Robert M. Solovay that all models of set theory excluding the axiom of choice have the property that all sets are Lebesgue measurable.

Our goal is thus to come up with a general framework where things can be consistently defined for a large class of sets.  
\subsection{Measure Theory Beginnings}  For the definitions, we take $\Omega$ to be the state space.
\begin{definition}[Sigma-Algebra]  Suppose $\Sigma$ follows the following properties:
\begin{enumerate}
\item $\emptyset \in \Sigma$
\item $A \in \Sigma \Rightarrow A^c \in \Sigma$
\item $A_1, A_2, \dots \in \Sigma$, then $\bigcup A_i \in \Sigma$
\end{enumerate}
Note that $2$ and $3$ imply $1$ since $(A \cup A^c)^c = \emptyset$.  Then $\Sigma$ is a sigma-algebra.
\end{definition}
Note that we also have countable intersections(this is an easy exercise).

\newpage
\section{September 1st, 2020}
Last time:
\begin{itemize}
 \item We discussed the notation of a $\Sigma$-algebra, a reasonable class of sets on which we will define measures.
 \item Properties: $\emptyset \in \SA, A \in \SA, A^c \in \SA, \bigcup A_i \in \SA$.
 \end{itemize} 
\subsection{Measures}
We are working in a space $(\Omega, \Sigma)$.  
\begin{definition}[Measure] A measure is a function $\mu: \Sigma \rightarrow [0, \infty]$ with the following properties:
\begin{itemize}
\item $\mu(\emptyset) = 0$
\item "Countable Additivity": $\mu(\bigcup A)i) = \sum \mu(A_i)$ for disjoint $A_i \in \Sigma$.
\end{itemize}
\end{definition}
\begin{example} If $\Omega$ is finite, ${1, 2, \dots, n}$, $\Sigma = 2^{\Omega}$, then all possible measures on $(\Omega, \Sigma)$ are given by fixing $a_1, a_2, \dots, a_n \in [0, \infty]$ and $\mu(A) = \sum_{i \in A} a_i$.
\end{example}
Properties of measures:
\begin{itemize}
\item Monotonicity: $A \subset B$, then $\mu(A) \le \mu(B)$. 
\begin{proof}
$B = A \cup (B\setminus A)$ and $B\setminus A \in \Sigma$, so $$\mu(B) = \mu(A) + \mu(B\setminus A) \ge \mu(A).$$
\end{proof}
\item Countable Subadditivity:  $A \subseteq \bigcup_{i=1}^\infty B_i$, then $\mu(A) \le \sum \mu(B_i)$.
\begin{proof}
We disjointify the $B_i$:  Define $C_1 = B_1, C_i = B_i \setminus B_{i-1}$.  Then
$$\mu(A) \le \mu(\bigcup C_i) = \sum \mu(C_i) \le \sum \mu(B_i).$$
\end{proof}
\item: Continuity from below:  If $A_i \uparrow A$, then $\mu(A_i) \uparrow \mu(A)$.
\begin{proof}
$A = A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus A_2) \dots$, so by countable additivity
$$\mu(A) = \sum_{i=1}^{\infty} \mu(C_i) = \lim_{n\rightarrow \infty} \sum_{i=1}^n \mu(C_i) = \lim_{n \rightarrow \infty} \mu(A_n).$$
\end{proof}
\item Continuity from above, if $A_i \downarrow A$,  and $\mu(A_1) < \infty$, then $\mu(A_i) \rightarrow \mu(A)$ 
\begin{proof}
We need the condition $\mu(A_i) < \infty$.  Take $A_i = [i, \infty)$ as a counterexample if we don't have that condition.

Define $A_1 \setminus A_i = B_i$, so $B_i \uparrow A_1 \setminus A$.  Then, use the continuity from below.  
\end{proof}
\end{itemize}
\subsection{Sigma algebras}
\begin{fact} For any $A \subset 2^{\Omega}$, define $$\Sigma(A) = \bigcap_{A \in \Sigma}\Sigma.$$ Then, $\Sigma(A)$ is a sigma-algebra.
\end{fact}
Note that $\Sigma(A)$ is the smallest sigma-algebra containing $A$.  For this reason, we call it the sigma-algebra \textbf{generated} by $A$.

\begin{example} Take $X, Y \subset 2^{\Omega}$.  We want to prove $\Sigma(X) = \Sigma(Y)$.  It suffices to show $X \subseteq \Sigma(Y)$ and $Y \subseteq \Sigma(X)$.  
\end{example}

\begin{definition}[Borel Sigma-Algebra] $(\Omega, \cU)$, a topological space with a family of open sets.  The \textbf{Borel Sigma-Algebra} is $\SB = \Sigma(\cU)$. 
\end{definition}

\begin{example} For $\Omega = \R$, $\SB$ is the sigma algebra generated by open sets in $\R$.  We also have $\SB$ is the sigma-algebra generated by open intervals in $\R$, which follows from the fact that any open set can be written as a countable union of open intervals.  Furthermore,
$$\Sigma((a, b): a, b\in \Q, \R) = \Sigma([a, b]: a, b \in \Q, \R),$$
since $[a, b] = \bigcap (a-1/n, b+1/n)$ and $(a, b) = \bigcup [a+1/n, b-1/n]$.
\end{example}

\subsection{Uniform Measure on the Borel Sets}
We will attempt to define the uniform measure on Borel sets of $\R$.  Broadly, we do it as follows:
\begin{enumerate}
\item Define it on a semi-algebra containing the intervals.
\item Extend the definition to an algebra.
\item Extend it to a sigma-algebra.
\end{enumerate}
\begin{definition}[Semi-algebra] $\Sigma \subset 2^\Omega$ is a semi-algebra if
\begin{itemize}
\item $A_1, A_2 \in \Sigma$ implies $A_1\cap A_2 \in \Sigma$
\item $A_1 \in \Sigma$ implies that $A_1^c = \bigcup_{i=1}^n B_i$ for $B_i \in \Sigma$.
\end{itemize}
\end{definition}
Note:  The set of intervals $\{(a, b): a, b \in \R\}$ is not a semi-algebra.  If $(a, b)^c = [b, \infty)$ which is not finitely coverable by disjoint open sets. Similarly, $\{[a, b]: a, b \in \R\}$ is not a semi-algebra.  

Claim: $\Sigma = \{(a, b]: a, b \in \R\}$ is a semi-algebra.  [This is left as an exercise].

Now, $\mu((a, b]) = b-a$. The proof that $\mu$ is countable additive on $\Sigma$.  If $A = \bigcup_{i=1}^{\infty} B_i$, $B_i$ disjoint, $A, B_i \in \Sigma$, then $\mu(A) = \sum_{i=1}^{\infty} \mu(B_i).$
\begin{proof}
We first show that $\mu(A) \ge \sum_{i=1}^{\infty} \mu(B_i)$.  This is an easy exercise, show $\mu(A) \ge \sum_{i=1}^{n}\mu(B_i)$, and we pass to the limit.

It suffices to show $\mu(A) \le \sum_{i=1}^{\infty} \mu(B_i)$.  We do this by exploiting compactness.

Let $A = (a, b] \supset [a+1/n, b] = A;$, take $B_i = (c_i, d] \subset c_i, d + \frac{\epsilon}{2^i} = B_i'$.  Note that $$A' \subset \bigcup_{i=1}^{\infty}B_i',$$
so there exists a finite subcover $A' \subset \bigcup_{j=1}^{k} B_{i_j}'.$  It is easy to show that $b - (a+1/n) \le \sum_{j=1}^{k} (d_{i_j}' - c_{i_j}')$. But note that 
$$\sum_{j=1}^{k} (d_{i_j}' - c_{i_j}') \le \sum_{j=1}^k d_{i_j} - c_{i_j} + \epsilon,$$
which implies that $$\mu(A) - 1/n \le \sum_{i=1}^{\infty}\mu(B_i) + \epsilon \Rightarrow \mu(A) \le \sum_{i=1}^{\infty} \mu(B_i).$$ 
\end{proof}
\begin{definition} $\SA$ is an algebra if 
\begin{itemize}
\item $\emptyset \in \SA$
\item $A_1 \in \SA$ implies $A^c \in \SA$
\item $A_1, \dots, A_n \in \SA$, then $\bigcup_{i=1}^n A_i \in A$.
\end{itemize}
The algebra generated by a semi-algebra is given by taking all possible disjoint finite unions.
\end{definition}
Claim: $\Sigma_a = \{\bigcup_{i=1}^n A_i\}$ for disjoint $A_i$ semialgebras is an algebra.
\begin{proof}
We show $A, B \in \Sigma_a \Rightarrow A\cup B \in \Sigma_a$ and $A^c \in \Sigma_a$.  
Note that $A = \bigcup_{i=1}^n C_i, B = \bigcup_{j=1}^k D_j$, so $$A \cap B = \bigcup_{i=1}^n \bigcup_{j=1}^k C_i \cap D_j,$$
and $C_i \cap D_j $ are disjoint.  Then $C_i, D_j \in \Sigma$ implies $C_i \cap D_j \in \Sigma$.

Then, if $A = \bigcup_{i=1}^k C_i$, then $A^c = \bigcap_{i=1}^k C_i^c$, and $C_i^c = \bigcup_{j=1}^\ell E_j \in \Sigma_a$.
\end{proof}

We extend $\mu$ to an algebra by $\mu(A) = \sum_{i=1}^k \mu(C_i)$, where $A = \bigcup C_i$ in the semi-algebra.
\pagebreak
\section{September 3rd, 2020}
Recall that we are aiming to define the uniform measure on $(\R, \SB)$.  

Last time:
\begin{enumerate}
\item We defined a \textbf{premeasure} on a semi-algebra, which $\Sigma_{semi} = \{(a, b]: -\infty \le a \le b \le \infty\}$, $\mu((a, b]) = b-a$ was countably additive.  
\item Extend $\mu$ to an algebra $\Sigma_a = $ disjoint union of elements of $\Sigma_{semi}$.  
\item For $A = \bigcup_{i=1}^k C_i \in \Sigma_a$, $$\mu(A) = \sum_{i=1}^k \mu(C_i).$$
\end{enumerate}
\subsection{Uniform Measure on the Borel Sets}

We first need show show $\mu$ is well defined.  Suppose $A = \bigcup_{i=1}^l C_i, \bigcup_{j=1}^\ell B_j$ for $C_i, B_i \in \Sigma_{semi}$.  We want
$$\mu(A) = \sum_{i=1}^k \mu(C_i) = \sum_{j=1}^{\ell} \mu(B_j).$$

Note that $C_i = \bigcup_{j=1}^{\ell} (C_i \cap B_j)$, which are all disjoint.  Similarly, $B_j = \bigcup_{i=1}^k (B_j \cap C_i)$, disjoint.  Thus, from the finite additivity of $\Sigma_{semi}$, we have
$$\sum_{i=1}^k \mu(C_i) = \sum_{i=1}^k \sum_{j=1}^\ell \mu(C_i \cap B_k) = \sum_{j=1}^\ell \mu(B_j),$$
as desired.

We will next show that $\mu$ is finitely additive additive.  First, if we have $A_1, A_2, \dots, A_n \in \Sigma_a$ disjoint, we show $\mu\left (\bigcup A_i\right ) = \sum \mu(A_i)$.

Note that each $A_i = \bigcup_{j=1}^{m_i} C_j^i$, which are disjoint, so 
$$\mu\left (\bigcup A_i\right ) = \mu \left ( \bigcup_i \bigcup_j C_j^i\right ) = \sum_{i=1}^n \sum_{j=1}^{m_i} \mu(C_j^i) = \sum_{i=1}^n \mu(A_i).$$

Next, we show $\mu$ is monotonic.  For $A, B \in \Sigma_a, A \subseteq B$, $B = A \cup (B \setminus A)$, so $$\mu(B) = \mu(A) + \mu(B \setminus A) \ge \mu(A).$$

We show countably additivity: Let $A = \bigcup_{i=1}^{\infty} \mu(A_i)$.  We need to show $\mu(A) = \sum_{i=1}^{\infty} \mu(A_i)$.  

We first show $\mu(A) \ge \sum_{i=1}^{\infty} \mu(A_i)$.  It suffices to show $\mu(A) \ge \sum_{i=1}^n \mu(A_i)$.  Since $\bigcup_{i=1}^n A_i \subseteq A$, monotonicity gives $\mu\left (\bigcup_{i=1}^n A_i\right ) \le \mu(A)$.

Next, we show $\mu(A) \le \sum_{i=1}^{\infty} \mu(A_i)$.  
First $A = \bigcup_{j=1}^k C_j$ for $C_j\in \Sigma_{semi}$, 
$A_i = \bigcup_{\ell}^{m_i} C_{\ell}^i$ for $C_{\ell}^j \in \Sigma_{semi}$.

Thus, $$\mu(A) = \sum_{j=1}^k \mu(C_j).$$
Hence, it suffices to show $\mu(C_j) \le \sum_{i=1}^{\infty} \mu(C_j \cap A_i),$ since 
$$\mu(A) = \sum \mu(C_j) \le \sum_{j=1}^k \sum_{i=1}^{\infty} \mu(C_j \cap A_i) = \sum_{i=1}^{\infty}\sum_{j=1}^k \mu(C_j \cap A_i).$$

Note that $C_j = \bigcup_{i=1}^\infty \bigcup_{\ell}^{m_i}C_j \cap C_\ell^i$, and we finish by using countable additivity for $\Sigma_{semi}$.

It suffices extend to $\Sigma(\Sigma_a)$ which is the sigma-algebra generated by $\Sigma_a$.  

\begin{thm}[Caratheodory's Extension Theorem] We have the following:
\begin{itemize}
\item Given a countably additive measure $\mu$ on an algebra $\Sigma_a$, it can be extended to a measure on $\Sigma(\Sigma_a)$.
\item If $\mu$ is $\sigma$-finite on $\Sigma_a$, the extension is unique.
\end{itemize}
A measure $\mu$ is $\sigma$-finite on $\Sigma_a$ if there exists $A_1 \subseteq A_2 \subseteq \dots \in \Sigma_a$ so that $\bigcup A_i = \Omega$, $\mu(A_i) \le \infty$ for all $i$.
\end{thm}
\begin{proof}
For example, consider $\Sigma_{semi} = \{(a, b] \cap \Q\}$.  Then $\Sigma = 2^\Q$.  The cardinality of every element in $\Sigma_{semi}$ is either $\infty$ or $0$.  Define $\mu(A) = \infty$ if $|A| = \infty$, else $0$.  One can check $\mu$ is a measure on $\Sigma_{semi}$.  We can also take the counting measure $\nu$.  This agrees on $\Sigma_{semi}$, but not on $\Sigma$.  We can check that $\nu$ is not sigma-finite.

We now show uniqueness, given $\sigma$-finiteness.  For simplicity, assume $\mu(\Omega) < \infty$.   If we have two measures $\mu_1, \mu_2$ on $\Sigma$ with $\mu_1(A) = \mu_2(A)$ for all $A \in \Sigma_a$, then we show $\mu_1(B) = \mu_2(B)$ for all $B\in \Sigma$.

A general strategy to show some property is true for a sigma-algebra is to show that the sets satisfying those properties must be closed under some natural operations and that any such family of sets must contain a sigma-algebra.
\begin{thm}[$\pi$ - $\lambda$] A class of sets $P$ is said to be a $\pi$-system if $A, B \in P$ implies $A \cap B \in P$.  A class of sets $G$ is said to be a $\lambda$-system if $\Omega \in G$, $A \subset B, A, B \in G$, then $B \setminus A \in G$, and $A_i \in G$ , $A_i \uparrow A \rightarrow A \in G$.  If $P$ is a $\pi$ system contained in $G$, a $\lambda$-system, then $\Sigma(P) \subset G$.
\end{thm}

Note that a semi-algebra is a $\pi$ system.  It suffices to consider the set $\mathscr{C} = \{A : \mu_1(A) = \mu_2(A)\}.$  We know that $\Sigma_{semi} \subset \mathscr{C}$.  To show $\Sigma \subset \mathscr{C}$, it suffices to show that $\mathscr{C}$ is a $\lambda$-system.  

Note that a sigma-algebra is a $\lambda$-system, so given any $\pi$-system $P$, $\Sigma(P)$ is the smallest $\lambda$-system containing $P$.

We have already verified $\Sigma_a \subset \mathscr{C}$. Furthermore, $\Omega \in \mathscr{C}$ because $\Omega$ is an algebra.  Finally, suppose we have $A \subset B$ , $A, B \in \mathscr{C}$.  We need $B \setminus A \in \mathscr{C}$.
$\mu_1(A) = \mu_2(A), \mu_1(B) = \mu_2(B)$ and $\mu(\Omega) < \infty$.  Since $\mu_1(\Omega) = \mu(\Omega) = \mu_2(\Omega) < \infty$,
$$\mu_1(B \setminus A) = \mu_1(B) - \mu_1(A) = \mu_2(B) - \mu_2(A) = \mu_2(B\setminus A).$$

For $A_i \uparrow A$, by continuity from below, $\mu_1(A_i) \rightarrow \mu_1(A), \mu_2(A_i) \rightarrow \mu_2(A)$, so $A \in \mathscr C$.
\end{proof}
An easy exercise is to modify the above prove to include the sigma-finite case.  The proof of the $\pi-\lambda$ theorem involves some set theory.  

We'll sketch the existence proof.  Suppose we have $\mu$ on $\Sigma_a$.  For example, take $B \subset \R$.  How do we define $\mu(B)$?  We could try to approximate $B$ by the union of intervals.

Define the outer measure, $\mu_*(B) = \inf \sum_{i=1}^{\infty}\mu(A_i)$ defined over covers of $B$.  Some properties are $\mu_*(B_1) \le \mu_*(B_2)$ if $B_1 \subseteq B_2$, $\mu_*(\emptyset) = 0$, and $\mu_*(\bigcup C_i) \le \sum_{i=1}^{\infty} \mu_*(C_i)$.

Define $\SA = \{A : \mu_*(E) = \mu_*(E \cap A) + \mu_*(E \cap A^C) \forall E\}.$  $\SA$ is a sigma algebra containing $\Sigma_a$ and $\mu_*$ is a measure when restricted to $\SA$.  
\pagebreak
\section{September 8th, 2020}
Last time, we completed the construction of the uniform measure on the Borel sets.  
\subsection{The Outer Measure}
On an algebra $\Sigma_a$,  let $\Sigma_a^\sigma$ be the elements formed by taking countable unions of elements of $\Sigma_a$.  Let $\Sigma_a^{\sigma \delta}$ contain countable intersections of elements of $\Sigma_a^\sigma$.  Notice that from the definition of an outer measure, for any set $B$, there exists a set $B' \in \Sigma_a^{\sigma \delta}$ such that $$B \subset B', \mu_*(B) = \mu_*(B').$$

This implies that $\mu_*(B' \setminus B) = 0$ and for every $N$ such that $\mu_*(N) = 0$, we can check that $N$ belongs to $\SA$.  Remark:  The construction defines the measure $\mu_*$ on sets of the form 
$A \cup B$, where $A$ is a Borel set and $\mu_*(B) = 0$.  It is easy to check that $\mu_*(B) = 0$ implies that there exists a Borel set $C$ such that $\mu_*(C) = 0$ and $B \subseteq C$.  Thus, we call it the completion of Borel sets.  This is a strictly larger sigma-algebra than the Borel sets, which follows from comparing cardinalities.  Namely, the cardinality of the Borel sets is $2^{\N_0}$.  Observe the Lebesgue sigma algebra contains $2^\text{Cantor Set} = 2^C$.

Meausres on the real line are characterized by distribution functions, which are non-decreasing right continuous functions $F$.  One can adopt the same strategy to define a measure on the real line by defining $\mu((a, b]) = F(b) - F(a)$.  Similarly, given $\mu$ on $(R, B(R))$, we can check that $F(b) = \mu((-\infty, b])$ is a distribution function. 

We can also consider $(\R^d, B(\R^d))$, the Borel sets on $\R^d$.  We claim that this is
$$\Sigma ((a_1, b_1) \times (a_2, b_2) \times \dots \times (a_d, b_d)) = \Sigma (B_1, B_2, \dots, B_d: B_i \in B(R)).$$

For distribution functions on $\R^d$, consider the lexigraphical partial order.  We would like them to satisfy,
\begin{itemize}
\item $F(x)$ is non-decreasing
\item $F(x)$ is right continuous: If $x_i \downarrow x$, then $F(x_i) \rightarrow F(x)$.
\item $F(x) \rightarrow 0$ as $x \downarrow -\infty$, $F(x) \rightarrow 1$ as $x \rightarrow \infty$.
\end{itemize}
The properties above are not actually enough to define a measure. (Consider the semibox in $\R^2$).  

However, for any $F$ such that $F(A) \ge 0$ for any $A \in \Sigma_{semi}$, the strategy to build a measure on $B(\R^d)$ from $\Sigma_{semi}\rightarrow \Sigma_a \rightarrow B(R^d)$ works.

\subsection{Functions Between Measure Spaces}
Suppose we have two measure spaces $(\Omega_1, \Sigma_1), (\Omega_2, \Sigma_2)$ and a function $f:\Omega_1 \rightarrow \Omega_2$.
\begin{definition}[Measurable Function] $f$ is said to be \textbf{measurable} if $f^{-1}(A_2) \in \Sigma_1$ for all $A \in \Sigma_2$.  If $(\Omega_2, \Sigma_2) = (\R, B(\R))$, then $f$ will be called a \textbf{random variable}.
\end{definition}

\begin{proposition} If $\Sigma_2 = \Sigma(\SA)$, then to check $f$ is measurable, it suffices to check $f^{-1}(B) \in \Sigma_1$ for all $B \in \SA$.
\end{proposition}
\begin{proof}
$$\Sigma' = \{B : f^{-1}(B) \in \Sigma_1\}$$ is a sigma-algebra: If $B \in \Sigma'$, then $B^c \in \Sigma'$, since $f^{-1}(B^c) = (f^{-1}(B))^c$.  $\Omega \in \Sigma'$ since $f^{-1}(\Omega_2) = \Omega_1$.  It is easy to check countable unions.  $\Sigma'$ is a sigma algebra containing $\SA$, so $\Sigma'$ contains $\Sigma(\SA).$  
\end{proof}

\begin{fact} If we have $f: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (\Omega_2, \Sigma_2)$, $f$ induces a measure $\mu_2$ on $(\Omega_2, \Sigma_2)$ where $\mu_2(B) = \mu(f^{-1}(B))$ for all $B \in \Sigma_2$.
\end{fact}

Some properties:
\begin{itemize}
\item If we have $f: (\Omega_1, \Sigma_1) \rightarrow  (\Omega_2, \Sigma_2), g:  (\Omega_2, \Sigma_2)\rightarrow (\Omega_3, \Sigma_3)$, then $h = g \circ f$ is measurable. 
\item If $X_1, X_2$ are two random variables, then $(X_1, X_2)$ is a measurable function into $(\R^2, B(\R^2))$.

We know that $B(\R^2) = \Sigma(I_1 \times I_2)$ for intervals.  Finally, $$(X_1, X_2)^{-1}(I_1 \times I_2) = X_1^{-1}(I_1) \cap X_2^{-1}(I_2)$$, so$ (X_1, X_2)^{-1}(I_1 \times I_2)$ is measurable.
\item Suppose $F$ is a continuous function from $(\Omega_1, B(\Omega_1)) \rightarrow (\Omega_2, B(\Omega_2))$. Then $F$ is measurable, since the preimage of open sets is open.
\item if $X_1, X_2, \dots, X_d$ is a random variable, then $X_1 + X_2 + \dots + X_d$ is a random variable.
\item If $f_n$ are random variables and $f_n \rightarrow f$ pointwise.  Then, $f$ is measurable.
\begin{proof}
Consider the set $\{f > x\} = \bigcup_{n=1}^{\infty}\bigcap_{m = n}^{\infty} \{f_m > x\}$ is measurable.  Then $(x, \infty)_{x \in \R}$ is a generating set.
\end{proof}
\item$ X: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (R, B(R))$ induces a measure $\mu$ on $(R, B(R))$, where $\mu(B) = \mu_1(X^{-1}(B))$ for all $B \in B(R)$.  It also induces a distribution function $F$, which is nondecreasing, right continuous, and $F(x) \uparrow 1$ and $x \rightarrow \infty$, $F(x) \downarrow 0$ as $x \rightarrow -\infty$.  
\end{itemize}

Given a distribution function, is there a random variable?  Given a distribution function, we can construct a measure on $\R$ by the Caratheodory Extension Theorem.  Let $I: (R, B(R), \mu) \rightarrow (R, B(R))$.  We could also take $X: ([0, 1], B([0, 1], \mu) \rightarrow (R, B(R))$, where $\mu$ is uniform.  Suppose $F$ is continuous and strictly monotone.  We want $X$ to induce the distribution $F$, so it suffices to show $X^{-1}(\infty, y) = [0, F(y)]$.  If we define $X(F(y)) = y,$, we get the above, but that's not always well defined.  For general distributions, one can come up with various definitions of an "inverse" which induces the desired properties.  One particular choice is
$$w \in (0, 1), X(\omega) = \sup\{y : F(y) < \omega\}.$$

It is an exercise to check that $\{\omega: X(\omega) < x\} = \{\omega : \omega \le F(x) \}$, which implies that $X$ is measureable.
\section{September 10th, 2020}
\subsection{Integration}
We work in $(\Omega, \Sigma, \mu)$ a sigma-finite measure space.  Often, we take it to be a probability measure.  The goal is to define a notion of integration for measurable functions and the behavior of integration with limits.  Consider a function $f: \Omega \rightarrow \R$ measurable with the Borel Sigma-algebra.  

Our strategy is as follows:
\begin{enumerate}
\item Consider simple functions.
\item Extend to bounded functions.
\item Extend to general functions.
\end{enumerate}
\subsection{Simple Functions}
\begin{definition}[Simple Function] Consider $f = 1_E$, where $\mu(E) < \infty$, an indicator function.  A \textbf{simple function} is a linear combination of indicator functions,
$$f = \sum_{i=1}^k c_i 1_{A_i}, \mu(A_i) < \infty,$$
where $A_i$ are disjoint.  

We'll define the integral of a simple function as 
$$\int f d\mu = \sum_{i=1}^k c_i \mu(A_i).$$
\end{definition}
First, note that any $f = \sum_{i=1}^k d_i 1_{B_i}$ can be represented as $\sum_{i=1}^k c_i 1_{A_i}$, where they are disjoint.  We verify that our definition is well defined.  Suppose
$$f = \sum_{i=1}^k c_i 1_{A_i} = \sum_{j=1}^m e_j 1_{F_j}.$$
Then, observe that for $i, j$ such that $A_i \cap F_j \ne \emptyset$, then $c_i = e_j$.  So, we have $f = \sum_{i, j} d_{i, j} 1_{A_i \cap F_j}$, and we can check that $d_{ij} = c_i = e_j$. 
Thus,
$$\sum_{i=1}^k c_i \mu(A_i) = \sum_{i, j} d_{ij}\mu(A_i \cap F_j) = \sum_{j=1}^m e_j \mu(F_j).$$

Some properties:
\begin{itemize}
\item If $f \ge 0$ then $\int f \ge 0$.
\item $\int af = a \int f$.
\item $\int (f+g) = \int f + \int g$.
\item If $g \le f$ then $\int g \le \int f$.
\item if $g = f$, $\int g = \int f$.
\item $|\int f| \le \int |f|.$ 
\end{itemize}
\subsection{Bounded Functions}
Suppose $|f| \le M$ and $f$ vanishes outside $E$ and $\mu(E) < \infty$.

We can approximate from above or below:
$$\sup \int_{g \le f} g \le \inf \int_{h \ge f} h$$
To prove equality, it suffices to show that there exists $g, h$ such that $\int h - \int g \le \epsilon$.  It suffices to construct $h$ such that $h-f < \epsilon$ and $f-g < \epsilon$.  Then $\int h - \int g \le 2 \epsilon \mu(E)$.

Note that the range of $f$ is $[-M, M]$, so we can discretize the interval into smaller intervals $I_1, \dots, I_k$ of size $\epsilon$.   

Then, define $A_1 = f^{-1}(I_1) \cap E$, and $f^{-1}(I_j) \cap E = A_j$.  Then.
 $$h = \sum ((j+1) \epsilon - M) 1_{A_j}, g = \sum j\epsilon - M)1_{A_j}.$$
 
 Thus, $\int f = \sup \int g = \inf \int h.$
 
 Observe that the new definition agrees with the old definition when $f$ is simple.
 
 As an exercise, we'll verify $\int f+g = \int f + \int g$.  Take Suppose $\int h_1 \ge \int f \ge \int g_1$ with $\int h_1 - g_1 < \epsilon$, and $\int h_2 \ge \int g \ge \int g_2$ with $\int h_2 - g_2 < \epsilon$.
 
 Then
 $$h_1 + h_2 \ge f+g \ge g_1 + g_2,$$
 and $$\int h_1 + \int h_2 < \int g_1 + g_2 + 2\epsilon = \int g_1 + \int g_2 + 2\epsilon.$$
 \subsection{General Functions}
 Assume $f \ge 0$.  Note that we can no longer approximate from above, so we approximate from below:
 $$\int f = \sup\{\int h, h \le f, \text{ bounded }\}.$$
 
Clearly, the definition agrees with the old one for bounded functions with finite support.

As an exercise, we'll prove that $\int f+g = \int f + \int g$.  If we have a bounded $h_1 \le f, h_2 \le g$, then $h_1 + h_2 \le f+g$, which implies that $\int h_1 + h_2 = \int h_1 + \int h_2 \le \int f+g$.  and $\sup \int h_1 + \sup \int h_2 = \int f+\int g$, so $\int f + \int g \le \int f+g$.

\begin{lemma} Suppose $E_n \uparrow \Omega$, $\mu(E_n)< \infty$.  Now, consider $(f \wedge n)1_{E_n}$, where $f\wedge n$ is the minimum of $f, n$.  Then, the function is bounded and has finite support.  Note that $(f \wedge n)1_{E_n}$.  We claim that
$$\int (f \wedge n)1_{E_n} \uparrow \int f.$$
\end{lemma}
\begin{proof}
It is clear that $$\lim \int (f \wedge n) 1_{E_n} \le \int f,$$
since $h_n = (f \wedge n)1_{E_n}$ is contained in the set of bounded functions for which the supremum is $\int f$.  

If suffices to show that $\lim \int (f \wedge n)1_{E_n} \ge \int f$.  Take $h$ bounded such that $\int f < \int g + \epsilon$.  There is a set $E$ such that $g \le M$ on $E$ and $g$ is $0$ on $E^c$.  

$g \le f$, so for any $n \ge M$, $h_n \ge g$ on $E_n \cap E$.
We claim that $$\int h_n \ge \int g - M \mu(E \setminus E_n).$$  Then $E_n \uparrow \Omega$, so $\mu(E \setminus E_n) \rightarrow 0$.
\end{proof}
Now, we conclude the original proof.  Note that 
$\int f+g = \lim \int((f+g)\wedge n)1_{E_n}$,
so $$\int ((f+g)\wedge n)1_{E_n} \le \int (f\wedge n) 1_{E_n} + \int (g \wedge n)1_{E_n}.$$
Taking limits gives the desired result.
\subsection{Arbitrary Measurable Functions}
Define $\int f$ only when $\int |f| < \infty$.  Define $$f = f^+ - f^-,$$
where $f^+ = f \vee 0, f^- = f \wedge 0$.

Then $$\int f = \int f^+ - \int f^-.$$
\begin{lemma} If $f_1, f_2$ nonnegative and $f = f_1 - f_2$, then
$$\int f = \int f_1 - \int f_2.$$
\end{lemma}
\begin{proof}
$f = f^+ - f^- = f_1 - f_2$, so $$f^+ + f_2 = f_1 + f^-,$$
then $$\int f^+ + \int f_2 = \int f_1 + \int f^-,$$
so $$\int f = \int f^+ - \int f^- = \int f_1 - \int f_2.$$
\end{proof}
\pagebreak
\section{September 15th, 2020}
\subsection{Properties of Integrals with Limits}
We assume for simplicity that our measure space $(\Omega, \Sigma, \mu)$ is finite.  Last time, we defined integrals for measurable functions starting with indicators, to simple functions, to non-negative functions, and finally to general functions.

Observe that if $f$ is $0$ almost surely, then $\int f = 0$.  Suppose $\{f_n\}$ is a set of measurable functions and $f_n \rightarrow f$ pointwise almost everywhere, then $\lim f_n$ is measurable.  In other words, there exists a set $E$ such that $\mu(E) = 0$ and $f_n$ converges on $E^c$.  Define $f$ to be $0$ on $E$ and $\lim f_n$ on $E^c$.  Note that $f$ is measurable.  Suppose $f_n$ "converge" to $f$.  When can one expect $\int f_n$ to converge to $\int f$?

\begin{definition}[Convergence in Measure] We say $f_n \rightarrow f$ in measure if given $\epsilon > 0$, $$\lim_{n \rightarrow \infty} \mu\left (|f_n - f| > \epsilon\right ) = 0$$  We denote this by $f_n \xrightarrow{\mu} f$.
\end{definition}
\begin{exercise} If $\mu(\Omega) < \infty$, then $f_n \rightarrow f$ almost everywhere implies that $f_n \xrightarrow{\mu} f$.
\end{exercise}
\begin{example} Suppose $f_n = 1_{[-n, n]}$ over $\R$.  Then $f_n \rightarrow f = 1_{\R}$, but $\mu(|f_n - f| > \epsilon) = \infty$.

Recall continuity of measure from below: If $A_n \uparrow A$ then $\mu(A_n) \uparrow \mu(A)$, but if $\mu(A) = \infty$, then $\mu(A) - \mu(A_n) = \infty$ for all $n$.  This doesn't happen for $\mu(\Omega) < \infty$.
\end{example}
\begin{example} If $f_n \xrightarrow{\mu} f$, then does $f_n \rightarrow f$ almost everywhere?  No:  Take $\Omega = [0, 1]$, $f = 0$ and $f_1 = 1_{[0, 1/n]}, f_2 = 1_{[1/n, 2/n]}, ..., f_n = 1_{[n-1/n, 1]}, f_{n+1} = 1_{[0, 1/(n+1)]}, ...$

There is always some interval where $f_n = 1$, so it does not converge pointwise to $0$.
\end{example}
\begin{example} If $f_n \xrightarrow{\mu} f$, then does $\int f_n \rightarrow \int f$?  No.
Take $f_n = \frac{1}{n}1_{[0, n]}$.  
\end{example}
\begin{thm}[Bounded Convergence Theorem] Suppose $\mu$ is finite and $f_n$ are supposed on $E$ such that $\mu(E)  < \infty$.

If $|f_n| < M$ and $f_n \xrightarrow{\mu} f$, then $\int f_n \rightarrow \int f$.
\end{thm}
\begin{proof}
$f$ must be $0$ almost everywhere outside $E$.  Define $F = \{|f_n - f| < \epsilon\}$.  Note that 
$$\left |\int_E f_n - \int_E f\right | \le \int_{F \cap E} |f_n - f| + \int_{E\cap F^c} |f_n - f|$$
$$\le \epsilon \mu(E) + 2M\mu(F^c \cap E) \xrightarrow{\epsilon \rightarrow 0} 0.$$
\end{proof}
\begin{thm}[Fatou's Lemma] If $f_n \ge 0$ then
$$\liminf_{n \rightarrow \infty} \int f_n d\mu \ge \int \left (\liminf_{n \rightarrow \infty} f_n\right )d\mu.$$
\end{thm}
\begin{proof}
Let $g_n(x) = \inf_{m\ge n}f_m(x)$.  Then $g_n(x) \uparrow g = \liminf f_n$.  

We know that $f_n \ge g_n$.  This implies that $\int f_n \ge \int g_n$.  We have that
$$\liminf \int f_n \ge \liminf \int g_n = \lim \int g_n.$$

Hence, It suffices to show that 
$$\lim \int g_n \ge \int g.$$

Recall that $g_n \uparrow g$ so $\lim g_n = g$.  Consider $g_n \wedge m$, a bounded function. Note that $g_n \wedge m \uparrow g \wedge m$, so by the Bounded Convergence Theorem,
$$\int (g_n \wedge m) \uparrow \int (g \wedge m).$$
Furthermore, we have 
$$\int g_n \ge \int (g_n \wedge m),$$
so $$\lim \int g_n \ge \lim \int (g_n \wedge m) \uparrow \int (g \wedge m) \uparrow \int g,$$
where the last inequality comes from approximation by bounded functions of finite support.
\end{proof}
\begin{thm}[Monotone Convergence Theorem] If $f_n \ge 0$ and $f_n \uparrow f$, then $\int f_n \uparrow \int f$.
\end{thm}
\begin{proof}
Note that $\int f \ge \lim_{n \rightarrow \infty} \int f_n$ since $\int f \ge \int f_n$.  Then $\int f \le \lim_{n \rightarrow} \int f_n$ by Fatou's lemma.
\end{proof}
\begin{thm}[Dominated Convergence Theorem] If $|f_n| \le g$ where $\int g \le \infty$ and $f_n \rightarrow f$ pointwise, then
$$\int f_n \rightarrow \int f.$$
\end{thm}
\begin{proof}
Note that $f_n + g \ge 0$, and $f_n + g \rightarrow f+g$ so by Fatou's lemma,
$$\liminf_{n \rightarrow \infty} \int f_n + g  \ge \int f+g d\mu,$$
which implies that $\liminf_{n \rightarrow \infty} \int f_n \ge \int f.$

Then, applying the result to $g-f_n$, we have $$\liminf_{n \rightarrow \infty} -f_n\ge \int -f \Rightarrow \limsup_{n \rightarrow \infty} f_n \le \int f,$$
which implies that $\lim \int f_n = \int f$, as desired.
\end{proof}

\subsection{Expected Value}
We have been discussing measurable functions, but these can easily be translated into the language of random variables.  Namely, if $X$ is a random variable and $\int |X| < \infty$, then $\int X = E(X)$, the expectation of $X$.

\begin{itemize}
\item $X_n \ge 0$, then $X_n \uparrow X \rightarrow E(X_n) \rightarrow E(X)$.
\item $|X_n| < Y$, $X_n \rightarrow X$, $E(Y) < \infty$, then $E(X_n) \rightarrow E(X)$.
\end{itemize}
\subsection{Change of measure for Integrals}
We have a random measurable map $$X: (\Omega_1, \Sigma_1, \mu_1) \rightarrow (\Omega_2, \Sigma_2) \xrightarrow{f} (\R, B(\R)).$$  
Then $f \circ X: (\Omega_1, \Sigma_1) \rightarrow (\R, B(\R))$, and if $\int |f \circ X| < \infty$, then $X$ induces a measure $\mu_2$ on $(\Omega_2, \Sigma_2)$ with $\mu_2(A) = \mu_1(X^{-1}(A)).$
Hence, we can discuss
$$\int_{\Omega_2}f d\mu_2.$$
\begin{thm}[Change of Measure]
$$\int_{\Omega_1} f \circ X d\mu_1 = \int_{\Omega_2} f d\mu_2.$$
\end{thm}
\begin{proof}
Let $f = 1_{E}$ for $E \in \Sigma_2$.
$$\int_{\Omega_2} f d\mu_2 = \mu_2(E) = \mu_1(X^{-1}(E)).$$
Then $f \circ X = 1(X^{-1}(E))$, so 
$$\int f \circ X = \mu_1(X^{-1}(E)).$$

For simple functions, we can use linearity of integrals for the result.  For nonnegative functions, we construct a monotone sequence of functions which increase to $f$.  One possible choice is 
$$f_n = \frac{\floor{2^n f}}{2^n} \wedge n.$$

We know that $f_n \uparrow f$ and $\int_{\Omega_1} f_n \circ X = \int_{\Omega_2} f_n$, and $f_n \circ X \uparrow f \circ X$, so by the monotone convergence theorem,
$$\int_{\Omega_1} f_n \circ X \rightarrow \int_{\Omega_1} f\circ X,$$
and $$\int_{\Omega_2} f_n \rightarrow \int_{\Omega_2} f,$$
so it follows that $\int_{\Omega_1}f \circ X = \int_{\Omega_2} f,$ as desired.
\end{proof}
\subsection{Product Measures}
We will relate high dimensional integrals with low dimensional ones with the notion of Product Measures.

Let $(\Omega_1, \Sigma_1, \mu_1), (\Omega_2, \Sigma_2, \mu_2)$ be measure spaces.  Consider $(\Omega_1 \times \Omega_2, \Sigma(\Sigma_1 \times \Sigma_2))$.  Note that $\Sigma_1 \times \Sigma_2$ is a semialgebra.  From here, we construct the product measure.
\begin{thm} There exists a unique measure on $\Sigma_p = \Sigma_1 \times \Sigma_2$, $\mu$ such that 
$$\mu(A \times B) = \mu_1(A) \mu_2(B)$$
for all $A \times B \in \Sigma_p$.  We will call this the product measure.
\end{thm}
\begin{proof}
Given a countable additive sigma-finite measure on a semi-algebra, it admits a unique extension to the generated sigma-algebra by Caratheodory's extension theorem.  To prove the existence and uniqueness, it suffices to check countable additivity on $\Sigma_1 \times \Sigma_2$.  

If we have $A \times B = \bigcup A_i \times B_i$, we want $$\mu(A \times B) = \mu_1(A)\mu_2(B) = \sum \mu_1(A_i)\mu_2(B_i).$$

Our strategy is to product to one dimension less. Fix $x \in A$.  Note that $B = \{y : (x, y) \in A \times B\}$.  But $A \times B = \bigcup A_i \times B_i.$  Consider all $A_i$'s that contain $x$.  Then, the corresponding $B_i$'s form a disjoint partition of $B$.  We know that $\mu_2(B) = \sum_{x \in A_i} \mu_2(B_i)$ for all $x \in A$, so in particular
$$1_A\mu_2(B) = \sum 1_A \mu_2(B_i).$$
We claim that the two functions are pointwise same on $\Omega_1$.  Then, we integrate (uses MCT) with respect to $\mu_1$ to get
$$\mu_1(A)\mu_2(B) = \lim_{n \rightarrow \infty} \sum_{i=1}^n \mu_1(A_i)\mu_2(B_i).$$
\end{proof}
\pagebreak
\section{September 17th, 2020}
\subsection{Product Measures, Continued}
We have a semialgebra $\Sigma_1 \times \Sigma_2$ and we denote $\Sigma_{1 \times 2} = \Sigma(\Sigma_1 \times \Sigma_2)$.  Let $E \in \Sigma_{1 \times 2}$.
\begin{lemma} For any $X \in \Omega_1$, the set
$$E_x = \{y \in \Sigma_2 : (x, y) \in E\}$$
is measurable.
\end{lemma}
\begin{proof}
If $E = E_1 \times E_2$, then either $E_x = \emptyset$ or $E_x = E_2$.  We show that the set of $E$ with this property forms a sigma-algebra.  If $E \in \mathcal{A}$, then $(E^c)_x = (E_x)^c$ so $E^c \in \mathcal{A}$.  

If $E_1, E_2, \dots \in \mathcal A$, then
$$\left (\bigcup_{i=1}^\infty E_i\right )_x = \bigcup_{i=1}^\infty (E_i)_x.$$

Hence, $\mathcal{A}$ is a sigma-algebra containing $\Sigma_1 \times \Sigma_2$, so $\mathcal{A} = \Sigma_{1 \times 2}$.
\end{proof}

\begin{thm} For any $E \in \Sigma_{1 \times 2}$, 
$$\mu_1 \times \mu_2(E) = \mu(E) = \int_{\Omega_1} \mu_2(E_x)d\mu_1,$$
and $\mu_2(E_x)$ is a measurable function from $\Omega_1 \rightarrow \R$.
\end{thm}
\begin{proof}
The result is clear for rectangles.  If $E_1, E_2 \in \mathcal{A}$, then $\mu_2(E_x) = \mu_2(E_{1x}) + \mu_2( E_{2x}) - \mu((E_{1} \cap E_{2})_x)$  We use the $\pi-\lambda$ theorem.  It suffices to show that $\mathcal A$ is a $\lambda$-system and we have that $A \supset \Sigma_1 \times \Sigma_2$, which is a $\pi
$-system.

It is clear that $\Omega_1 \times \Omega_2 \in \mathcal A$.  We claim that if $E_n \in \mathcal A$, then $E_n \uparrow E$ implies $E \in \mathcal A$.  
Note that $(E_n)_x \uparrow E_x$ for all $x$ then $\mu_2(E_{nx}) \uparrow \mu_2(E_x)$ and if we define $\mu_2(E_{nx}) = f_n(x)$, then $f_n(x) \uparrow E_x$ is measurable, as desired.  
Finally, we show that if $E_1 \supset E_2$ and $E_1, E_2 \in \mathcal A$, then $E_1 \setminus E_2 \in \mathcal A$.  This is clear since $(E_1 \setminus E_2)_x = E_{1x} \setminus E_{2x}$ so $\mu((E_1 \setminus E_2)_x) = \mu_2(E_{1x}) - \mu_2(E_{2x})$ is measurable as the difference of finite measurable functions.

The same argument shows that $\mathcal A$ is a $\lambda$-system if we define $\mathcal A$ to be all $E$ so that both conclusions of the theorem hold. 

\end{proof}
\begin{thm}[Fubini] Let $f \ge 0$ or $\|f\|_1 < \infty$.  Then 
\begin{enumerate}
\item For all $x$, $f(x, \cdot)$ is measurable on $\Sigma_2$.
\item $\int f(x, \cdot)$ is measurable on $\Sigma_1$.
\item $\int \int f(x, \cdot) = \int f$.
\end{enumerate}
\end{thm}
\begin{proof}
We have verified this for $f = 1_E$.  Suppose $f \ge 0$.  By linearity of integrals and the fact that the sum of measurable functions is measurable, the claim holds for simple functions.  For general $f \ge 0$, take a sequence of simple functions $f_n \uparrow f$.  

Then,
$$\int \left( \int f_n(x, \cdot) d\mu_2 \right )d\mu_1 = \int f_n d(\mu_1\times \mu_2),$$
so the result follows from the monotone convergence theorem.

For general $f \in L^1(\R)$, $f = f^+ - f^-$, so we use the above to conclude.
\end{proof}
\begin{example}[Not-integrable Function] Let $\Omega_1 = \Omega_2 = \N$.  Suppose $\mu_1, \mu_2$ are counting measures. Let $f(m, m) = 1$, $f(m+1, m) = -1$ and $f(m, n) = 0$.  Then 
$$\sum_m \sum_n f(m, n) = 1, \sum_n \sum_m f(m, n) = 0.$$
The failure is that $f \not \in \ell^1$.
\end{example}
\begin{example}[Not $\sigma$-finite] Let $\Omega_1 = \Omega_2 = (0, 1)$.  Let $\mu_1$ be the uniform measure and $\mu_2$ be the counting measure.  Let $E = \{(x, x): x \in (0, 1)\}$.   Then $\int \int \mu_2(E_x) d\mu_1 = 1$, but $\int \int \mu_1(E_y) d\mu_2 = 0$. 
\end{example}
\subsection{Independence}
\begin{definition}[Naive Independence] If $X_1, X_2$ are random variables, then $X_1$ and $X_2$ are independent if $$P(X_1 \in E, X_2 \in F) = P(X_1 \in E)P(X_2 \in F).$$
\end{definition}
We will generalize this notion.
\begin{definition}[Independence] For a $(\Omega, \Sigma, \mu)$, if $\Sigma_1, \Sigma_2, \dots, \Sigma_k \subset \Sigma$ are said to be 
\textbf{mutually independent} if for any subset $\{i_1, i_2, \dots, i_\ell\} \subset \{1, \dots, k\}$ and sets $A_{i_1}, A_{i_2}, \dots, A_{i_\ell}$, $A_{i_j} \in \Sigma_{i_j}$ 
$$\mu(A_{i_1} \cap \dots \cap A_{i_\ell}) = \prod \mu(A_{i_j}).$$

This is the same as the condition $$\mu(A_1 \cap A_2 \cap \dots A_k) = \prod_{i=1}^k \mu(A_i),$$
since we take some of the $A_i = \Omega$.
\end{definition}
\begin{definition}[Independent Random Variables] $X_1, \dots, X_k$ are mutually independent of $\{\Sigma(X_i)\}$ are mutually independent.
\end{definition}
\begin{thm} Suppose $A_1, A_2, \dots \subset \Sigma$ are mutually independent $\pi$-systems.  Then $\Sigma(A_i)$ are also mutually independent.
\end{thm}
\begin{proof}
Wlog, we can assume $\Omega \in A_i$ for all $i$.  Fix $B_2 \in A_2, \dots, B_\ell \in A_\ell$.  For $B_1 \in \Sigma(A_1)$, define the two measures $\mu', \mu''$ as 
$$\mu'(B_1) = \mu(B_1 \cap B_2 \cap \dots\cap B_\ell),$$
$$\mu''(B_1) = \mu(B_1) \prod_{i=2}^\ell \mu(B_i).$$  

We claim that $\mu' = \mu''$.  Observe that $\mu'$ and $\mu''$ agree on $A_1$ by hypothesis, so the claim holds by the uniqueness part of the Caratheodory Extension theorem on $\Sigma(A_1)$.

$\Sigma(A_1), A_2, \dots, A_\ell$ are mutually independent $\pi$ systems.  We iterate to get that $\Sigma(A_i)$ are mutually independent.
\end{proof}
\begin{example}[Pairwise Independent $\ne$ Mutually Independent]  Take $X_1, X_2, X_3 \in \{0, 1\}$,  Pick $(X_1, X_2, X_3)$ uniformly from all triples $(x_1, x_2, x_3)$ such that $x_1 + x_2 + x_3 = 0 \pmod 2$.  Note that $P(X_i = 1) = P(X_i = 0) = 1/2$.  It is clear that $(X_i, X_j)$ are independent, but $(X_1, X_2, X_3)$ are not independent since $P((X_1, X_2, X_3) = (1, 1, 1)) = 0 \ne (1/2)^3$.
 \end{example}
\begin{thm}[Kolmogorov's 0-1 Law]
 Suppose $X_1, X_2, \dots$ are independent random variables.  Consider $$T_n = \sigma(X_n, X_{n+1}, \dots),$$
 and let $$T = \cap_{n=1}^{\infty} T_n$$
 (this is known as a tail-sigma algebra).  Then $T$ is a $\mu$-trivial sigma algebra: for all $E \in T$, $\mu(E) = 0$ or $1$.
 \end{thm}
 \begin{proof}
 The idea is $E$ is independent of $X_1, \dots, X_{n-1}$, so $E$ is independent of $\sigma(X_1), \sigma(X_2), \dots, \sigma(X_{n-1})$.  Hence $E$ is independent of $\bigcap_{i=1}^{n-1}\sigma(X_i)$, so $E$ is independent of $\bigcap_{i=1}^{\infty} \sigma(X_i)$, so $E$ is independent of $\Sigma(X_1, X_2, \dots)$.  But $E \in T \subset \Sigma(X_1, \dots)$, so $P(E \cap E) = P(E)P(E) = P(E)$, so $P(E) = 0$ or $1$.
 
 Claim: If $A_{ij}$ for $j=1, \dots, m_i$ such that $A_{ij}$ are all $\pi$-systems containing $\Omega$ are mutually independent, then $\Sigma(A_{i1}, A_{i2}, \dots, A_{im_i})$ are also mutually independent.  To prove this, let $A_i = \{B_1 \cap B_2 \cap \dots \cap B_{m_i}:B_j \in A_{ij}\}$.
 
 We know that $\Sigma(X_1), \dots $ are independent $\pi$ systems,so $\Sigma(X_1, \dots, X_n)$ and $\Sigma(X_{n+1}, \dots)$ are independent.  Hence $E$ is independent of $\bigcap_{i=1}^\infty \Sigma(X_i)$, so $E$ is independent of $T$, which gives the result. 
 \end{proof}
\begin{thm}[Kolmogorov Extension] Take $(\R^n, B(\R^n), \mu_n)$ a consistent family of measures on $\R^n$: for $A \in B(\R^n)$
$$\mu_{n+1}(A \times \R) = \mu_n(A).$$
Then there exists a measure $\mu$ on $(\R^\N, B(\R^\N))$ such that $\mu$ agrees with $\mu_n$ on $\R^n \times \R \times \R \times \dots$.

\end{thm}
\pagebreak
\section{September 22nd, 2020}
Last time, we discussed product measures, independent random variables/sigma algebras, and how to construct infinitely many independent random variables.  We also proved the $0-1$ law for tail-sigma algebras.

If we have $(\Omega, \Sigma, \P)$ and random variables $X_1, X_2, \dots$, $T_n = \Sigma(X_n, X_{n+1}, \dots)$ and $T_\infty = \bigcap T_n$ is a sigma algebra that is P-trivial.

Any event that does not depend on any finite set of $X_i$'s is in the tail-sigma algebra.  For example, let $Y = \limsup X_i$ and $E = \{Y < t\}$.  Note that $Y$ does not depend on finitely many $X_i$'s.  Another example is $S_n = \sum_{i=1}^n x_i$ and we define $Y = \limsup \frac{S_n}{n}$.

When does $\frac{S_n}{n}$ have a limit?

\subsection{Law of Large Numbers}
We have $X_1, X_2, \dots$ independent random variables.  What is the asymptotic behavior of $\frac{S_n}{n}$?


Suppose $X_1, X_2, \dots$ have $E(X_i^2) < C$, $E(X_iX_j) = 0$ and $E(X_i) = 0$.  Then, 
$$\frac{S_n}{n} \xrightarrow{\P} 0 \Leftrightarrow \P\left (|\frac{S_n}{n}| > \epsilon \right ) \rightarrow 0.$$ 
\begin{proof}
We first note Markov's Inequality: Suppose $X$ is a nonnegative random variable.  For any positive $c$, 
$$P(X > c) \le \frac{E(X)}{c}.$$

Furthermore, note that $$\left \{ \left| \frac{S_n}{n}\right| > \epsilon\right \} = \left \{\left (\frac{S_n}{n}\right )^2 > \epsilon^2\right \}.$$

By Markov's Inequality,
$$\P((\frac{S_n}{n})^2 > \epsilon^2) \le \frac{1}{n^2\epsilon^2} E(S_n^2),$$
and finally,
$$E(S_n)^2 = E((X_1 + \dots X_n)^2)=\sum EX_i^2 + \sum E(X_iX_j) \le nC$$
so 
$$\frac{1}{n^2\epsilon^2} E(S_n^2) \le \frac{nC}{n^2\epsilon^2} = \frac{C}{n\epsilon^2} \rightarrow 0.$$
\end{proof}
\begin{corollary}If $X_1, X_2, \dots$ are independent with the same distribution and $E(X_i) = \mu$ $E(X_i^2) = \sigma^2$, then 
$$\frac{S_n}{n} \xrightarrow{\P} \mu.$$
\end{corollary}
\begin{proof}
Note that $E(\overline{X_i}\overline{X_j}) = E(\overline{X_i})E(\overline{X_j}) = 0$ by Fubini's theorem.  Hence we apply the previous theorem to $\bar{X_i} = X_i - \mu$.
\end{proof}
\begin{fact} Chebyshev's Inequality: For any RV X, 
$$P(|X| > t) \le \frac{E(X^2)}{t^2}.$$
\end{fact}
\begin{example}[Polynomial Approximation]
Task: Given $f:[0, 1] \rightarrow \R$ continuous, and $\epsilon > 0$, find a polynomial $f_n(x)$ such that 
$$|f_n(x) - f(x)| < \epsilon$$
for all $x \in [0, 1]$.

Let $$f_n(x) = \sum_{m=0}^n \binom{n}{m}x^m(1-x)^{n-m}f\left (\frac{m}{n}\right ).$$
We expect $f_n(x) \approx f(x)$ by the Binomial Theorem.  Precisely,
$$f_n(x) = E\left (f\left (\frac{S_n}{n}\right )\right )$$
where $S_n \sim Bin(n, x)$ with $S_n = \sum_{i=1}^n X_i$ for $X_i \sim Ber(x)$.  It suffices to show that $\frac{S_n}{n} \approx x$.

By the Law of Large Numbers,
$$P\left (\left |\frac{S_n}{n} - x\right | > \epsilon\right ) \rightarrow 0.$$

Since $f$ is continuous on $[0, 1]$, it is uniformly continuous, so that given $\delta$, there exists $\epsilon$ such that for all $x, y$ with $|x - y| < \epsilon$, $|f(x)- f(y)| < \delta$.  If we let the event above be $A^c$, then, 
\begin{align*}
E(f(S_n/n)) &= E(f(S_n/n)1_A) + E(f(S_n/n)1_{A^c}) \\
&= f(x)P(A) + E(f(S_n/n) - f(x))1_A) + E(f(S_n/n))1_{A^c} \\
& \le f(x)P(A) + \delta P(A) + \sup_{x \in [0, 1]} f(x)P(A^c)
&\rightarrow f(x).
\end{align*}

Note that $$P(A^c) \le \frac{\text{Var}(S_n)}{n^2 \epsilon^2} \le \frac{1}{n\epsilon^2}$$
since $\text{Var}(X_i) \le 1$ for $X_i \in [0, 1]$.

Hence, for any $x \in [0, 1]$, $f_n(x) \rightarrow f(x)$ uniformly as $n \rightarrow \infty$.
 \end{example}
 
Now, our goal is to prove the law of Large Numbers without the second moment assumption.  Namely, for $X_1, X_2, \dots$ iid with $E|X_i| < \infty$, $EX_i = 0$,
$$\frac{S_n}{n} \xrightarrow{P} 0.$$
Our strategy is truncation.  
\begin{definition} For any random variable $X$, we will consider the random variable from $X_M = X1_{|X| < M}$. Note that we have $E(X_M^2) < \infty$ for all $M$ even if $E(X^2) = \infty$.
\end{definition}
\begin{thm} Suppose that for each $n$ there exists a constant $b_n$ such that 
$$\sum_{i=1}^n P(|X_{n_i}| > b_n) \rightarrow 0$$
and $$\sum_{i=1}^n\frac{ E(\overline{X_{n_i}})^2}{b_n^2} \rightarrow 0.$$

Then
$$\sum_{i=1}^n\frac{ X_{n_i} - E(\overline{X_{n_i}})}{b_{n}} \rightarrow 0.$$
\end{thm}
\begin{proof}
We first prove that 
$$Y = \sum_{i=1}^n \frac{\overline{X_{n_i}} - E(\overline{X_{n_i}})}{b_n} \rightarrow 0.$$
This follows from Chebyshev, since $E(Y) = 0$ and 
$$\text{Var}(Y) \le \sum_{i=1}^n\frac{E(\overline{X_{n_i}}^2)}{b_n^2}.$$

Then $\sum_{i=1}^n P(|X_{n_i} > bn|) \rightarrow 0$ so if $X_{n_i} < b_n$, $X_{n_i} = \overline{X_{n_i}}$.

Let $B = \{X_{n_i} \ne \overline{X_{n_i}} : i \in \{1, \dots, n\}\}.$  Then $$P(B) \le \sum_{i=1}^n P(|X_{n_i}| > b_n) \rightarrow 0,$$
so it follows that 
$$\sum_{i=1}^n\frac{ X_{n_i} - E(\overline{X_{n_i}})}{b_{n}} \rightarrow 0.$$
\end{proof}

\begin{lemma}
Suppose $X_1, X_2, \dots$ are iid.  Suppose that $$KP(|X_1| > K) \rightarrow 0.$$

Then
$$\frac{\sum_{i=1}^n X_i - n E(X_1 1\{|X_1| < n\})}{n} \rightarrow 0$$
in measure.
\end{lemma}
\begin{proof}
Note that this does not imply $E(X_1) < \infty$.  Form a triangular sequence from the $X_i$'s and let $b_n = n$. 
 We show that $\sum_{i=1}^n P(|X_i| > n) \rightarrow 0$ and $\sum_{i=1}^n E(\overline{X_i}^2) \rightarrow 0$.

For 2, it suffices to show
$$E(\overline{X_i}^2)/n \rightarrow 0.$$

Note that $|\overline{X_i}|= |X_i| 1\{|X_i| < n\}.$  Suppose $X$ is a non-negative random variable. 
Note that 
$$E(X) \approx \sum_{n=1}^{\infty} P(X > n).$$

Similarly,
$$E(X^2) \approx \sum_{n=1}^{\infty} nP(X > n).$$

Then 
$$E(\overline{X_i}^2) \approx \sum_{K = 1}^n KP(X_1 > K).$$

It suffices to show that 
$$\frac{\sum_{k=1}^n kP(|X_1| > k)}{n} \rightarrow 0,$$
which follows from the fact that $kP(|X_1| > k) \rightarrow 0$.
\end{proof}

\begin{thm}[Law of Large Numbers] If $X_1, X_2, \dots$ iid and $E(|X_1|) < \infty$ and $E(X_1)  = 0$, then $S_n/n \rightarrow 0$ in measure.
\end{thm}
\begin{proof}
Note that $kP(|X_1| > k)  \le E(|X_1| 1\{|X_1| > k\}) \rightarrow 0$, by the dominated convergence theorem. By the lemma,
$$\frac{S_n}{n} - E(\overline{X_1}) \rightarrow 0,$$
and note that $E(\overline{X_1}) \rightarrow E(X_1) = 0$ by the dominated convergence theorem.
\end{proof}
\pagebreak
\section{September 24th, 2020}
\subsection{Law of Large Numbers, continued}
Last time, we began discussing the Law of large numbers.  Recall:
\begin{itemize}
\item Markov's Inequality: $$P(|X| > c) \le \frac{E(|X|)}{c}.$$
\item With $X_1, X_2, \dots$ iid, $E(X_i) = 0$.  When $E(X_1^2) < c$, 
$$S_n/n \xrightarrow{P} 0.$$
\item Under 1st Moment condition, we used truncation to make thinks bounded and have second moments.  We discussed triangular arrays and saw a theorem which proves a LLN type of statement for truncated variables.
\item We showed that the truncation has no limiting effect.  Then, we considered 
$$\sum X_i 1_{|X_i| < n}/n \rightarrow 0,$$
which implied the law of large numbers.
\end{itemize}
\begin{example} Let $X_1, X_2, \dots$ be iid with $X_i \ge 0$.  Suppose $E(X_1) = \infty$.  Then
$$\frac{\sum_{i=1}^n X_i}{n} = ?$$
Let $Y_i \sim X_i 1_{|X_i| < M}$.  Then $S_n'/n \sum_{i=1}^n Y_i/n \rightarrow E(Y_i)$ by the weak law of large numbers.  But by nonnegativity, $S_n/n > S_n'/n \rightarrow E(Y_i)$, but $E(Y_i)$ can be made arbitrarily large by choosing $M$ very large.  

For any $c$,
$$P\left (\frac{S_n}{n} > c\right ) \rightarrow 1,$$
so $S_n/n \rightarrow \infty$. 
\end{example}
\begin{example} Let $X= 2^i$ with probability $1/2^i$ for $i \ge 1$.  Note $E(X) = \infty$.

Let $X_1, X_2, \dots$ be iid $X$.  What is the growth rate of $S_n$?   One expects to see some $X_i$'s take value comparable to $n$ since $P(X_1 = n) = \frac{1}{n}$.  

We will control the growth with truncation.  Let $\alpha_n = \log n + k(n)$, $b_n = 2^{\alpha_n}$.  We need to show that 
$$\sum_{i=1}^n P(X_i > b_n) \rightarrow 0,$$
and $$\frac{\sum_{i=1} ^nE(X_i^2 1_{X_i < b_n})}{b_n^2} \rightarrow 0.$$

Note that $$P(X_i > b_n) \approx \frac{1}{b_n} = \frac{1}{n2^{k(n)}},$$
so $$\sum P(X_i > b_n) = \frac{1}{2^{k(n)}} \rightarrow 0.$$

Then,
$$E(X_i^2 1_{X_i < b_n})\approx \sum_{i=1}^{\alpha(n)}2^{2i}/2^i = \sum_{i=1}^{\alpha(n)}2^i \approx 2^{\alpha(n)} = b_n.$$

Then,
$$\frac{\sum_{i=1} ^nE(X_i^2 1_{X_i < b_n})}{b_n^2} \approx \frac{nb_n}{b_n^2} = \frac{1}{2^{k(n)})} \rightarrow 0.$$

Therefore,
$$\frac{S_n - nE(\overline{X_i})}{b_n} \rightarrow 0.$$

Note that $E(\overline{X_i}) = \alpha(n)$, so 
$$\frac{S_n - n(\log n +k(n))}{n2^{k(n)}}$$

If we choose $\log \log n$, then
$$\frac{S_n - n(\log n + \log \log n)}{n \log n} \rightarrow 0,$$
so $$\frac{S_n}{n\log n} \rightarrow 1 \Longrightarrow S_n = \Theta(n\log n).$$
\end{example}
\subsection{Almost Sure Convergence}
Let $X_1, X_2, \dots$ iid, $E(X_i) = 0$, $E(X_i^2)< C$.

We know that $$\frac{S_n}{n} \xrightarrow{P} 0,$$
but do we have $$\frac{S_n}{n} \rightarrow 0,$$
almost surely?

\begin{lemma}[Borel-Cantelli] If events $E_i$ satisfy $\sum_{i=1}^{\infty} P(E_i) < \infty$, then $P(E_i \text{ infinitely often}) = 0$.
\end{lemma}
\begin{example}
Let $\epsilon > 0$.  We want
$$P\left (\left |\frac{S_n}{n}\right | > \epsilon,  i. o. \right )  = 0.$$

In order to apply BC, we have to show 
$$\sum P\left (\left |\frac{S_n}{n}\right | > \epsilon \right )<\infty,$$
but 
$$\sum P\left (\left |\frac{S_n}{n}\right | > \epsilon \right ) \approx \frac{1}{\epsilon^2 n} \rightarrow \infty.$$

We try to get around this by assuming a higher moment.  Suppose $E(X^4) < \infty$.  

Then,
$$\frac{E(S_n^4)}{n^4} = \frac{E((\sum_{i=1}^n X_i)^4)}{n^4}=\frac{nE(X_1^4) + n^2 E(X_1^2X_2^2)}{n^4}\approx \frac{1}{n^2}.$$
So 
$$P\left (\left |\frac{S_n}{n}\right| > \epsilon\right ) \le \frac{1}{\epsilon^4}E((S_n/n)^4) \approx \frac{1}{\epsilon^4 n^2},$$
which gives that 
$$\sum P\left (\left |\frac{S_n}{n}\right| > \epsilon\right ) < \infty.$$
\end{example}

Can one use naive Markov to show a subsequence converges?  

If we let $K(n) = n^2$, 
$$P(|\frac{S_{k(n)}}{k(n)} |> \epsilon) \approx \frac{1}{n^2}$$
so we can take the infinite sum and it approaches $0$. 

Define $$D(n) = \sup_{k(n) \le i \le k(n+1)} |S_i - S_{k(n)}|.$$

It suffices to show that $$\frac{D(n)}{k(n)} \rightarrow 0.$$

We know that 
$$P(|D_n/k(n)| > \epsilon) \le \sum_{i=k(n)}^{k(n+1)}P\left (\frac{|S_i - S_{k(n)}|}{k(n)}> \epsilon\right ),$$
by subadditivity.  By Chebyshev,
$$\sum_{i=k(n)}^{k(n+1)}P\left (\frac{|S_i - S_{k(n)}|}{k(n)}> \epsilon\right ) \le \sum\frac{i-k(n)}{k(n)^2 \epsilon^2} \le \frac{(k(n+1) - k(n))^2}{2k(n)^2} \approx  \frac{1}{n^2} ,$$
so $\frac{D_n}{k(n)} \rightarrow 0$ almost surely by BC.
\pagebreak
\section{September 29th, 2020}
Recall from last time:
\begin{itemize}
\item Weak law of Large Numbers, using triangular arrays and truncation,
\item The Borel-Cantelli Lemma,
\item The Strong Law of Large Numbers, assuming 4th moments with Markov, and assuming 2nd moments, we proved convergence along a subsequence and controlled oscillations.
\end{itemize}
\begin{fact} $X_n \rightarrow X$ in probability if and only if for any sequence of $X_n$, there exists a subsequence which converges almost surely.
\end{fact}
Today, we will prove the most general version of SLLN, under the first moment assumption.
\subsection{General Law of Large Numbers}
Let $E_1, E_2, \dots$ be pairwise independent events, where $p_i = P(E_i)$.  Assume that $\sum P_i \rightarrow \infty$.  We have $S_n = \sum_{i=1}^n 1_{E_i}$, and we would like to consider $\frac{S_n}{E(S_n)}$.  We claim that 
$$S_n/E(S_n) \rightarrow 1,$$
almost surely.
\begin{proof}
Let $a_n = \{|S_n - E(S_n)| > \epsilon E(S_n)\}$.  We want to bound $P(a_n)$.  It suffices to prove that for any $\epsilon > 0$, $\sum a_n < \infty$, by the Borel-Cantelli lemma.

Note that
 $$P(|S_n - E(S_n)| > \epsilon E(S_n)) \le \frac{\text{Var}(S_n)}{\epsilon^2 (E(S_n))^2},$$
 and $$\text{Var}(S_n) = \sum_{i=1}^n \text{Var}(1_{E_i}) = \sum_{i=1}^n p_i(1-p_i) \le 1.$$
 Hence, $\text{Var}(S_n) \le E(S_n)$ and 
 $$a_n \le \frac{E(S_n)}{\epsilon^2(E(S_n))^2} = \frac{1}{\epsilon^2} \frac{1}{E(S_n)}.$$
 
 Denote $E(S_n) = g_n$.  Let $k(n)$ be the least element such that $g_{k(n)} \ge n^2$.  
 
 We have that $$S_{k(n)}/g_{k(n)} \rightarrow 1,$$
 almost surely, by applying Borel-Cantelli.  It suffices to control the error between the subsequence.
 
 Let $k(n) \le m \le k(n+1)$.  We would like to show that 
 $$S_m/g_{m} \rightarrow 1.$$
 But notice that $S_{k(m)} \le S_m \le S_{k(m+1)}$ since indicator functions are nonnegative.  So,
 $$S_{k(n)}/g_{k(n+1)} \le S_m/g_m \le S_{k(n+1)}/g_{k(n)} = \frac{S_{k(n+1)}}{g_{k(n+1)}}\frac{g_{k(n+1)}}{g_{k(n)}} \approx\frac{S_{k(n+1)}}{g_{k(n+1)}}\frac{(n+1)^2}{n^2} \rightarrow 1.$$
 We can have a similarly bound for the bottom term, and the result follows from the squeeze theorem.
\end{proof}

\begin{thm}[Strong Law of Large Numbers] Let $X_1, X_2, \dots$ be iid and $E|X_i| < \infty$, $E(X_i) = 0$.  Then
$$S_n/n \rightarrow 0$$
almost surely.
\end{thm}
\begin{proof}
We will prove convergence of $S_n^+/n$ and $S_n^-/n$, where $S_n^+ = \sum_{i=1}^n X_i^+$ and similarly for the negative.  Hence, we can assume without loss of generality that $X_i \ge 0$.  We start by applying truncation: $\bar{X_i} = X_i 1_{|X_i| \le i}.$  

It suffices to prove $\bar{S_n}/n \rightarrow 0$ almost surely.  This follows from the fact that $\bar{X_i} = X_i$ for large enough $i$ almost surely, since $\sum P(\bar{X_i} \ne X_i) = \sum_{i=1}^\infty P(X_i > i) = E(X_i) < \infty.$  

By Markov,
$$P\left ( \left|\frac{\overline S_n - E(\bar S_n)}{n}\right|> \epsilon\right) \approx \frac{Var(\bar{S_n} - E(\bar{S_n}))}{\epsilon^2 n^2}.$$

This will not necessarily be summable over all $n$, so we choose a subsequence.  We will choose $k(n) = \alpha^n$ for a fixed $\alpha > 1$.  

We need to show that $\sum Var(\overline{S_{k(n)}})/k(n)^2 < \infty$.  Note that 
$$Var(\overline{S_{k(n)}})/k(n)^2 \le \sum_{i=1}^{k(n)} E \bar{X_i}^2/k(n)^2$$
and $$E\bar{X_i}^2 \approx \sum_{j=1}^i jP(X> j).$$ 

We can hence rewrite our expression as 
$$\sum_{n=1}^{\infty} \frac{\sum_{i=1}^{k(n)} \sum_{i=1}^i jP(X > j)}{k(n)^2}\le \sum_{j=1}^{\infty} \sum_{n=1}^{\infty} \frac{jP(X > j)}{k(n)}1(k(n) \ge j) \approx \sum_{j=1}^{\infty} jP(X > j) \cdot \frac{1}{j} \approx E(X),$$
and we have that $E(X) < \infty$.

Hence 
$$\frac{\bar{S}_{k(n)} - E(\bar{S}_{k(n)})}{k(n)} \rightarrow 0,$$
almost surely.

Then, $E(\bar{S}_{k(n)})/k(n) \rightarrow E(x)$ since $E(\bar{X}_i) \rightarrow E(X)$ by DCT, so $\bar{S}_{k(n)}/k(n) \rightarrow E(X)$ almost surely.  Since $\bar{X_i} > 0$, we have 
$$\bar{S}_{k(n)} \le \bar{S}_m \le \bar{S}_{k(n+1)},$$
and we can apply the squeezing argument from before but within bounds of $1/\alpha, \alpha$.  It suffices to choose any $\alpha > 1$, so choosing $\alpha$ arbitrarily close to $1$ gives that $\limsup$ and $\liminf$ are both equal to $E(X)$.

Hence,
$\bar{S}_m/m \rightarrow E(X)$
almost surely, and by BC we have that $$\frac{S_m}{m} \rightarrow E(X),$$
when $X\ge 0$ almost surely.  The full theorem comes from splitting $S_m = S_m^+ - S_m^-$.
\end{proof}
\begin{corollary} If $X_i$ iid with $E(X_i) = \infty$ then $S_n/n \rightarrow \infty$.  
\end{corollary}
\begin{proof}
Take $X_i^m = X_i 1(X_i < m).$  $E(X_i^m) < \infty$ and $\frac{S_n^m}{n} \rightarrow E(X_i^m)$ and $S_n/n \ge S_n^m/n$ for all $n, m$ so choosing large enough $m$ gives the result.
\end{proof}
\subsection{Second Proof of SLLN}
We will use the following:
\begin{thm}[Kolmogorov's Maximal Inequality] Let $X_1, X_2, \dots$ be independent with $E(X_i^2) < \infty$, $E(X_i) = 0$.  
$$P(\max_{k \le n} |S_k| > \epsilon) \le \frac{Var(S_n)}{\epsilon^2}.$$
\end{thm}
\begin{proof}
Let $A_n = \{\max_{k \le n} |S_k| > \epsilon\}$/  Suppose $T_k$ is the event that $k$ is the smallest index such that $|S_k| > \epsilon$.  Then, $$A_n = \bigcup_{k=1}^n T_k.$$
$$E(S_n^2) \ge E(S_n^2 1_{A_n}) = E(S_n^2 \sum_{k=1}^n 1_{T_k})$$
$$ = E((S_k + (S_n - S_k))^2 1_{T_k})$$
$$ = E(S_k^21_{T_k}) + E((S_n-S_k)^21_{T_k}) + E(S_k(S_n - S_k)1_{T_k}).$$
The last term is $0$ since $S_k$ and $1_{T_k}$ are measurable functions with respect to $\{X_1, \dots, X_k\}$, but $S_n - S_k = \sum_{j=k+1}^n X_j$ is measurable with respect to $\{X_{k+1} \dots, X_{n}\}$.  

Hence,
$$E(S_n^2 1_{T_k}) \ge E(S_k^2 1_{T_k}) \ge \epsilon^2 P(T_k),$$
since $|S_k| > \epsilon$ on $T_k$,
which gives $$P(A_n) \le \frac{Var(S_n)}{\epsilon^2}$$
by the union bound.
\end{proof}
\begin{thm} Suppose $X_1, \dots$ are independent mean 0 random variables.  If $\sum_{i=1}^{\infty} Var(X_i) < \infty$, then $\sum_{i=1}^n X_i$ converges almost surely.
\end{thm}
\pagebreak
\section{October 1st, 2020}
Last time:
\begin{itemize}
\item We covered the SLLN using an exponentially growing convergent subsequence.
\item Kolmogorov's Maximal Inequality:  Let $X_1, X_2, \dots$ be independent with $E(X_i^2) < \infty$, $E(X_i) = 0$.  
$$P(\max_{k \le n} |S_k| > \epsilon) \le \frac{Var(S_n)}{\epsilon^2}.$$
\end{itemize}
\subsection{Another Proof of SLLN, continued}
\begin{thm} Suppose $X_1, \dots$ are independent mean 0 random variables.  If $\sum_{i=1}^{\infty} Var(X_i) < \infty$, then $\sum_{i=1}^n X_i$ converges almost surely.
\end{thm}
\begin{proof}
We would like to show that $S_1, S_2, \dots$ converges.  It suffices to show that $(S_i)$ is almost surely Cauchy.  Given any $\epsilon > 0$, there exists $n_0$ such that for all $n_1, n_2 > n_0$ $|S_{n_1} - S_{n_2}| < \epsilon$.

Then,
$$P\left (\sup_{n \le k \le m} |S_k - S_n| > \epsilon\right ) = \epsilon^{-2}\sum_{k=n}^m Var(X_k).$$
Hence,
$$P\left (\sup_{n \le k } |S_k - S_n| > \epsilon\right ) = \epsilon^{-2}\sum_{k=n}^\infty Var(X_k),$$
where we take the limit $m \rightarrow \infty$ and use continuity from below.  Denote $A_n = \{\sup_{n \le k } |S_k - S_n| > \epsilon\}$.  If we let $B_n = \{\sup_{k_1, k_2 \ge n} |S_{k_1} - S_{k_2}| < 2\epsilon \}$, then $P(B_n) \ge 1-P(A_n)$.  Note that $B_n$ increases and $P(\bigcup B_n) \ge 1 - \lim P(A_n) = 1$, and since $\epsilon$ is arbitrary, taking the intersection over $\epsilon = 1/m$ implies that $S_k$ is Cauchy almost surely.
\end{proof}

\begin{thm} Suppose $X_i$ are iid with $E|X_i| < \infty$, $E(X_i) = 0$.  Then $\sum_{k=1}^n \frac{X_k}{k}$ converges almost surely.
\end{thm}
\begin{proof}
Let $\overline{X}_k = X_k 1_{|X_k| \le k}$.  We first show that $\sum \overline{X_k}/k$ converges almost surely, and $X_k$ and $\overline{X_k}$ at finitely many points(by BC), which gives the desired result.  

It suffices to show that 
$$\sum_{k=1}^{\infty} Var(\overline{X_k})/k^2 = \sum_{k=1}^{\infty} E(\overline{X_k}^2)/k^2 < \infty.$$

Then $$E(\overline{X_k}^2) = \sum_{i=1}^{\infty} iP(|\overline{X_k}|  > i) \le \sum_{i=1}^k iP(|X_1| > i).$$

Hence,
$$\sum_{k=1}^{\infty} E(\overline{X_k}^2)/k^2 \le  \sum_{i=1}^{\infty} iP(|X_1 > i|) \sum_{k=i}^{\infty} \frac{1}{k^2} \approx \sum_{i=1}^{\infty} iP(|X_1 > i|) \frac{1}{i} \approx E|X_1| < \infty.$$
\end{proof}
\begin{lemma}[Kronecker's Lemma] Suppose $a_n \uparrow \infty$ and $y_k$ are real numbers such that 
$\sum_{k=1}y_k/a_k$ converges.  Then $\sum_{k=1}^n y_k/a_n \rightarrow 0$.  
\end{lemma}
\begin{proof}
Note that $\sum_{k=1}^n y_k/a_k = b_n$ with $b_n \rightarrow b$.  Then, $y_k = (b_k - b_{k-1})a_k$.  
Then 
\begin{align*}
a_n^{-1}\sum_{i=1}^n y_k &= a_n^{-1}\sum_{k=1}^n (b_k-b_{k-1})a_k \\
&= a_n^{-1}\left (a_nb_n +\sum_{k=1}^{n-1}b_k(a_k - a_{k-1})\right ) \\
&= b_n - \sum_{k-1}^{n-1} b_k\left ( \frac{a_k-a_{k-1}}{a_n}\right )\\
\end{align*}

Hence, $\sum_{k-1}^{n-1} b_k\left ( \frac{a_k-a_{k-1}}{a_n}\right ) \rightarrow b$ and $b_n \rightarrow b$, so the difference converges to 0.
\end{proof}
This implies SLLN, since $\sum X_k/k $ converges almost surely.  We claimed that this was quantitative.
 
\begin{example}[Tighter Bound]
 Suppose $X_1, X_2, \dots$ are iid and $E(X_i = 0)$, $E(X_1^2) < c$.  We have already proved that $\sum_{i=1}^n X_i/n \rightarrow 0$ almost surely.  We can also show that $$\frac{\sum_{i=1}^n X_i}{\sqrt{n} \log{n}^{1/2+\epsilon}} \rightarrow 0$$ almost surely, for any $\epsilon > 0$.
\begin{proof}
 Let $a_k =\sqrt{k} \log{k}^{1/2 + \epsilon}$.  It suffices to show the convergence of 
 $$\sum_{k=1}^n \frac{X_k}{a_k}.$$
 
 Note that 
 $$\sum_{k=1}^{\infty} Var(X_k/a_k) \approx \sum_{k=1}^{\infty} \frac{C}{k(\log k)^{1+2\epsilon}} < \infty.$$

\end{proof}
\end{example}
 \begin{example}
 Let $X_1, X_2, \dots$ be iid with $EX_i = 0$.  Assume that $E|X_1|^p < \infty$ for some $p \in (1, 2)$.    Then
 $S_n/n^{1/p} \rightarrow 0$.  
 \begin{proof}
 Let $Y_k = X_k 1_{|X_k| < k^{1/p}}$.  Note that 
 $$\sum_{k=1}^{\infty} P(Y_k \ne X_k) = \sum_{k=1}^{\infty} P(|X_k|^p > k) \le E|X_1|^p < \infty.$$
 
 We then show the convergence of $$\sum \frac{Y_k - EY_k}{k^{1/p}}.$$ 
 It suffices to show that $\sum_{k=1}^{\infty} Var(Y_k)/k^{2/p} < \infty$.  Then
 $$\sum_{k=1}^{\infty} Var(Y_k)/k^{2/p}  \le \sum_{k=1}^{\infty} EY_k^2/k^{2/p},$$ where $m = k_0^{1/p}$.  Then, $EY_k^2 = \sum_{m=1}^{k^{1/p}} m^2 P(x \in [m, m+1])$, so this is approximately
 $$\sum_{m=1}^{\infty} m^2P(x \in [m, m+1])\sum_{k = k_0}^{\infty}\frac{1}{k^{2/p}} \approx \sum_{m=1}^{\infty} m^2P(x \in [m, m+1]) \frac{1}{k_0^{2/p - 1}},$$
 and $k_0^{2/p - 1} = m^{2-p}$, so this simplifies to 
 $$\sum_{m=1}^{\infty} m^p P(x \in [m, m+1]) \approx E|X|^p < \infty.$$
 
Finally $E(Y_k) = -E(X 1_{|X_k| > k^{1/p}})$ and 
$$E(X 1_{|X_k| > k^{1/p}}) = k^{1/p}E((X/k^{1/p})1_{|X| > k^{1/p}}) \le k^{1/p} E(|X/k^{1/p}|^p 1_{|X| > k^{1/p}}).$$
This can be written as 
$$k^{1/p-1} E(|X|^p 1_{|X| > k^{1/p}}) \rightarrow 0,$$
so 
$$|\sum EY_k| \le \sum_{k=1}^n k^{1/p-1}b_k \approx_{k=1}^n n^{1/p}b_k \rightarrow 0.$$ 
 \end{proof}
 
 \end{example}
\begin{lemma}[Glivenko-Cantelli] Consider a $X_1, X_2, \dots$ iid for some distribution $F$.  Define
$$F_n(y) = \frac{1}{n}\sum_{i=1}^n 1_{X_i \le y}.$$

Then, almost surely, $$\sup_y |F_n(y) - F(y)| \xrightarrow{n \rightarrow \infty} 0.$$
\end{lemma}
\begin{proof}
The uniformity follows because $F$ and $F_n$ are monotone.  
\end{proof}
\pagebreak
\section{October 6th, 2020}
\subsection{Convergence of Distributions}
\begin{lemma}[Glivenko-Cantelli] Consider a $X_1, X_2, \dots$ iid for some distribution $F$.  Define
$$F_n(y) = \frac{1}{n}\sum_{i=1}^n 1_{X_i \le y}.$$

Then, almost surely, $$\sup_y |F_n(y) - F(y)| \xrightarrow{n \rightarrow \infty} 0.$$
\end{lemma}
\begin{proof}
Note that $F_n$ is monotonically increasing from $0$ to $1$.    We divide the real line into intervals depending on $F$ as follows:  Fix $\epsilon > 0$.  Say $Z_0 = -\infty$, $Z_1 = \inf \{y : F(y) \ge \epsilon \epsilon\}$, and $Z_n = \inf \{y : F(y) \ge n \epsilon\}$.  We eventually have a sequence of random variables $\{Z_n\}$.  We also define $Z_{1/\epsilon} = \infty$.  

For any $i$, $\sup |F_n(x) - F(x)|$ is small for all $x \in [Z_i, Z_{i+1})$.   Note that $F(Z_{i+1}) - F(Z_i) \le \epsilon$.  Then, we apply SLLN:  We let $w_j = 1(X_j \le Z_i)$ and $w_j' = 1(X_j < Z_{i+1})$.  Then $E(w_j) = F(Z_i)$ and $E(w_j') = F(Z_{i+1})$.  Then we use SLLN to show that the end points are uniformly close and we sandwich the limit inside $[Z_i, Z_{i+1})$.
\end{proof}

\begin{definition} Let $F_n, F$ be distribution functions.  We say $F_n \rightarrow F$ in the sense of distributions in $F_n(x) \rightarrow F(x)$ such that $F$ is continuous at $x$.
\end{definition}

\begin{example} Let $F_n$ be the distribution induced by the point mass at $1/n$.  We see that $F_n(0) = 0$, which does not converge to $F(0)$, which is $1$.  
\end{example}
\begin{example} Suppose $X_i$ are iid $Ber(\pm 1)$.  Let $S_n = \sum_{i=1}^n X_i$ and consider $\frac{S_n}{\sqrt{n}} \rightarrow \text{Standard Gaussian}$.

Take any random variable with distribution $F$.  Let $X_n = X + \frac{1}{n}$, then $X_n \rightarrow X$ in distribution.  For any continuity point $z$ of $F$, $F_n(z) \rightarrow F(z)$ and $F_n(z) = F(z - \frac{1}{n})$, so fro all $X_n \le Z, X \le z-\frac{1}{n}$ which implies that 
$$P(X_n \le z) = P(X \le z-1/n) = F(z-1/n).$$
\end{example}

We investigate the relationship between notions of convergence.  If $X_n \rightarrow X$ in probability, does $X_n \rightarrow X$ in distribution? Yes.

If $X_n \rightarrow X$ in probability, $P(|X_n -X| > \epsilon) \rightarrow 0$.  Note that 
$$P(X_n \le z) \le P(X \le z+\epsilon) + P(|X_n - X| > \epsilon),$$
so $\limsup P(X_n \le z) \le P(X \le z + \epsilon) \le P(X \le z)$.

Then $$P(X_n \le z) \ge P(x \le z-\epsilon) - P(|X_n - X| > \epsilon),$$so $\liminf P(X_n \le Z) \ge P(X \le z - \epsilon) \ge P(X \le z-)$, so $P(X_n \le z) = P(X \le z)$.

The converse does not make sense.  One example that fails where $X_n \rightarrow X$ in distribution but $X_n \not \rightarrow X$ where $X = Ber(\pm 1)$ and $X_1, X_2, \dots$ are all $-X$.  Then $X_i \sim X$, but $X_i - X = -2X$.  

\begin{thm}[Skorokhod Representation] If $F_n \rightarrow F$ in distribution, then there exists a probability space $(\Omega, \Sigma, P)$ and random variables $X_n, X$ defined on $\Omega$ such that $X_n \sim F_n$ and $X_n \rightarrow X$ almost surely.
\end{thm}
\begin{proof}
We did some similar when given a distribution $F$, we constructed a random variable with distribution $F$.  We work in the uniform space $([0, 1], B, P)$.  Then $X_n = F_n^{-1}(\omega) = \sup\{y : F_n(y) < \omega\}$ and $X(\omega) = F^{-1}(\omega) = \sup\{y : F(y) < \omega\}$.  It suffices to show that $F_n^{-1}(\omega) \rightarrow F^{-1}(\omega)$.

Define $F_n'^{-1}(\omega) = \inf \{z : F_n(z) > \omega\}$ and similarly $F_n'^{-1}$. Let $A = \{\omega: F^{-1}(\omega) < F'^{-1}(\omega)\}$.  We showed in homework that $A$ is countable.  We now prove that $X_n \rightarrow X$ in $A^c$.  Suppose that $F_n^{-1}(\omega) \rightarrow z$.  Let $u < z < v$ where $u, v$ are continuity points and $F(u) < \omega - \epsilon$, $F(v) > \omega + \epsilon$.  These can be chosen because the set of discontinuities of a function is countable.  For all large $n$, $F_n(u) < \omega - \epsilon/2$ and $F_n(v) > \omega + \epsilon/2$ because $F_n \rightarrow F$ at all continuity points.  Then $F_n^{-1}(\omega) \ge u$ and $F_n^{-1}(\omega) \le v$, hence $$\limsup_n |F_n^{-1}(\omega) - z| \le u-v.$$
But $u, v$ were arbitrary continuity points, so $$\limsup_n |F_n^{-1}(\omega) - z|  = 0.$$
\end{proof}
\begin{fact} $X_n \rightarrow X$ in distribution if and only if for any bounded continuous function $g$, $E(g(X_n)) \rightarrow E(g(X))$.
\end{fact}
\begin{proof}
If $X_n \rightarrow X$ in distribution, then we have $Y_n \rightarrow Y$ with $Y_n \sim X_n$, $Y \sim X$ so $g(Y_n)$ has the same distribution as $g(X_n)$.  Thus $E(g(Y_n)) = E(g(X_n))$.  Then, $g$ is continuous so $g(Y_n) \rightarrow g(Y)$ almost surely since $Y_n \rightarrow Y$ almost surely, and $g$ is bounded so $E(g(X_n)) = E(g(Y_n)) \rightarrow E(g(Y)) = E(g(X))$.

For the other direction, if $E(g(X_n)) \rightarrow E(g(x))$ for all bounded continuous functions $g$, we choose a mollified indicator function.  Then $E(g(X_n)) \ge P(X_n \le z) $ and $E(g(X_n)) \rightarrow E(g(X)) \le P(x \le z + \epsilon)$.  Hence,
$$\limsup P(X_n \le z) \le P(x \le z+\epsilon).$$
Similarly, 
$$\liminf P(X_n \le z) \ge P(X \le z - \epsilon),$$
which gives that $P(X_n \le z) \rightarrow P(X \le z)$ by using the fact that $z$ is a continuity point.
\end{proof}
This gives an alternate definition of convergence in distribution.

For any topological space and a sequence of measures $\mu_n$ and $\mu$ on $\Omega$, we can say $\mu_n \rightarrow \mu$ in distribution if for all bounded continuous $g : \Omega \rightarrow \R$, $E_{\mu_n}g \rightarrow E_\mu g$.

\begin{thm}[Continuous Mapping Theorem] Suppose $X_n \rightarrow X$ and $g: \R \rightarrow \R$ is a measurable function such that $C_g$ is a set of continuity points of $g$ such that $P(X \in C_g) = 1$, then $g(X_n) \rightarrow g(X)$ in distribution.
\end{thm}
\begin{proof}
Note that the set of continuity points for a measurable function $g$, $C_g$ is measurable.
\end{proof}
\pagebreak
\section{October 8th, 2020}
Last time,
\begin{itemize}
\item We defined distributional convergence.
\item We proved the Skorokhod Representation Theorem.
\item We showed an equivalent notion of distributional convergence, namely for bounded continuous functions $g$, $E(g(x_n)) \rightarrow E(g(x))$ implies $x_n \rightarrow x$ in distribution.
\end{itemize}
\subsection{Weak Convergence}
\begin{thm}[Continuous Mapping Theorem] Suppose $X_n \rightarrow X$ and $g: \R \rightarrow \R$ is a measurable function such that $C_g$ is a set of continuity points of $g$ such that $P(X \in C_g) = 1$, then $g(X_n) \rightarrow g(X)$ in distribution.
\end{thm}
\begin{proof}
We claimed that $C_g$, the set of continuity points is measurable.  We know that $X_n \rightarrow X$ in distribution, so there exists $Y_n \rightarrow Y$ almost surely with $X_n \sim Y_n$, $X \sim Y$.  Then $g(X_n) \sim g(Y_n)$ and $g(X) \sim g(Y)$.  Then $Y \in C_g$ almost surely because $X \sim Y$.  Therefore, $g(Y_n) \to g(Y)$ almost surely, which implies distributional convergence.  
\end{proof}
\subsection{Portmanteau's Lemma}
We show many equivalent conditions for distributional convergence.
\begin{enumerate}
\item $F_n \xrightarrow{d} F$ 
\item For any open $U$, $\liminf P_n(U) \ge P(u)$.
\item For any closed $V$, $\limsup P_n(V) \le P_n(V)$.
\item For any $A$ such that $P(\partial A) = 0$($\partial A = \overline{A}\setminus A^o$), we have $P_n(A) \rightarrow P(A)$.
\end{enumerate}
\begin{proof}
We first show 1 implies 2.  Let $Y_n \sim F_n$, $Y \sim F$ with $Y_n \rightarrow Y$ almost surely.  Let $F_n = 1(Y_n \in U)$, $f = 1(Y \in U)$.  Then $P(Y_n \in U) = P_n(U)$.  Finally, $$\liminf_{n \rightarrow \infty} f_n \ge f.$$
This is because $f(\omega) = 1$ if $Y (\omega) \in U$ and $0$ otherwise.  Pick $\omega$ such that $f(\omega) = 1$.  Then $Y(\omega) \in U$ and $Y_n(\omega) \rightarrow Y(\omega)$ and $U$ is open, we know that for large $n$, $Y_n(\omega) \in U$.  Hence, $f_n(\omega) = 1$ for large $n$, so $\liminf f_n = 1$.  

Finally, by Fatou's Lemma, 
$$\liminf P_n(U) \ge \int \liminf f_n \ge \int f = P(U).$$

2 implies 3 is easy.  We take $V^c$, then $P_n(V) = 1 - P_n(V^c)$ and $\limsup P_n(V) = 1 - \liminf P_n(V^c).$

We show 3 implies 4.  Note that $\overline A = A \cup \partial A$ and $P(\overline A) = P(A)$.  Then $A^o = A \setminus \partial A$ and $P(A^o) = P(A)$.  By 2, $\liminf P_n(A^o) \ge P(A)$ and $\limsup P_n(\overline A) \le P(A)$ and $P_n(A^o) \le P_n(A) \le P_n(\overline A)$.  Therefore, $P_n(A) \rightarrow P(A)$.

Finall 4 implies 1.  If $P_n(A) \rightarrow P(A)$ for $P(\partial A) = 0$, then choose $A = (-\infty, x]$ for a continuity point $x$.  Then $P(\{x\}) = 0$, so we get $F_n(x) = P_n(A) \rightarrow P(A) = F(x)$.
\end{proof}
\subsection{Helly's Selection Theorem} 
\begin{thm}Given a sequence of distributions $F_1, F_2, \dots$, there exists a subsequence $n_1, n_2, \dots$ and a right continuous non-decreasing function $F$ so that $F_{n_k}(x) \rightarrow F(x)$ for all continuity points.   
\end{thm}

Note that $F$ may not be a distribution since $F(\infty) - F(-\infty) \ne 1$.  For example, take $F_n(x) = 1_{x \ge n}$.  Then $F_n(x) = 0$ for $n \ge x$ so $F_n \rightarrow 0$, which is not a distribution.
\begin{proof}
We want $F_{n_k}(x) \rightarrow F(x)$.  If we fix $x$, $0 \le F_n(x) \le 1$ so we can pick a subsequence so that $F_{n_k}(x) \rightarrow a$ from the compactness of $[0, 1]$.  We first ensure convergence for the rationals.  Let $\{q_n\}$ be an enumeration of the rationals.  Iteratively choose subsequences $n_k^i$ such that $\{n_k^i\}$ is a subsequence of $\{n_k^{i-1}\}$ and $F_{n_k}^(q_i)$ converges.  

Hence, $F_{n_k^k}(q)$ converges for all $q \in \Q$.  We will call this $\{n^k\}$ for convenience.  Let the limit be $\lim_{k\to \infty} F_{n^k} = G(q)$.

Then, define $F(x) = \inf_{q > x} G(q)$.    We show $F_{n_k}(x) \rightarrow F(x)$ for all $x$ with $F$ continuous at $x$.  Pick $q_1, q_2, q_3\in Q$ so that $q_1 >x > q_2 > q_3$.   Then $F_{n_k}(x) \le F_{n_k}(q_1)$ so $\limsup f_{n_k}(x) \le G(q_1)$ so $\limsup F_{n_k}(x) \le F(x)$. 

Since $x$ is a continuity point, there exists $q_3$ close to $x$ so that $F(q_3) \ge F(x) - \epsilon$, but $F(q_3) = \inf_{q > q_3} G(q)$, so there exists $q_2$ with $q_3 < q_2 < x$ so that $G(q_2) \ge F(q_3) + \epsilon$ so $G(q_2) \ge F(x) - 2\epsilon$.  But $F_{n_k}(x) \ge F_{n_k}(q_2)$ so $\liminf F_{n_k}(x) \ge G(q_2) \ge F(x) - 2\epsilon$ so $\liminf F_{n_k}(x) \ge F(x)$.
\end{proof}
When does the limit preserve mass?  
\begin{definition} Given $\epsilon > 0$, $\{F_n\}$ is tight if there exists $M_\epsilon$ so that $F_n$ in the sequence 
$$F_n(M_\epsilon) - F_n(-M_{\epsilon}) \ge 1-\epsilon.$$
\end{definition}
Tightness is a necessary and sufficient condition for preserving mass.
\begin{proof}
We have $F_{n_k} \rightarrow F$ for continuity points.  We show $F$ has mass $1$.  We can assume $M_\epsilon, -M_\epsilon$ are continuity points.  Then 
\begin{align*}
F_{n_k}(M_\epsilon) - F_{n_k}(-M_\epsilon) \to F(M_\epsilon) - F(-M_\epsilon) \ge 1 - \epsilon.
\end{align*}
\end{proof}
If a sequence is not tight, then there exists subsequential limits with mass less then $1$.
\subsection{Fourier Transforms}
Let $X$ be a random variable with distribution $F$.  For any $t$, define $\phi(t) = E(e^{itx})$.  

We have some properties:
\begin{itemize}
\item $\phi(0) = 1$.
\item $\overline{\phi(t)} = \phi(-t)$.
\item $|\phi(t) | \le 1$.  
\begin{proof}
We use Jensen's Inequality: For a convex function $\phi(x_1, x_2, \dots, x_n)$ with $E|\phi| < \infty$, $E|X_i| < \infty$ for all $i$, then $E(\phi) \ge \phi(EX_1, EX_2, \dots)$.

Take $(x, y) \mapsto (x^2 + y^2)^{1/2}$.  Then 
$$|\phi(t)| \le E|e^{itx}| = 1.$$
\end{proof}
\item $\phi$ is uniformly continuous.  $$|\phi(t+h) - \phi(t)| \le |E(e^{i(t+h)} - e^{itx})| \le  E|e^{itx}(e^{ihx}-1)| =  E|e^{ihx} - 1| \rightarrow 0,$$
by the Bounded Convergence Theorem.
\end{itemize}
\pagebreak
\section{October 13, 2020}
\subsection{Fourier Transforms, Continued}
Recall $\phi(t) = E(e^{itx})$.  The broad goal is to study distributional convergence using the characteristic functions.  
\subsection{Differentiability of Characters}
When is $\phi(t)$ differentiable?  If $\phi$ is differentiable at $t$, then $\phi'(t) = E(ix e^{itx})$, so $\phi'(0) = E(ix)$.  For this to make sense, we might need $E|X| < \infty$.
\begin{thm} If $E|X| < \infty$, then $\phi$ is continuously differentiable.   
\end{thm}
\begin{proof}
\begin{align*}
\lim_{h \rightarrow 0} \frac{\phi(t+h) - \phi(t)}{g} &= E\left (\frac{e^{i(t+h)X} - e^{itX}}{h}\right ) \\
\end{align*}
It suffices to show that $$E\left (\frac{e^{i(t+h)X} - e^{itX}}{h} - ixe^{itX}\right ) \rightarrow 0.$$
\begin{align*}
E\left (\frac{e^{i(t+h)X} - e^{itX}}{h}  - iXe^{itX}\right) &= E(iXe^{itX}(\frac{^{itX}-1}{ihX} - 1 )) \\
&= E\left (iXe^{itX}\left ( \frac{\int_0^h e^{inX} - 1 }{h} - 1\right )\right )\\ 
&\le E(|iXe^{itX}| \sup_{u\le h} |e^{iuX} - 1| ) \rightarrow 0 \\
\end{align*}
by DCT, since the argument is at most $2|X|$.

Then, $\phi'$ is continuous since $$\phi'(t+h) - \phi'(t) = E(ixe^{i(t+h)x} 0 e^{itx}) = E(ixe^{itx}[e^{ihx} - 1]) \rightarrow 0$$
by DCT.
\end{proof}
Similarly if $E|X|^k < \infty$, then $\phi(t) \in C^k$.  The proof follows straightforwardly by induction on the moment.
\begin{lemma} If $\phi$ is twice-differentiable, then $E(X^2) < \infty$.
\end{lemma}
\begin{proof}
Note that 
$$\frac{2\phi(0) - \phi(h) - \phi(-h)}{h^2} \xrightarrow{h \rightarrow 0} -\phi''(0).$$
But this quantity is exactly
$$\frac{2 - 2E(\cos(hx))}{h^2} \to -\phi''(0)$$
since $\phi(h) = E(\cos(hx) + i\sin(hx))$ and $\phi(-h) = E(\cos(hx) - i\sin(hx))$.

Finally, $\cos(x) = 1 - \frac{x^2}{2}$, so we $f_h(X) \rightarrow X^2$, $f_h(X) \ge 0$ and $E(f_h(x)) \rightarrow \phi''(0)$(where $f_h(X)$ is the term inside the expectation).  It follows that $E(X^2) \le \lim E(f_h(X)) = -\phi''(0)$.   Furthermore, $\phi''(0) = E(-X^2)$.
\end{proof}

\subsection{Fourier Inversion Formula}
Let $X \sim \mu$ and $\phi(t) = E(e^{itx})$.  Let $F$ be the distribution induced by $\mu$.  We would like to approximate $F(b) - F(a)$.  

We consider $$\int_{-T}^T\int_{a}^b e^{-iut}\phi(t) du dt = \int_{-T}^T \frac{e^{-iat} - e^{-bt}}{it} \phi(t).$$
Then $$\int_{-T}^T \frac{e^{-iat} - e^{-bt}}{it} \int_\R e^{itx} d\mu= \int_\R \int_{-T}^T \frac{e^{it(x-a)} - e^{it(x-b)}}{it} = 2\int_\R \int_{0}^T \frac{\sin(t(x-a)) - \sin(t(x-b))}{t}dt.$$

Hence, we have 
$$2 \int_{\R}d\mu \int_{0^T} \frac{\sin(t(x-a)) - \sin(t(x-b))}{t}dt.$$
Then, for any $x$, $$\int_{0}^T \frac{\sin (\theta x)}{x} $$ is bounded as $T \rightarrow \infty$ and converges to $\frac{\pi}{2}\text{sgn}(\theta)$. 
It follows that the integral converges to $\pi \mu(\{a\}) + \pi\mu(\{b\}) + 2\pi \mu(a, b)$.  Dividing by $2\pi$ throughout, gives $$\mu(a,b) + \frac{\mu(\{a\} \cup \{b\})}{2}.$$
\begin{lemma} If $F_1, F_2$ are two distributions with $\phi_1(t) = \phi_2(t)$, where $\phi_1$ and $\phi_2$ are the characteristics functions, then $F_1 = F_2$.
\end{lemma}
\begin{proof}
Consider the set $A = \{x : \mu_1(x), \mu_2(x) > 0\}$, which is countable. For all $a, b, \in A^c$, $\mu_1([a, b]) = \mu_2([a, b])$.  Then, send $a \to -\infty$ to show $F_1(b) = F_2(b)$ for all $b \in A^c$, but $A^c$ is dense so $F_1 = F_2$ everywhere by right-continuity.
\end{proof}
\begin{thm}
If $\phi(t)$ is integrable, then $F$ has a density: $F[a, b] = \int_{a}^b f dx$ for all $a, b$.

Namely, $$f(x) = \frac{1}{2\pi}\int_\R e^{-ixt}\phi(t)dx.$$
\end{thm}
\begin{proof}
We first show there are no atoms, $\mu(x) = 0$ for all $x$.   Then for $a < x < b$
$$\mu(x) \le \int |\phi(t)| |b-a| \rightarrow 0.$$

Finally, $$\int_{a}^b\frac{1}{2\pi}\int_\R e^{-iut}\phi(t) = \mu(a, b) = \mu[a, b].$$
It is easy to show that $f$ is real valued. 
$$\overline{f}(u) = \int e^{iut}\overline{\phi}(t) = \int_{\R} e^{iut}\phi(-t) = \int_R e^{-iut}\phi(t)dt = f(u).$$
\end{proof}
\pagebreak
\section{October 15th, 2020}
Recall
\begin{itemize}
\item Moments of a Random Variable implies smoothness of the characteristic functions In particular, the existence of the $k$-th moment implies that $\phi^{(k)}$ exists and is continuous.
\item The Inversion Formula: $$\frac{1}{2\pi}\int_{-T}^T\int_{a}^b e^{-itu} \phi(t)dt \xrightarrow{T \to \infty} \frac{1}{2}\mu(\{a, b\}) + \mu(a, b).$$
\item If $\phi(t)$ is integrable, then a density exists - namely
$$f(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{-itx}\phi(t).$$
This implies that $\int_{a}^b f = \mu[a, b]$.  We can also show that $f$ is continuous since $|f(t+h) - f(h)| \rightarrow 0$ uniformly by the dominated convergence theorem.
\end{itemize}
\subsection{Characteristic Functions}
\begin{proposition}
If $X, Y$ are independent random variables, $\phi_{X+Y}(t) = \phi_X(t) \phi_Y(t)$.
\end{proposition}
\begin{proof}
$$E(e^{it(X+Y)}) = E(e^{itX}e^{itY}) = E(e^{itX})E(e^{itY}).$$
\end{proof}
\begin{proposition}
$$\phi(t) = E\left(\sum_{m=0}^n \frac{(itX)^m}{m!}\right) + o(t^n).$$
\end{proposition}
\begin{proof}
Note that 
$$\left |\phi(t) - E\left(\sum_{m=0}^n \frac{(itx)^m}{m!}\right)\right | \le E\min(2|x|^n/n!, |x|^{n+1}/(n+1)!).$$
This follows from the fact that 

$$\int_{0}^x e^{is}(x-s)^n ds = \frac{x^{n+1}}{n+1} + \frac{i}{n+1}\int e^{is}(x-s)^{n+1}ds.$$
Putting $n=0$ gives 
$$e^{ix} = 1 + ix + i^2 \int e^{is}(x-s).$$
It follows by induction that 
$$e^{ix} - \sum_{m=0}^{n}\frac{(ix)^m}{m!} = \frac{i^{n+1}}{n!}\int_{0}^x e^{is}(x-s)^n dx.$$
Finally, 
$$\left |\frac{i^{n+1}}{n!}\int_{0}^x e^{is}(x-s)^n dx \right|\le \int_{0}^x (x-s)^n/n! = \frac{x^{n+1}}{(n+1)!}.$$

Then 
\begin{align*}
\frac{1}{n!} \int_{0}^x e^{is}(x-s)^n &= \frac{1}{n!}(e^{is}/i)(x-s)^n|_{0}^x + \int_{0}^x ne^{is}/i(x-s)^{n-1})\\
&= \frac{1}{n!}(ix^n - in\int_0^x e^{is}(x-s)^{n-1})\\
&= \frac{1}{(n-1!)}\int(x-s)^{n-1}(1-e^{is}) \\
&\le 2x^n/n!.
\end{align*}
It follows that $|\phi(t) - \sum E(itX)^m/m!| \le E \min(2(tX)^n/n!, (tx)^{n+1}/(n+1)!)$
\end{proof}
\subsection{Weak Convergence}
\begin{thm} For $F_n$ distributions with $\phi_n(t)$, $F_n$ converge in distribution if and only if $\phi_n \rightarrow \phi$(where $\phi$ is continuous at $0$).
\end{thm}
\textbf{Remark}:  Consider $N(0, 1)$.  Then $\phi(t) = e^{-t^2}/2$.  It's easy to prove that $\phi'(t) = E(ixe^{itx}) = -t\phi(t)$, using integration by parts to simplify the right side with $\phi(0) = 1$.  If $X \sim N(0, 1)$ $\sigma X \sim N(0, \sigma^2)$ so $\phi(t) = e^{-\sigma^2X^2/2}.$  Then $\phi_n \rightarrow \delta_0$.  But $F_n(x) \rightarrow \frac{1}{2}$ for all $x$.  We will see that continuity at $0$ gives the tightness condition.
\begin{proof}
We prove the forward direction.  $F_n \rightarrow F$ in distribution implies that $E(g(X_n)) \to E(g(X))$ for bounded continuous functions.  The result follows since $e^{itx}$ is a bounded continuous function.

We prove the converse.  We first show that $\{F_n\}$ is a tight sequence.
Note that 
$$\int_{-u}^u (1-e^{itx}) = 2u - \int_{-u}^u (\cos{tx} + i\sin{tx})dt = 2u - \frac{2\sin{ux}}{x}.$$

Then
$$\frac{1}{2u} \int_{-u}^u (1-e^{itx})= 1 - \frac{\sin{ux}}{ux}.$$
Then $|\sin{x}| \le |x|$, so 
\begin{align*}
\frac{1}{2u} \int_{-u}^u (1 - \phi_n(t)) &= \int 1 - \frac{\sin{ux}}{ux} d\mu_n \\
&\ge \frac{1}{2} \mu_n(|x| \ge 2/u).
\end{align*}
Then $\phi_n(0) \rightarrow \phi(0) = 1$, so
$$\mu_n(|x| > 2/u) \le u^{-1} \int_{-u}^u (1-\phi_n(t))dt \rightarrow u^{-1} \int_{-u}^u (1-\phi(t))dt \le 2\epsilon,$$
for some choice of $u$ from the continuity of $\phi$ at $0$. 

Since $\epsilon$ is arbitrary, it follows that $\mu_n$ is tight.  Now we show that $F_n$ converges to $F$ in distribution.  Note that any subsequence has a convergent subsequence which converges to a distribution.  Then the subsequences converge to $\phi_{n_i} \rightarrow \phi_F$, and $\phi_n \rightarrow \phi$ so $\phi_F = \phi$.  Hence, every subsequence has a convergence converges to the same $F$ by the uniqueness of characteristic functions.  Then $F_n(x) \to F(x)$ for all continuity points $x$ of $F$ and it follows that for $F_n(x)$, and subsequence admits a further subsequence converging to $F(x)$.  Hence $F_n(x) \to F(x)$, since otherwise one can extract a subsequence such that $|F_{{n_i}}(x) - F(x)| > \delta$ for all $i$.
\end{proof}

\pagebreak
\section{October 20th, 2020}
\subsection{Basic Central Limit Theorem}
Under natural conditions, we will prove that 
$$\frac{S_n}{\sqrt{n}} \xrightarrow{d} N(0, \sigma^2).$$
\begin{example} Let $X_i = \pm 1$ with probability $1/2$.  We can analyze binomial coefficients to prove a central limit theorem.
\end{example}
\begin{thm} Let $X_i$ be iid $E(X_i) = 0$, $E(X^2) = 1$.  Then
$$\frac{S_n}{\sqrt{n}} \xrightarrow{d} N(0, 1).$$
\end{thm}
\begin{proof}
Recall that $\phi_z(t) e^{-t^2/2}$.  Hence, we show that $\phi_{S_n/\sqrt{n}}(t) \to e^{-t^2/2}$.

Then $$\phi_{S_n/\sqrt{n}}(t) = \phi_{S_n}(t/\sqrt{n}).$$  Then $S_n = \sum X_i$ iid, so $$\phi_{S_n}(t/\sqrt{n})) = (\phi_{X_1}(t/\sqrt{n}))^n.$$

Then $$|\phi_{X_i}(t/\sqrt{n}) - E\sum_{m=0}^2 (itx/\sqrt{n})^m/m!|\le E(\min\{(itx/\sqrt{n})^2, (itx/\sqrt{n})^3\}) .$$

Then, the expectation simplifies to $1 - t^2/2n$ since $E(X_1) = 0, E(X_1^2) = 1$.

Then,
$$E = E(\min\{(itx/\sqrt{n})^2, (itx/\sqrt{n})^3\}) = o(t^2/n),$$
or $\frac{E}{t^2/n} \to 0$ as $t^2/n \to 0$.  This is because $$E = \frac{t^2}{n} E(\min(X^2, t/\sqrt{n}X^3)) \xrightarrow{t^2/n \to 0} 0,$$
by DCT.

Hence,
$$\phi_{X_1}(t/\sqrt{n})^n = (1 - t^2/2n + o(t^2/n))^n.$$  

We show that 
$$(1 - t^2/2n + o(t^2/n))^n \to e^{-t^2/2}.$$
More generally, we show that if $c_n \to c$, then $(1 + c_n/n)^n \to e^c$.

\begin{lemma} Suppose we have complex numbers $z_1, z_2, \dots, z_n, w_1, w_2, \dots, w_n$ with $|z_i||w_i| \le \theta$.  Then 
$$\left |\prod_{i=1}^n z_i - \prod_{i=1}^n w_i\right |\le \theta^{n-1}\sum_{i=1}^n |z_i - w_i|.$$
\end{lemma}
\begin{proof}
We use telescoping.
$$\prod z_i - \prod w_i = \prod_{i=1}^n z_i - \prod_{i=1}^{n-1} z_iw_n  + prod_{i=1}^{n-1} z_iw_n - \prod_{i=1}^{n-2}z_iw_{n-1}w_n + \dots - \prod_{i=1}^{n} w_i. $$

Then, using the triangle inequality and summing the bound gives that 
$$|\prod z_i - \prod w_i| \le \theta^{n-1} \sum |z_i - w_i|.$$
\end{proof}

We first bound $$\left|(1 + c_n/n)^n - e^{c_n}\right|.$$
The whole thing is the same as $|(1 + c_n/n)^n - (e^{c_n/n})^n|$.  Note that $e^{x} \ge 1+x$ for real $x$ and $|e^x - (1+x)| \le |x|^2$ for complex $x$.

Note that $1 + |c_n/n| \le e^{|c_n/n|} = \theta$, so the error is bounded by
$$ne^{|c_n/n|(n-1)}|e^{c_n/n} - (1 + c_n/n)| \le e^{|c_n/n|(n-1)}|c_n|^2/n,$$
and it follows that $(1 + c_n/n)^n \to e^{c}$.
\end{proof}
\subsection{Lindeberg-Feller CLT}
\begin{thm}[Lindeberg-Feller]
We prove CLT for a triangular array of random variables.  Consider
\begin{align*}
&X_{11} \\
&X_{21}, X_{22} \\
&\dots \\
&X_{n1}, X_{n2}, \dots, X_{nn}
\end{align*}
independent.
We suppose that $EX_{ni} = 0$, $\sum_{i=1}^n EX_{ni}^2 \to 1$ and $\sum_{i=1}^n E(X_{ni}^2, 1(|X_{ni}| > \epsilon)) \to 0$ for all $\epsilon > 0$.

Let $S_n = \sum_{i=1}^n X_{ni}$.  We show that $S_n \xrightarrow{d} \mathcal N(0, 1)$.
\end{thm}


\begin{example} For the previous example, we had $X_1, X_2, \dots$ and $S_n = \sum_{i=1}^n X_i$.  We take $X_{ni} = X_i/\sqrt{n}$.  We know that $EX_{ni} = 0$ and $E(X_{ni}^2) = \frac{1}{n}$, so $\sum EX_{ni}^2 = 1$.

Then $$\sum_{i=1}^n E(X_{ni}^2, 1(|X_{ni}| > \epsilon))  = nE(X_{ni}^2, 1(|X_{ni}| > \epsilon)) = E(X_1^2 1|X_1| > \epsilon\sqrt{n}) \to 0.$$
\end{example}
\begin{proof}
Note that $\phi_{S_n}(t) = \prod \phi_{X_{ni}}(t)$.  Let $EX_{ni}^2 = \sigma_{ni}^2$.   We have that $\sum \sigma_{ni}^2 = 1$.  We want to show that $\phi_{S_n}(t) \to e^{-t^2/2}.$

Note that
\begin{align*}
|\phi_{X_{ni}}(t) - (1 - \sigma_{ni}^2t^2/2)| &\le E(\min(t^2X_{ni}^2, t^3 X_{ni}^3)) \\
&\le E(t^2 X_{ni}^2 1|X_{ni} > \epsilon|) + E(t^3 X_{ni}^3 1|X_{ni}| < \epsilon|) \\
&\le E(t^2 X_{ni}^2 1|X_{ni}| > \epsilon) + \epsilon t^3 E(X_{ni}^2 1|X_{ni}| < \epsilon).
\end{align*}
If we sum the left and take limits, we have 
$$\sum_{i=1}^n E_{ni} \le 0 + \epsilon t^3 \xrightarrow{n \to \infty, \epsilon \to 0} 0.$$

Now, from the previous lemma, note that $|\phi_{ni}(t)| \le 1$.  We also have $|1 - \sigma_{ni}^2t^2/2| \le 1$, since $\sup \sigma_{ni}^2 \to 0$.  This is because $$\sigma_{ni}^2 = EX_{ni}^2 = E(X_{ni}^2 | |X_{ni}| < \epsilon) + E(X_{ni}^2| |X_{ni}| > \epsilon) \to 0.$$ Hence,
$$\left |\prod \phi_{ni}(t) - \prod(1 - \sigma_{ni}^2t^2/2)\right | \to 0.$$

Finally, $$\log \left (\prod(1 - \sigma_{ni}^2t^2/2)\right ) = \sum \log(1 - \sigma_{ni}^2t^2/2) \approx -\sum \sigma_{ni}^2t^2/2 \to -t^2/2,$$
so $$\phi_{S_n}(t) \to e^{-t^2/2}.$$
\end{proof}
\subsection{Kolmogorov Three-Series Theorem}
\begin{thm}[Kolmogorov Three-Series]  Let $X_1, \dots, X_i, \dots$ be independent and $\sum_{i=1}^n$ converges almost surely.  The above and the following are equivalent.
For $A > 0$,
\begin{enumerate}
\item $\sum_{i=1}^{\infty} P(|X_i| > A) < \infty$.
\item Where $\overline{X_i} = X_i1(|X_i| < A)$, $\sum_{i=1}^n E\overline{X_i}$ converges.
\item $\sum_{i=1}^{\infty} Var(\overline{X_i}) < \infty$.
\end{enumerate}
\end{thm}
\begin{proof}
We first prove the converse.  It suffices to show that $\sum \overline{X_i}$ converges almost surely, since $\sum \overline{X_i}$ converges is equivalent to $\sum \overline{X_i}$ converging by BC.  

By the Kolmogorov Maximal Inequality, $\sum Var < \infty$ implies converges of centered independent random variables, so $\sum \overline{X_i} - E(\overline{X_i})$ converges and $\sum E(\overline{X_i})$ converges by assumption, so $\sum \overline{X_i}$ converges.

Now,we prove the forward direction.  The first condition is easy since by BC, $|X_i| < A$ eventually, so $\sum P(|X_i| > A) < \infty$.  We now prove $\sum Var(\overline{X_i}) < \infty$.  Suppose not: let $c_n = \sum_{i=1}^n Var(\overline{X_i})$.  Then $c_n \to \infty$.  Define $\overline{X}_{ni} = \frac{\overline{X_i} - E\overline{X_i}}{\sqrt{C_n}}$.  Note that $E\overline{X_{ni}} = 0$, $\sum_{i=1}^n E\overline{X_{ni}}^2 = 1$.  Finally, for $\epsilon > 0$,
$$\sum E(\overline{X_{ni}}^2, 1(|X_{ni}| > \epsilon)) \to 0,$$
since $\overline{X_{ni}} > \epsilon$ implies that $\overline{X_i} - E\overline{X_i} > \epsilon \sqrt{c_n}$.  By Lin-Fell CLT, 
$$\sum \frac{\overline{X_i} - E\overline{X_i}}{\sqrt{c_n}} \to \mathcal N(0, 1).$$
But by hypothesis, $\sum \overline{X_i}$ converges almost surely, so
$$\sum \overline{X_i}/\sqrt{c_n} \to 0,$$
which is a contradiction.  

Hence, $\sum Var(\overline{X_i}) < \infty$.  By KMI, $\sum \overline{X_i} - E\overline{X_i}$ converges, so $\sum E \overline{X_i}$ converges, giving 2.
\end{proof}
\pagebreak
\section{October 22nd, 2020}
\subsection{CLT with Unbounded Variance}
Let $X_1, X_2, \dots$ by iid symmetric random variables, and $P(|X_i| > x) = \frac{1}{x^2}$ for all $x > 1 $.  Then
$$E(X^2) = \int 2xP(|X| > n) = \int 2/x = \infty.$$

We will handle this by using truncation, applying BC, and using CLT for truncated variables.  We want to truncate at the smallest possible level so that BC can still be applied.  

Define $X_i^n = X_i 1(|X_i| \le \sqrt{n} \log \log{n})$ for $i \le n$.  Then,
$$P(X_1 + \dots + X_n \ne X_1^n + X_2^n + \dots + X_n^n) \le \frac{n}{(\sqrt{n}\log\log n)^2} \to 0.$$

By symmetry, $EX_i^n = 0$.  Then$$E((X_{i}^n)^2) \le \int_{1}^{\sqrt{n}\log \log n}2xP(|X| > n) = \int_{1}^{\sqrt{n}\log \log n} \frac{2}{x} = \log(n(\log \log n)^2) = \log n + 2\log \log \log n.$$

We can also lower bound it by approximately $\log n$, so $E((X_1^n)^2) = \log n + o(\log n)$.  
Using LF CLT, take $$\frac{X_1^n}{\sqrt{n\log n}},\frac{X_2^n}{\sqrt{n\log n}}, \dots, \frac{X_n^n}{\sqrt{n\log n}}.$$

We show the sum of variances to a limit, and contributions from large variances add up asymptotically to $0$.

Note that 
$$\sum_{i=1}^n Var(\overline{X_{n, i}}) = n\frac{\log n + o(\log n)}{n \log n} \to 1.$$

Then 
$$\sum_{i=1}^n E(\overline{X_{n, i}}^2, |\overline{X}_{n, i}| > \epsilon) \to 0,$$
since $$
\frac{X_i 1(|X_i| \le \sqrt{n }\log \log n)}{\sqrt{n \log n}} \le \frac{\log \log n}{\sqrt{\log n}} < \epsilon$$
for large $n$.

By $LF$, $\frac{\overline{S_n}}{\sqrt{n \log n}} \xrightarrow{d} N(0, 1)$, and by $BC$ it follows that $\frac{S_n}{\sqrt{n \log n}} \xrightarrow{d} N(0, 1).$
\subsection{General Theory of Distributions}
We have a separable metric space $(S, d)$ and we define distributions convergence in this context.  
\begin{definition} $X_n \xrightarrow{d} X$ if $Ef(X_n)\ to Ef(X)$ for any bounded continuous function $f$.
\end{definition}
\begin{thm}[Equivalent Conditions for Convergence] The following are equivalent.
\begin{enumerate}
\item $Ef(X_n) \to Ef(X)$ for all bounded continuous $f$.
\item For closed $K$, $\limsup P(X_n \in K) \le P(X \in K)$.
\item For open $U$, $\liminf P(X_n \in U) \ge P(X \in U)$.
\item For $A$ with $P(\partial A) = 0$, $P(X_n \in A) \to P(A)$.
\item If $f$ is a bounded measurable function and $D_f$ denotes the discountinuity points of $f$, if $P(X \in D_f) = 0$, then $Ef(X_n) \to Ef(X).$
\end{enumerate}
\end{thm}
Remark: We have not stated a Skorokhod Representation Theorem. 
\begin{proof}
We only show 4 implies 5, the rest are trivial.  Suppose $|f(x)| \le K$.  Divide $[-K, K]$ into intervals of size $\epsilon$, and call them $I_i$.  Let $A_i = f^{-1}(I_i)$.  It suffices to show that $P(X_n \in A_i) \to P(X \in A_i)$.  Note that $\partial A_i \subset D_f \cap f^{-1}(i\epsilon) \cup f^{-1}((i+1)\epsilon)$.  Then, we choose the partition so that the boundary points have $0$ mass(which is possible since the set of boundary points with positive mass is countable).  Hence, the boundary probability is $0$ and the result follows.
\end{proof}
\subsection{Convergence in $\mathbf{\R^d}$}
We can talk about distribution functions $F(X) \uparrow 1$ as $X \uparrow \infty$ and $F(X) \downarrow 0$ with $X \downarrow -\infty$ that are right continuous and monotone.  We also require that all rectangles have positive mass.  

Then, we define weak convergence as $F_n(x) \to F(x)$ for all continuity points of $X$.  In $d=1$, there were at most countably many discontinuity points.  In $d > 1$, this is false.  For example, take $Y = U[0, 1]$, $X= 0$ and $(0, Y)$.  The distribution function is given by 
$$F(x, y) = \begin{cases} 1, x \ge 0, y \ge 1 \\ y, x \ge 0, 0 \le y \le 1,\\ 0 \text{ else}.
\end{cases}$$
which is discontinuous at each $(0, y)$.

\begin{exercise} For each coordinate, the discontinuity points for each coordinate $D_i$ is countable.  
\end{exercise}

\begin{thm} $F_n(x) \to F(x)$ at continuity points is equivalent to $X_n \to X$ in distribution.
\end{thm}
\begin{proof}
If $X_n \to X$ in distribution, then $F_n(x) \to F(x)$ and $x$ is a continuity point, since if $x$ is a continuity point, the hyperplane passing through $x$ has mass $0$ so $P(X \in \partial A) = 0$ so $P(X_n \in A )\to P(X \in A)$, and by definition, this is $F_n(x) \to F(x)$.

If $F_n(x) \to F(x)$ for all continuity points, then given $A = (a_1 \times b_1] \times \dots \times (a_d \times b_d]$, then $P(X_n \in A) \to P(X \in A)$.  Observing that any open set can be approximated from the inside by a disjoint union of such rectangles, we find that $\liminf P(X_n \in U) \ge \lim P(X_n \in B) = P(X \in B) \approx P(X \in U)$.
\end{proof}
\section{October 27th, 2020}
\subsection{Fourier Inversion in $\R^d$}
For a distribution $F$ on $\R^d$ with $X \sim F$, 
$$\phi_X(t) = E(e^{i\left<t, x\right>}),$$
where $$\langle t, x \rangle = \sum_{i=1}^{d} t_i x_i.$$
We can show the inversion theorem generalizes:  take $A = [a_1, b_1] \times [a_2, b_2] \times \dots \times [a_d, b_d].$  Suppose $P(\partial A) = 0$.  

In 1D, recall that we had
$$\frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} - e^{-itb}}{it} \phi(t) \xrightarrow {T \to \infty}  \mu(a, b) + \frac{1}{2}\mu(\{a, b\}).$$
Let $t = (t_1, t_2, \dots, t_d)$.  Then 
$$(2\pi)^{-d}\int_{-T, T}^d \prod_{i=1}^d \psi(t_i, a_i, b_i) \phi(t_i) \to \mu(A),$$
where $\psi(t, a, b) = \frac{e^{-ita} - e^{-itb}}{it}.$  The result follows from applying Fubini's theorem to get 
$$\int \prod_{i=1}^{d} \int_{-T}^T \psi(t_i, a_i, b_i) e^{it_i x_i},$$
and $\int_{-T}^T \psi(t_i, a_i, b_i) e^{it_i x_i} \to 1(X_i \in (a_i, b_i)) + 1/2 1(X_i \in \{a_i, b_i\}).$

It follows that 
$$\int \prod_{i=1}^{d} \int_{-T}^T \psi(t_i, a_i, b_i) e^{it_i x_i} \to \mu(A^o) = \mu(A).$$
\subsection{Convergence of Characters}
\begin{thm} $X_n \Rightarrow X_{\infty}$ if and only if $\phi_{X_n} \to \phi_{X_\infty}$.  
\end{thm}
\begin{proof}
The forward direction follows from the definition of bounded continuous functions converging.  For the converse, we show that $\phi_{X_n} \to \phi_{X_\infty}$ implies tightness, which implies existence of subsequential limits, which implies that every subsequential limit has characteristic function $\phi_{X_\infty}$, which by inversion implies that the limit is unique.  We already have continuity at $0$.  Note that $X_n = (X_n^1, \dots, X_n^d),$ $X_{\infty} = (X_\infty^1, \dots, X_\infty^d).$  We have that $\phi_{X_n^i}(t) \to \phi_{X_\infty^i}(t)$.  It follows that each of $\{X_n^i\}_n$ are tight sequences.  

By tightness, there exists $P(X_n^i \in [-M_i, M_i]^c) < \frac{\epsilon}{d}$.  Then,
$$P(X_n \in (\bigtimes [-M_i, M_i])^c) \le \epsilon,$$
so $X_n$ is tight, as desired.
\end{proof}
\begin{proposition} $X_n \Rightarrow X_\infty$ if and only if $\langle \theta, X_n \rangle \Rightarrow \langle \theta, X_\infty \rangle$ for all $\theta \in \R^d$.
\end{proposition}
\begin{proof}
We could use the continuous mapping theorem or characteristics functions.  We could also use characteristic functions.  We know that $\phi_{X_n}(t\theta) \to \phi_{X_\infty}(t\theta)$, but note that $\phi_{X_n}(t \theta) = \phi_{\theta \cdot X_n }(t) \to \phi_{\theta X_\infty}(t).$  The other direction is clear taking $\phi_{\theta X_n}(1) = \phi_{X_n} (\theta) \to \phi_{X_\infty}(\theta).$
\end{proof}


\subsection{Multivariate Central Limit Theorem}

\begin{definition}  $X$ is said to have multivariate Gaussian distribution with covariance $\Sigma$ and mean $0$ if 
$$\phi_X(\theta) = e^{\theta^T \Sigma \theta / 2}.$$
\end{definition}
\begin{thm}[Multivariate CLT] If $X_1, \dots$ are iid random vectors with mean $0$ covariance $\Sigma$, then 
$$\frac{X_1 + X_2 + \dots + X_n}{\sqrt{n}} \xrightarrow{d} \mathcal N(0, \Sigma).$$ 
\end{thm}
\begin{proof}
The proof follows from using 1D CLT and using the lemma above.  
\end{proof}

\subsection{Poisson Processes}
\begin{thm}
Suppose we had a triangular arrow $\{X_n^i\} $ with $X_{i^n} \sim Ber(P_{n, i})$ where $\sum_{i=1}^n P_{n, i} \to \lambda$ and $\max_{1 \le i \le n} P_{n, i} \to 0$.  Then,
 $$\sum_{i=1}^n X_i^n \xrightarrow{d} Poi(\lambda)$$
 where $P(X=  k) = e^{i\lambda} \lambda^k/k!$.
 \end{thm}
 \begin{proof}
The characteristic function for a Poisson variable is
$$\phi_X(t) = \sum e^{itx} e^{-\lambda} \lambda^{k}/k! = e^{-\lambda} \sum (e^{it} \lambda)^k/k! = e^{-\lambda} e^{e^{it \lambda}} = e^{\lambda(e^{it} - 1)}.$$

The characteristic function of $Ber(P_{ni})$ is $$\phi(t) = 1 - P_{ni} + P_{ni} e^{it} = 1 + P_{ni}(e^{it} - 1).$$

Then,
$$\phi_{X_1^n + \dots + X_n^n}(t) = \prod_{i=1}^n (1 + P_{ni}(e^{it} - 1)) = A.$$
If we compute $e^{\sum P_{ni}(e_{it} - 1)} = B$.

We find that $$|A - B| \le \sum_{i=1}^n P_{ni} |e^{it} - 1|^2 \le 4 \sum_{i=1}^n P_{ni}^2 \le 4 \max P_{ni} \sum  P_{ni} \to 0.$$

Then $e^{\sum P_i (e^{it} - 1)} \to e^{\lambda(e^{it} - 1)} = \phi_{Poi(\lambda)}(t).$
\end{proof} 
\subsection{Signed Measures}
\begin{definition} In a space $(\Omega, \Sigma)$, we have $\mu : \Sigma \to (-\infty, \infty]$ so that $\mu(\emptyset) = 0$ and 
if $A = \bigcup E_i$ disjoint, then either $\mu(A) = \infty$ of $\mu(A) < \infty$.  In the first case $\sum \mu(E_i)_{-} < \infty$ and $\sum \mu(E_i)_+ = \infty$.  In the second case, we need $\sum |\mu(E_i)| < \infty$, which implies that $\sum \mu(E_i) = \mu(A)$ is well-defined.
\end{definition}
\begin{example}  If $\mu_1, \mu_2$ are finite measures, then $\mu_1 - \mu_2$ is a signed measure.
\end{example}
\begin{example}
In $(\Omega, \Sigma, \mu)$ $f$ is a measurable function with $\int f_- < \infty$ and $\int f_+ = \infty$, then $\nu(A) = \int_A f$ is a signed measure.
\end{example}

\begin{definition} A set $A \in \Sigma$ is positive if for all $B \subset A$ $\mu(B) \ge 0$.
\end{definition}
\begin{lemma} If $A_1, A_2, \dots$ are positive then so is $\bigcup A_i$.  
\end{lemma}



\begin{thm}[Hahn Decomposition Theorem] Given signed measures, there exists $A, B$ so that $A$ is negative, $B$ is positive, $A, B$ disjoint and 
$$\mu(E) = \mu(E\cap A) + \mu(E \cap B).$$
\end{thm}
\begin{lemma} Suppose $C$ is such that $\mu(C) < 0$.  Then there exists $D \subset C$ with $\mu(D) < 0$.
\end{lemma}
\pagebreak
\section{October 29th, 2020}
\subsection{Decomposition Theorems}

\begin{definition} A set $S$ is a null set if all its measurable subsets have $0$ measure.
\end{definition}

\begin{lemma} Suppose $C$ is such that $\mu(C) < 0$.  Then there exists $D \subset C$ with $\mu(D) < 0$.
\end{lemma}

\begin{proof}
We iteratively throw out sets with positive mass.  Is $S$ is negative, we are done.  Otherwise, there are sets of positive mass.  Let $n_1$ be the smallest integer such that there exists $E_1$ with $\mu(E_1) \ge \frac{1}{n_1}$.  Then, consider $S \setminus E_1$ and repeat.  

If the process stops in finite steps, we are done, since $S \setminus (E_1 \cup E_2 \cup \dots\cup E_k)$ is a negative set for some $k$.  If not, let $S' = S \setminus \bigcup_{i=1}^{\infty} E_i$.  Because $\mu$ is a signed measure,
$$\mu(S) = \mu(S') + \sum_{i=1}^{\infty} \mu(E_i).$$
Note that $\mu(S) < 0$ and $\mu(S') < 0$, so we must have $\sum_{i=1}^{\infty} \mu(E_i) < \infty$.  Then $S'$ must be negative.  If not, there exists $F \subset S'$ with $\mu(F) > 1/N$ for some N.  But after some $i_0$, $\mu(E_i) < \frac{1}{n}$, so we would have thrown away $F$ instead of the $E_i$, a contradiction.
\end{proof}
Now we proof the Hahn Decomposition Theorem:
\begin{proof}
If there does not exist any set $S$ with negative measure, we are done.    If not, there exist negative sets.  Let $\alpha = \inf\{\mu(C) : C \text{ negative}\}.$  Let $C_1, C_2, \dots$ be increasing negative sets with $\mu(C_i) \downarrow \alpha$.  We have shown that $B = \bigcup_{i=1}^{\infty} C_i$ is negative.  Then $\mu(B) \ge \alpha$ since $\alpha$ is the infimum.  However, $\mu(B) \le \mu(C_i)$ for all $i$, so $\mu(B) \le \alpha$, which implies that $\mu(B) = \alpha$.

We claim that $B^c$ is positive.  If not, it contains a set with $\mu(D) < 0$, which implies there exists $E \subset D$ with $\mu(E) < 0$ and $E$ is a negative set.  Hence $B \cup E$ is a negative set with $\mu(B \cup E) < \alpha$, a contradiction.

The composition is then $\Omega = B \cup B^c$.  Suppose there is a set $C \cup C^c$ with $C$ negative and $C^c$ positive.  Then $C \cap B^c$ is both negative and positive so it is a null set.  Similarly, $C^c \cap B$ is a null set.  
\end{proof}
\begin{definition} Meausres $\mu, \nu$ are mutually singular if there is a set $A$ with $\mu(A) = 0$ and $\nu(A^c) = 0$.
\end{definition}
\begin{thm}[Jordan Decomposition] For $\alpha$ a signed measure, there exists mutually singular measures $\alpha^+$ and $\alpha^-$ such that $\alpha = \alpha^+ - \alpha^-$.
\end{thm}
\begin{proof}
We take $\alpha^+ = \alpha \vert_{B^c}$ and $\alpha^- = -\alpha \vert_B$ where $\Omega = B \cup B^c$ is the Hahn Decomposition.  In this case $\alpha^+$ and $\alpha^-$ are unique, since null sets are measure $0$ and don't change the value of the measures.
\end{proof}
\begin{thm}[Lebesgue Decomposition Theorem]
If $\mu_1, \mu_2$ are finite measures, then $\mu_2 = \mu_2^a + \mu_2^s$ such that there exists $g \ge 0$ with $\mu_2^a(E) = \int_E g d\mu_1$ and $\mu_2^s$ is singular with respect to $\mu_1$.
\end{thm}
\begin{proof}
We extract $\mu_2^a$ from $\mu_2$.  Then, consider $\mathcal H = \{h : h \ge 0, \int h\, d\mu_1 \le \mu_2(E), E \in \Sigma\}.$  We want $g$ is to be the maximal element of $h$.    Note that if $h_1, h_2 \in \mathcal H$ then $h_1 \vee h_2 \in \mathcal H$.  If we take $A : \{h_1 > h_2\}$, then 
$$\int_E h_1 \vee h_2 \, d\mu_1 = \int_{E \cap A} h_1 \, d\mu_1 + \int_{E \cap A^c} h_2\, d\mu_1 \le \mu_2(E \cap A) + \mu_2(E \cap A^c) = \mu_2(E).$$

Let $\alpha = \sup_{h \in \mathcal H} \int h d\mu_1$.  Let $\{h_i\}$ be so that $\int h_i d\mu_1 \uparrow \alpha$.  Then $g = \lim h_i$ is well defined since $h_i$ are increasing and 
$$\int_E g \, d\mu_1 = \lim \int_E h_i\, d\mu_1 \le \mu_2(E).$$
Hence, $g \in \mathcal H$ and by MCT, $\int g = \alpha$.  

Define $\mu_2^a(E) = \int_E g\,d\mu_1$.  It suffices to show that $\mu_2' = \mu_2 - \mu_2^a$ is singular with respect to $\mu_1$.

Consider the signed measure $\mu_2' - \epsilon \mu_1$ and consider the positive and negative part $A_\epsilon, B_\epsilon$.  If $\mu_1(A_\epsilon) > 0$ then $g + \epsilon 1_{A_\epsilon} \in \mathcal H$ since 
$$\int g + \epsilon 1_{A_\epsilon} d\mu_1 = \mu_2^a(E) + \epsilon \mu(1(A_\epsilon \cap E) \le \mu_2^a(E) + \mu_2'(E)= \mu_2(E).$$
However, $$\int g + \epsilon 1_{A_\epsilon}d\mu_1 = \alpha + \epsilon \mu_1(A_\epsilon) > \alpha,$$
a contradiction.  

For any $\epsilon$, the positive part of $\mu_2' - \epsilon \mu_1$, $A_\epsilon$ has $0$ $\mu_1$-mass.  Where $B_\epsilon$ is the negative part, take $A = \bigcup_{n=1}^{\infty} A_{1/n}$ and $B = \bigcap_{n=1}^{\infty} B_{1/n}$.  We claim that $\mu_2'(B) = 0$.  If $\mu_2'(B) > 0$, then $\mu_2'(B) - \epsilon \mu_1(B) > 0$ for some small $\epsilon$, but $B$ is contained in the negative set $B_\epsilon$.  

The uniqueness is easy to show.
 \end{proof}
 
 \begin{definition} $\mu_2$ is said to be absolutely continuous with respect to $\mu_1$( denoted $\mu_2 << \mu_1$) if for any $A$ with $\mu_1(A) = 0$, $\mu_2(A) = 0$.
 \end{definition}
 
 \begin{thm}[Radon-Nikodym Theorem] Suppose $\mu, \nu$ are $\sigma$-finite measures and $\nu$ is absolutely continuous with respect to $\mu$, then there is a $h \ge 0$, so that $\nu(E) = \int_E h\, d\mu$.
 \end{thm} 
\begin{proof}
By the Lebesgue decomposition, $\mu_2 = \mu_2^a + \mu_2^s$, but $\mu_2 << \mu_1$, $\mu_2^s = 0$.    It's easy to check the uniqueness of $h$ almost surely.
\end{proof}
\subsection{Conditional Expectation}
\begin{definition}  Suppose we have $(\Omega, \Sigma, P)$ and a random measurable variable $X$ with $E|X| < \infty$.  Let $\mathcal F \subset \Sigma$, another $\sigma$-algebra.  We define $E(X | \mathcal F)$ as an $\mathcal F$ measurable function with the property that for all $S \in \mathcal F$,
$$\int_S E(X | \mathcal F) d\mu = \int_S X d\mu.$$ 
\end{definition}
\begin{lemma} 
$|E(X|\mathcal F)|$ is in $L^1$.  
\end{lemma}
\begin{proof}
Let $A = \{E(X|F) > 0\} \in F$ .  Then
$$\int |E(X|F)| = \int_A E(X|F) - \int_{A^c} E(X|F) = \int_A X - \int_{A^c} X \le \int |X|.$$
\end{proof}

We first address \textbf{existence}.  Assume $X \ge 0$.  Define a measure $\mu'$ on $(\Omega, \mathcal F)$ where $\mu'(E) = \int_E X\, d\mu_1.$  We have that $\mu'$ is a measure on $(\Omega, \mathcal F)$.  It is easy to check that $\mu' << \mu_1$.  By RN, there exists $g$ measurable with $\mu'(E) = \int_E g\, d\mu_1$.  The theorem also shows uniqueness almost surely, so we define $E(X|F) = g$.

Now we handle uniqueness.  Suppose $h_1, h_2$ are two versions of $E(X|F)$.  Then
 $$\int_E h_1 = \int_E h_2 = \int_E X$$
 and $\int h_1 - h_2 = 0$, so $h_1 = h_2$ almost surely.

Some facts about conditional expectations:
\begin{itemize}
\item Suppose $B \in F$ and $X_1 = X_2$ on $B$.  Then $E(X_1|F) = E(X_2|F)$ almost surely on $B$.
\item Linearity: $X, Y \in \Sigma$, then $E(aY + bY | F) = aE(X|F) + bE(Y|S)$ almost surely.
\item Monotonicity: $X \le Y$ implies that $E(X|F) \le E(Y|F)$ almost surely.  
\item $X_n \ge 0$ and $X_n \uparrow X$, $E(X) < \infty$ then $E(X_n|F) \uparrow E(X|F)$.  Then, we adapt the proof of MCT to prove this.  
\end{itemize}
\pagebreak
\section{November 3rd, 2020}
\subsection{Properties of Conditional Expectations}
We have a mapping $(\Omega, \Sigma, P) \xrightarrow{X} (\R, B(\R))$, and a subsigma algebra $\mathcal F$ so that $E(X|\mathcal F)$ is the $\mathcal F$-measurable function such that 
$$\int E(X|\mathcal F) = \int_A X$$
for all $A \in \mathcal F$.


\begin{remark} 
In a sense, this corresponds to the maximum information of $X$ given the subsigma algebra $\mathcal F$.  
\end{remark}

\begin{exercise}
 State and prove versions of Fatou's Lemma, MCT, and DCT for conditional expectation.
\end{exercise}
\begin{thm}[Jensen's Inequality] Let $\phi$ be a convex function and $X$ a random variable so that $E|X|, E|\phi(X)| < \infty$.  Then
$$\phi(E(X|\mathcal F)) \ge \phi(E(X | \mathcal G)) \text{ almost surely.}$$
\end{thm}

\begin{thm}[Tower Property] Given $\mathcal G_1 \subset \mathcal G_2 \subset \Sigma$,
$$E(E(X|\mathcal G_2)|\mathcal G_1) = E(X|\mathcal G_1) \text{ almost surely.}$$
\end{thm}
\begin{proof}
For $A \in \mathcal G_1$,
$$\int_A E(E(X|\mathcal G_2)|\mathcal G_1) = \int_A E(X|\mathcal G_2) = \int_A X = \int_A E(X|\mathcal G_1).$$
\end{proof}

\begin{thm}  For $\mathcal G \subset \Sigma$, $X$ is a random variable that is independent of $\mathcal G$, which means that $\mathcal G$ and $\Sigma(X)$ are independent as subsigma algebras of $\Sigma$.  Then 
$$E(X|\mathcal G) = E(X), a.s.$$
\end{thm}
\begin{proof}
For two random variables $Y, S$ independent, $E(YZ) = E(Y)E(Z)$.  Then 
$$\int_A E(X|G) = \int_A X = \int X1_A = E(X)P(A) = \int_A E(X).$$
\end{proof}
\begin{thm}  Suppose $X, Y$ are random variables with $Y$ $\mathcal G$-measurable with $E|X| < \infty$ and $E|XY| < \infty$.  Then $E(XY|G) = YE(X|G)$.  
\end{thm}
\begin{proof}
Take $Y=1_B$ with $B \in \mathcal G$.  Then $E(X1_B|G) = 1_BE(X|G)$.  Then 
$$\int_A E(X1_B|G) = \int_A X1_B = \int_{A \cap B} X= \int_A E(X|G)1_B.$$
\end{proof}

\begin{thm} Suppose $X \in L^2$.  If we take $\mathcal G \subset \Sigma$ and $Y$ to be a $G$ measurable function in $L^2$.  Then $E((X-Y)^2) \ge E((X - E(X|G))^2)$.
\end{thm}  
\begin{remark} The geometric interpretation of this result is that $E(X|\mathcal G)$ is the projection of $X$ onto $L^2(\mathcal G)$.
\end{remark}
\subsection{Regular Conditional Probabilities}
We will call $P(A|\mathcal G) = E(1_A| \mathcal G)$.  For any $A_1, A_2, \dots$ disjoint,
$$P(\bigcup A_i | \mc G) = \sum P(A_i \mc G), a.s.$$
by applying $MCT$ for conditionals to the function $\sum_{i=1}^n 1_{A_i}$.  However, one can face issues to ensure the almost surely condition holds simultaneously for all such families of sets.  This is because the nullsets can depend on the set system $\{A_i\}$, which means for no $\omega \in \Omega$, we can expect $P(\bigcup A_i | \mathcal G) = \sum_{i=1}^n P(A_i | \mathcal G)$ for all sets $A_i$. 


\begin{definition}[Regular Conditional Probability].  If we have $f: \Omega \times B(\R)$, for all $A \in B(\R)$, $$f(\cdot, A): \Omega \to [0, 1]$$
is a version of $P(A|\mathcal G)$ and almost surely, $f(\omega, \cdot)$ is a probability measure.
\end{definition}
\begin{remark} These only exist in the case of "nice spaces", namely Polish spaces.  The key thing that make the construction possible is the separability.
\end{remark}
\subsection{Martingales}
We have a stochastic process $X_1, X_2, \dots, X_n, \dots$ measurable random variables with $E|X_n| < \infty$ for all $n$.  
\begin{definition} A filtration is a sequence of $\Sigma$-algebras,
$$F_0 \subseteq F_1 \subseteq \dots \subset \Sigma.$$
\end{definition}
We assume $X_n$ is $F_n$-measurable.  Since $X_n$ is in $L^1$, we discuss $E(X_n|F_m)$ for $m < n$.  If $m \ge n$ then $E(X_n|F_m) = X_n$ almost surely.

\begin{definition} A process $H_1, H_2, \dots$ is predictable if $H_n$ is $F_{n-1}$ measurable.
\end{definition}

\begin{definition} Suppose $\{X_n\}$ is $F_n$ adapted.  Then $X_n$ is a martingale if for all $n$, $E(X_n|F_{n-1}) = X_{n-1}$ almost surely.
\end{definition}
\begin{example}[Linear Martingale]  Suppose $Z_i$ are all iid $Ber(\pm 1)$.  Then $X_n = \sum_{i=1}^n Z_i$, $F_n = \sigma(Z_1, \dots, Z_n)$.  
$$E(X_n|F_{n-1}) = E(Z_n + X_{n-1}|F_{n-1}) = X_{n-1} + E(Z_n|F_{n-1}) = X_{n-1}.$$
\end{example}

\begin{definition} A submartingale is when $E(X_n|F_{n-1}) \ge X_{n-1}$ almost surely, and a supermartingale is when $E(X_n | F_{n-1}) \le X_{n-1}$ almost surely.
\end{definition}
\begin{example}[Exponential Martingale] If $X_n$ are iid, $E(X_n) = 1$.  $M_n = \prod_{i=1}^n X_i$ is a martingale.
\end{example}
\begin{thm}
Given a martingale $X_n$, we can generate a new martingale using predictable processes.  Suppose $H_i$ predictable $X_n$ is a martingale.  Then $$Y_n = \sum_{i=1}^n H_n(X_n - X_{n-1})$$ is a martingale.
\end{thm}
\begin{proof}
Note that $Y_n$ is $F_n$-adapted since $X_n, H_n$ are $F_n$-adapted.  Then 
\begin{align*}
E(Y_n|F_{n-1}) &= E(\sum_{i=1}^{n-1} H_i(X_i - X_{i-1}) + H_n(X_n - X_{n-1}) | F_{n-1}) \\
&= Y_n + E(H_n(X_n - X_{n-1}) | F_{n-1}) \\
&= Y_n.
\end{align*}
\end{proof}
\pagebreak
\section{November 5th, 2020}
\subsection{Martingales, Continued}
Recall the setting: we have a filtration $\mc F_1 \subseteq \dots \subseteq \mc F_n$ and $X_n$ adapted to $(F_n)$ in $L^1$ if $E(X_n|\mc F_{n-1}) = X_{n-1}$ almost surely.  We can extend this to $E(X_n|\mc F_m) = X_m$ for $m \le n$ by iteratively using the tower property.

\begin{example} A simple random walk $S_n = \sum_{i=1}^n X_i$, where $X_i$ are iid $Ber(\pm 1)$, is a martingale.  If we change $X_i \sim Ber(p)$, then it becomes a submartingale because the mean of each entry is strictly positive.  
\end{example}

\subsection{Predictable Sequences}
\begin{definition} A sequence $H_n \in F_{n-1}$ is called predictable.
\end{definition}
Given a martingale sequence $X_n$ and a predictable process $H_n$, we showed that the process $M_n = \sum_{i=1}^n H_i(X_i - X_{i-1})$ was a martingale, where $M_0 = 0$.

\begin{proposition}Suppose $X_n$ is a submartingale.  Define $M_n = \sum_{i=1}^n H_i(X_i - X_{i-1})$.  If $H_n \ge 0$ and is predictable, then $M_n$ is a submartingale.
\end{proposition}
\begin{proof}
\begin{align*}
E(M_n|\mc F_{n-1}) &= M_{n-1} + E(H_{n}(X_n - X_{n-1})| \mc F_n) \\
&= M_{n-1} + H_nE(X_{n} - X_{n-1}| \mc F_n ) \\
&\ge M_{n-1}.
\end{align*}
\end{proof}

\subsection{Stopping Times}
\begin{definition} A stopping time $\tau$ is a random variable from $(\Omega, \Sigma, \P) \to \N \cup \infty$ such that the event $\{\tau \le n\}$ is $\mc F_n$ measurable.
\end{definition}
\begin{remark} Given the filtration, then we should be able to consider the information up to time $n$ to determine whether $\{\tau \le n\}$ is measurable.
\end{remark}
\begin{example} Consider the symmetric random walk $S_n = \sum X_i$ which are iid $Ber(\pm 1)$.  Let $F_n = \sigma(X_1, X_2, \dots, X_n)$.  We can pick the stopping time $\tau_0$ as the minimum time so that $S_n = 0$.  

In general, if we look at the last hitting time: $\hat{\tau}$, the max $n$ so that $S_n = 0$ is not a stopping time since we would need to consider all the noise in the future as well.  
\end{example}

\begin{thm} Suppose $X_n$ is a martingale and $\tau$ is a stopping time.  The process $Z_n = X_{n \wedge \tau}$ is a martingale.
\end{thm}
\begin{proof}
The strategy is to show that $Z_n$ is a transform of $X_n$.  To do this, we have to construct a predictable process $H_n$ such that $(H \cdot X) = Z$.  Define $H_n = 1\{\tau \ge n\}$. 

Then, 
$$\sum_{i=1}^n H_i(X_i - X_{i-1}) = X_{n \wedge \tau} - X_0.$$

$H_i$ is predictable since $\tau$ is a stopping time, and $H_i = 1_{Z \ge i} = 1 - 1\{Z \le i-1\}$, which is $\mc F_{i-1}$ measurable.  Hence $X_n$ is a martingale so $X_{n \wedge \tau} - M_0$ is a martingale, so $X_{n \wedge \tau}$ is a martingale.
\end{proof}
\begin{remark} Observe that the same holds for submartingales and supermartingales.
\end{remark}

\subsection{Expectation for Stopped Martingales}
\begin{lemma} If $\tau_1 \le \tau_2$ almost surely for two stopping times, then $E(X_{n \wedge \tau_1}) \le E(X_{n \wedge \tau_2})$ for a submartingale $X_n$.
\end{lemma}
\begin{proof}
It suffices to show that $E(X_{n \wedge \tau_1}) \le E(X_n)$.  It $\tau_1 \ge n$, the result is clear.  

We saw that $H$ is a predictable sequence and $M_n = X_{n \wedge \tau}  - X_0 = (H\cdot X)$.    Call $Z_n = (1 - H) \cdot X$, which is also a submartingale.  

Then $Z_n + M_n = X_n - X_0$ is a submartingale, so $E(Z_n) \ge 0$ for all $n$.  Finally, $$E(Z_n) + E(M_n) = E(X_n) - E(X_0) \Longrightarrow E(M_n) \le E(X_n) - E(X_0)$$
but $E(M_n) = E(X_{n \wedge \tau}) - E(X_0)$, so it follows that $E(X_{n \wedge \tau}) \le E(X_n)$.

Define $Z_n = X_{n \wedge \tau_2}$.  We know that $Z_n$ is a submartingale.  Then $E(Z_{n \wedge \tau_1}) \le E(Z_n) = E(X_{n \wedge \tau_2})$.
\end{proof}
\begin{remark} The same argument shows that if $X_n$ is a martingale, then $E(X_{n \wedge \tau}) = E(X_n) = E(X_0)$ for all $n$ and stopping times $\tau$.  However, it does not follow generally that $E(X_\tau) = E(X_0)$.
\end{remark}
\begin{example}  Let $S_0 = 1$, $S_i = 1 + \sum_{i=1}^n X_i$, for $X_i \sim Ber(\pm 1)$.  Take $\tau = \min\{n : S_n = 0\}$.  One can show that $\tau < \infty$ almost surely.  However, $S_0 = 1$, $ES_\tau = 0$.
\end{example}

\subsection{Convex Functions and Martingales}
The following theorem will prove to be useful.
\begin{thm} Suppose $\phi$ is a convex function and $X_n$ is a martingale.  Then $\phi(X_n)$ is a submartingale.
\end{thm}
\begin{proof}
This follows from the conditional Jensen's inequality. 
$$E(\phi(X_n)| \mc F_{n-1}) \ge \phi(E(X_n|\mc F_{n-1})) = \phi(X_{n-1}).$$
The above also works in $X_n$ is a submartingale and $\phi$ is increasing as well.
\end{proof}
We often choose $\phi(X) = X^+$ or $\phi(X) = (X-a)^+$.
\begin{thm}[Doob's Maximal Inequality]  Suppose $X_n$ is a submartingale,  Then for any $a > 0$, 
$$P(\max_{0 \le k \le n} X_k \ge a) \le \frac{E(X_n^+)}{a}.$$
\end{thm}
\begin{proof}
Define $\tau$ to be the stopping time, $\{\inf_{0 \le k \le n} X_k \ge a \}$.  We know that $X_{k \wedge \tau}$ is a submartingale so $E(X_{n \wedge \tau}) \le E(X_n)$.  The LHS can be decomposed in $E(X_{n \wedge \tau}1_{\tau \le n}) + E(X_{n \wedge \tau}1_{\tau > n})$, and $$E(X_{n \wedge \tau}1_{\tau > n}) = E(X_n 1_{\tau > n}),$$
so it follows that 
$$E(X_{n \wedge \tau}1_{\tau \le n}) \le E(X_n) - E(X_n 1_{\tau > n}) = E(X_n 1_{\tau \le n}) \le E(X_n^+).$$
On the other hand, 
$$E(X_{n \wedge \tau}1_{\tau \le n}) = E(X_\tau 1_{\tau \le n}) \ge a P(\tau \le n),$$
so it follows that 
$$aP(\tau \le n) \le E(X_n^+),$$
which gives the result.
\end{proof}
\begin{remark} Recall the Kolmogorov Maximal Inequality, $X_i$ are iid mean $0$, $EX_i^2 < \infty$, $S_k = \sum_{i=1}^k X_i$.  Then $$P(\max_{0 \le k \le n} |S_k| \ge x) \le Var(S_n)/x^2.$$
This is a corollary of Doob's by considering the submartingale $M_n = S_n^2$.
\end{remark}


\pagebreak
\section{November 10th, 2020}
\subsection{Convergence of Martingales}
Recall Doob's inequality: If $X_n$ is a sub-martingale, then $$P(\sup_{0 \le k \le n} X_k > x) \le E(X_n^+)/x.$$

We begin by proving the upcrossing inequality: 
\begin{thm}[Upcrossing Inequality] Suppose $X_n$ is a sub-martingale.  Pick $a \le b$.  Define $\tau_0: \inf t: X_t \le a$, $\tau_1: \inf t \ge \tau_0, X_t \ge b, \dots$.  Each of $\tau_0 \to \tau_1$, $\tau_2 \to \tau_3, \dots$ is an upcrossing.  Let $N(a, b, n)$ be the number of crossings up to time $n$.  
$$E(N(a, b, n)) \le \frac{E((X_n - a)^+) - E((X_0 - a)^+)}{b-a}.$$
\end{thm}
\begin{proof}
Consider the sub-martingale $M_n = (X_n - a)^+ + a$, since $f(x) = (x-a)^+ + a$ is convex and increasing.

Define the predictable process $$H_k = 1_{\tau_0 < k \le \tau_1} + 1_{\tau_2 < k \le \tau_3} + \dots.$$

We know that $H \cdot M$ is a sub-martingale.  $M_n - M_0 = H \cdot M + (1-H) \cdot M$ is a submartingale so 
$$E(M_n - M_0)\ge E(H \cdot M).$$
We know that 
$$E((X_n - a)^+ - E(X_0 - a)^+)\ge H \cdot M \ge (b-a)N(a, b, n) + \text{ some correction}.$$
\end{proof}

\begin{thm}[Martingale Convergence Theorem] If $X_n$ is a submartingale with $\sup EX^+ < \infty$, then as $n \to \infty$, $X_n $ converges almost surely to a limit $X$ with $E|X| < \infty$.
\end{thm}
\begin{proof} Fix $a < b$.  Then 
$$E(N(a, b, n)) \le \frac{E((X_n - a)^+)}{b-a} \le \frac{E(X_n^+ + |a|)}{b-a}.$$

Then $E(N(a, b, \infty)) < \infty$ since $E(X^+) < \infty$ uniformly in $n$.  Now, choosing $a < b \in \Q$, one can ensure almost surely that 
$$N(q_1, q_2, \infty) <\infty$$
for all $q_1 < q_2 \in \Q$.  It follows that 
$$\bigcap_{a, b \in \Q} \{N(a, b, \infty) < \infty\}$$
is probability $1$, so it implies that $X_n$ has a limit.  We show that $\limsup X_n = \liminf X_n$.  If not, one can find $$\liminf X_1 < q_1 < q_2 < \limsup X_n.$$

This implies that we have infinitely many crossing between $\liminf X_n$ and $\limsup X_n$ and hence between $q_1, q_2$, which is a contradiction. 

Finally, we show $E|X| < \infty$.  $EX^+ < \infty$ and $X_n \to X$ so $X_n^+ \to X_n^+$.  By Fatou's Lemma, $EX^+ \le \liminf E(X_n^+) < \infty$.  Then,
$$\sup E(X_n^-) = \sup E(X_n^+) - E(X_n)\le \sup E(X_n^+) - E(X_0) < \infty.$$
Using Fatou's lemma again shows that $EX_n^- < \infty$, so we have $E|X| < \infty$.
\end{proof}
\begin{corollary} If $X_n \ge 0$ is a supermartingale, then $X_n \to X$ almost surely so that $E|X| < \infty$.
\end{corollary}
\begin{proof}
$Y_n = -X_n$ is a sub-martingale and $Y_n^+ = 0$ so $EY_n^+ = 0$.  Hence $\sup E(Y_n^+) < \infty$.  The previous result shows that $Y_n \to Z \in L^1$ almost surely so $X_n \to -Z$.
\end{proof}
\subsection{Martingales with Bounded Increments}
\begin{thm} Suppose $X_n$ is a martingale sequence satisfying $|X_n - X_{n-1}| < K$ almost surely for all $n$.  Then, define 
$$A = \{X_n \text{ converges}\}, B = \{\limsup X_n = \infty, \liminf X_n = -\infty\}.$$
Then $P(A \cup B) = 1$. 
\end{thm}
\begin{proof}
Let $\tau = \inf \{t : X_t \ge M\}$.  WLOG take $X_0 = 0$, since $X_n - X_0$ is a martingale.    Define $Y_n = X_{\tau \wedge n}$, which is a martingale.  Then $Y_n^+ \le M+K$ so $EY_n^+ \le M+K$ for all $n$.  By the Martingale Convergence Theorem, $Y_n$ converges almost surely implies that $X_n$ converges on the event $\tau = \infty$.  Sending $M$ to infinity,
$$\bigcap_{M = 1}^{\infty} \{Y_n \text{ converges}\}.$$
It follows that $X_n$ converges on the event $\bigcup_{m=1}^{\infty} \{\tau_m = \infty\}$.  Equivalently, $\limsup X_n < \infty$.  On the event $\limsup X_n < \infty$, $X_n$ converges.  Because $X_n$ is a martingale, we can work with $-X_n$ to conclude $X_n$ converges on the event $\{\liminf X_n > -\infty\}$.  So either $X_n$ converges, or $\limsup X_n = \infty$ and $\liminf X_n = -\infty$.
\end{proof}
\subsection{Lp inequalities}
\begin{thm}
If we have submartingale sequence $X_n$ and $E|X_n|^P < \infty$ for $1 < p < \infty$ for all $n$ then 
$$E(\max_{0 \le k \le n} X_k^+)^p \le \left (\frac{p}{p-1}\right )^p E((X_n^+)^p).$$
\end{thm}
\begin{remark} The statement is \textbf{false} for $p = 1$.
\end{remark}
\begin{proof}
Let $Y_n = \max_{0 \le k \le n} X_k^+.$  By Doob's Maximal inequality, $P(Y_n > \lambda) \le E(X_n^+1{Y_n > \lambda})/\lambda$, so it follows that
\begin{align*}
E(Y_n^p) &= \int p\lambda^{p-1} P(Y_n > \lambda) d\lambda \\
& \le \int p\lambda^{p-2} E(X_n^+ 1_{Y_n > \lambda}) \\
&= \int p\lambda^{p-2} \int X_n^+ 1_{Y_n > \infty} \\
&= p \int X_n^+ \int_0^{Y_n} \lambda^{p-2} \\
&= \frac{p}{p-1} \int X_n^+ (Y_n)^{p-1}.
\end{align*}
By Holder's inequality,
$$E(Y_n^p) \le \frac{p}{p-1} E(X_n^+ Y_n^{p-1}) \le \frac{p}{p-1} E((X_n^+)^p)^{1/p} E(Y_n^{p})^{(p-1)/p}.$$

Dividing by both sides,
$$E(Y_n^p) \le\left( \frac{p}{p-1}\right)^pE((X_n^+)^p).$$
However, $E(Y_n^p)$ could be infinite.  To fix this, one can show by truncation the same inequality holds for the truncated version, 
$$E(|Y_n \wedge M|^p) \le \left (\frac{p}{p-1}\right )^p E(X_n^+)^p.$$
Taking $M \to \infty$ and applying the monotone convergence theorem gives the result.
\end{proof}
\pagebreak
\section{November 12th, 2020}
\subsection{Lp Inequalities, continued}
Last time, we used Doob's Maximal Inequality to show that 
$$E(|\max_{i \le n} X_i^+|^[) \le \left(\frac{p}{p-1}\right)^p E|X_n|^p.$$
\begin{example}[Counterexample for $p = 1$] Take a simple random walk with $S_0 = 1, S_1 = X_1, S_2 = X_1 + X_2$ where $X_i \sim Ber(\pm 1)$, iid.  Let $\tau = \inf \{t : S_t = 0\}$.  $S_{n \wedge \tau}$ is a non-negative martingale.

Then, $E(S_{n \wedge \tau})  = ES_0 = 1$.  We will show that 
$$P(\max_{i \le n} S_{n \wedge \tau} > M) = \frac{1}{M},$$
so it follows that 
$$E(\max_{i \le n} S_{n \wedge \tau}) \approx \sum_{m=1}^{\infty} \frac{1}{m} \to \infty.$$
\end{example}

\subsection{$L^p$ Convergence}
\begin{thm} Suppose $X_1, X_2, \dots$ is a martingale sequence with $\sup E(|X_n|^p) < \infty$.  Then, there exists a random variable $X$ so that $E|X_n - X|^p \to 0$.
\end{thm}
\begin{proof}
$X_n \in L^p$ implies that $X_n \in L^1$ and $E(|X_n|)^p \le E(|X_n|^p)$.  So $\sup E|X_n| < \infty$ and By Fatou's lemma, $E|X|^p \le \liminf E(|X_n|^p) < \infty$, so $X_n \to X$ almost surely by martingale convergence.  

Using the $L^p$ maximal inequality, $|X_k|$ is a submartingale so 
$$E(\max_{k \le n} |X_k|^p) \le (p/(p-1))^p E(|X_n|^p).$$
Sending $n \to \infty$ and using MCT, it follows that 
$$E(\sup |X_n|^p) < \infty.$$

Finally, $X_n \to X$ so $|X_n - X| \le 2\sup |X_n|$. It follows that $|X_n - X|^p \le (2 \sup |X_n|)^p$ and using the dominated convergence theorem, it follows that $E|X_n - X|^p \to 0$.
\end{proof}
\begin{example} From our simple random walk from earlier, $S_{n \wedge \tau} \ge 0$ is a martingale so it converges.  We can show that $S_{n \wedge \tau} \to 0$, since $S_{n \wedge \tau} \to X$ and $|S_{n+1 \wedge \tau} - S_{n \wedge \tau}| = 1$ unless $S_{n \wedge \tau} = 0$.  But $ES_{n \wedge \tau} = 1$, so $S_{n \wedge \tau} \not \to 0$ in $L^1$ since that implies that $ES_{n \wedge \tau} \to 0$.
\end{example}

\begin{thm}[Doob's Decomposition] Suppose $X_n$ is an $\mc F_n$ adapted submartingale.  Then, there exists a unique predictable non-negative process $A_n$ with $A_0 = 0$ such that $X_n = M_n + A_n$ where $M_n$ is an $\mc F_n$ adapted martingale.
\end{thm}
\begin{example} Take $S_n = \sum_{i=1}^n Z_i$ where $Z_i \sim Ber(p)$.  Note that $E(Z_i) = p$ so it is a sub-martingale.  We know that $M_n = \sum Z_i - p$ is a martingale since $E(Z_i - p) = 0$. Then $S_n = M_n + np$.  In this case $A_n = np$.
\end{example}
\begin{proof}
We have $X_n = M_n + A_n$, then $E(X_n | \mc F_{n-1}) = M_{n-1} + A_n = X_{n-1} - A_{n-1} + A_n$.  Hence,
$$E(X_n|\mc F_{n-1}) - X_{n-1} = E(X_n - X_{n-1}|\mc F_{n-1}) = A_n - A_{n-1} \ge 0,$$
since $X_n$ is a submartingale.  Then, we have $A_0 = 0$ and $$A_n = \sum_{m=1}^n E(X_m - X_{m-1}|\mc F_{m-1}).$$

$A_n$ is predictable since it is $F_{n-1}$ measurable.  Then, $M_n = X_n - A_n$.    This is a martingale since 
$$E(M_n | \mc F_{n-1}) = E(X_n | \mc F_{n-1}) - A_n = X_{n-1} - A_{n-1}= M_{n-1}.$$
\end{proof}
\subsection{$L^1$ Convergence, Uniform Integrability}
\begin{definition} $X_n$ is said to be uniformly integrable if given $\epsilon > 0$, there exists $M$ so that 
$$E(|X_n|1_{|X_n| > M}) < \epsilon.$$
\end{definition}
\begin{thm} If $X_n$ is a martingale, then the following are equivalent:
\begin{itemize}
\item $X_n$ is UI.
\item $X_n \to X$ almost surely and in $L^1$ for some $X$.
\item $X_n = E(X|\mc F_n)$.
\end{itemize}
\end{thm}
\begin{proof}
Uniform integrability implies that $\sup E|X_n| < \infty$.  By the martingale convergence theorem, we have $X$ so that $X_n \to X$ almost surely and $X \in L^1$.  Convergence in $L^1$ was a homework problem(9.5b).  

For 2 implies 3, we want to show that $X_n \to X$ as, L1 implies that $X_n = E(X|\mc F_n)$.  It suffices to show that 
$$\int_aA X_n = \int_A X$$
for $A \in F_n$.  Notice that $\int_A X_n = \int_A X_m$ for $m \ge n$ by the martingale property($E(X_m | \mc F_n) = X_n$).  It follows that $$|E(X; A) - E(X_m; A)| \le E(|X - X_m|; A) \le E|X - X_m| \ to 0.$$

For 3 implies 1, $X_n = E(X|\mc F_n)$ implies that $X_n$ is a martingale since 
$$E(X_n|\mc F_{n-1}) = E(E(X|\mc F_n) | F_{n-1}) = E(X|\mc F_{n-1}) = X_{n-1}.$$
This was a homework problem.
\end{proof}
\begin{corollary} $X$ is a random variable, then $X_n = E(X|\mc F_n)$ is a martingale.  We know that $X_n \to Y$ almost surely and in $L^1$.  Let $\mc F_\infty = \sigma(\bigcup_{n=0}^{\infty} \mc F_n)$.  Then, $Y = E(X|\mc F_\infty)$.
\end{corollary}
\begin{proof}
It suffices to show that $E(X; A) = E(Y; A)$ for $A \in \mc F_\infty$.  Instead, we show that $E(X; B) = E(Y; B)$ for all $B \in \bigcup_{n=0}^{\infty} \mc F_n$, and use the $\pi-\lambda$ theorem.

Take $B \in F_n$. We know that $E(X; B) = E(X_n; B)$ since $X_n = E(X|\mc F_n)$ and $E(X_n ; B) = E(X_m ; B) \to E(Y ; B)$ since $X_m \to Y$ in $L^1$.  
\end{proof}
\subsection{Optimal Stopping Theorems}
We know that $E(X_0) = E(X_{n \wedge \tau})$ since it is a martingale.  We want to find conditions so that $E(X_0) = E(X_\tau)$.
\begin{lemma} If $X_n$ is UI and $\tau$ is a stopping time, then $\overline{X_n} = X_{n \wedge \tau}$ is UI.
\end{lemma}
\begin{proof}
We start with 
$$E(|\overline{X_n}|1_{|\overline{X_n}| > M}) = E(|X_n|1_{|\overline{X_n}| > m}1_{\tau > n}) + E(|X_\tau|1_{|\overline{X_n}| > m}).$$
Observe that $|\overline{X_{n}}|$ is a submartingale so $E|\overline{X_n}| \le E|X_n|$ since $X_n$ is UI.  Then 
$$\sup E(|\overline{X_n}|) \le \sup E(|X_n|) < \infty,$$
so it follows that $|\overline{X_n}| \to |X|$.  It follows that both sets in above are sets of small probability.
\end{proof}

From the UI, we can show that $E(X_{n \wedge \tau}) \to E(X_\tau)$ and $E(X_{n \wedge \tau}) = E(X_0)$, so $E(X_\tau) = E(X_0)$, as desired.

Other conditions where OST holds:
\begin{itemize}
\item $X_n$ is a martingale sequence and $X_n - X_{n-1}$ have the property that $|X_n - X_{n-1}| \le B$ almost surely, and $E(\tau) < \infty$.  
\end{itemize}
\pagebreak
\section{November 17th, 2020}
\subsection{More Optimal Stopping Conditions}

Other conditions where OST holds:
\begin{itemize}
\item $X_n$ is a martingale sequence and $X_n - X_{n-1}$ have the property that $|X_n - X_{n-1}| \le B$ almost surely, and $E(\tau) < \infty$.  
\item $X_n$ is a martingale sequence and $X_n - X_{n-1}$ have the property that $E(|X_n - X_{n-1}| |\mc F_{n-1}) \le B$ almost surely, and $E(\tau) < \infty$.  
\begin{proof}
Suppose $X_0 = 0$. We note that $X_n = \sum_{i=1}^n X_i - X_{i-1}$, so $$X_{n \wedge \tau} = \sum_{i=1}^\tau X_{i} = X+{i-1} = \sum (X_i - X_{i-1}) 1(\tau > i-1).$$

Finally, $|X_{n \wedge \tau}| \le \sum |X_i - X_{i-1}|1_{\tau >i -1} = A$ and it follows that 
$$\sum E(|X_i - X_{i-1}|1_{\tau > i-1}) = \sum E(E(|X_i - X_{i-1}|1_{\tau > i-1}|F_{i-1})) \le \sum E(1_{t > i-1} B) = BE\tau,$$
\end{proof}
\end{itemize}

\begin{remark} The amount of information is somehow "encoded" in the moment of the martingale you take.  
\end{remark}
\begin{corollary} Let $\tau = \tau_0$ and consider $S^* = \max_{i < \tau} S_i$.  Then $ES^* = \infty$.
\end{corollary}
\begin{proof}
We can compute 
$$P(S^* \ge n) = P_1(\tau_x < \tau_0) = \frac{1}{x-1 + 1} = \frac{1}{x}.$$
\end{proof}
\subsection{Reverse Martingales}
For standard $X_0, X_1, \dots$ and a filtration $\mc F_n$ so that $E(X_n|\mc F_{n-1}) = X_{n-1}$ almost surely.
\begin{definition} We have $F_0 \supset F_1 \supset F_2 \dots$ and 
$$E(X_i|F_{i+1}) = X_{i+1}$$
almost surely.  We could also write it this as $E(X_0|F_i)$ for a decreasing sequence of $\Sigma$ algebras $F_0 \supset F_1 \supset \dots$.
\end{definition}

\begin{lemma} If $X_i$ is a reverse martingale, then there exists $X$ with $X_i \to X$ almost surely, $X_i \to X$ in $L^1$ and $X = E(X_0|F_{\infty})$ where $F_\infty = \bigcap F_i$.
\end{lemma}
\begin{proof}
The proof in the forward direction used the upcrossing lemma: for any $q_1 < q_2$ rational, $\lim E(N(q_1, q_2, n)) < \infty$ if $\sup EX_n^+ < \infty$.

Fixing some $n$ and going from $n \to 0$ gives a forward martingale.  Then if $N'(q_1, q_2, n)$ is the crossing number, 
$$E(N'(q_1, q_2, n)) \le \frac{E(|X_0| + q_1)}{q_2 - q_1}.$$
Hence, $\lim E(N'(q_1, q_2, n)) < \infty$ almost surely for all $q_1 < q_2 \in \Q$.  It follows that $X_i$ has a limit almost surely, which we will call $X$.  For $L^1$ convergence, it suffices to show UI.  Each $X_i = E(X_0|F_i)$, so it follows that this is uniformly integrable.  

Finally, $X = \lim X_i$ and hence $\cap F_i$ measurable.  Take $B \in \mc F_\infty$.  It suffices to show that 
$$\int_B X = \int_B X_0,$$
which follows from the fact that $B \in F_\infty \subset F_i$ for all $i$ and $E(X_i;B) \to E(X;B)$.
\end{proof}
\subsection{Applications of Reverse Martingales}
\begin{definition} Let $\mathcal E_n = \Sigma (A: A\text{ is invariant under permutation of the first n coordinates})$.  This means that for $\omega \in A$, we have $\sigma \omega \in A$ for $\sigma \in S_n$. Then $\mathcal E = \bigcap \mc E_n$. 
\end{definition}
\begin{thm}[Hewitt-Savage 0-1 Law] If $X_0, X_1, X_2, \dots$ iid and $A\in \mc E$ then $P(A) \in \{0, 1\}.$
\end{thm}
\begin{proof}
We want to show that $\mc E$ is independent of itself.  Fix some $K$ and some bounded function $\phi: \R^k \to \R$.  We show this is independent of $\mc E$.  This would imply that $\mc E$ is independent of $\mc E$ since we can take $\phi$ as indicators so that $\Sigma(X_1, X_2, \dots, X_k)$ is independent of $\mc E$.  Then $\bigcup_{k=1}^{\infty} \Sigma(X_1, \dots, X_k)$ is independent of $\mc E$, which is a $\pi$-system, so it follows that 
$\Sigma(X_1, X_2, \dots)$ is independent of $\mc E$.  But $\mc E$ is a subsigma algebra of the former.  

The first step is to symmetrize $\phi(x_1, x_2, \dots, x_k)$: define 
$$A_n(\phi) = \frac{1}{(n)_k}\sum_{i} \phi(X_{i_1}, X_{i_2}, \dots, X_{i, k}).$$
Note that $A_n(\phi)$ is $S_n$ invariant.  Furthermore, $A_n(\phi)$ is $\mc E_n$ measurable and 
$$A_n\phi = E(A_n\phi|\mc E_n) = \frac{1}{(n)_k} \sum E(\phi(X_{i_1}, \dots, X_{i_k})| \mc E_n).$$
Since $\mc E_n$ is $S_n$ invariant, it follows that 
$$E(\phi(X_{i_1}, X_{i_2}, \dots, X_{i_k}) | \mc E_n) = E(\phi(X_1, X_2, \dots, X_k) | \mc E_n).$$
Hence, we have that 
$$A_n\phi = E(\phi(X_1, X_2, \dots, X_k)| \mc E_n).$$

This is a reverse martingale since $\mc E_n \downarrow E_n$.  Hence 
$$A_n\phi \to E(\phi(X_1, X_2, \dots, X_k) | \mc E).$$

It suffices to show that $E(\phi(X_1, X_2, \dots, X_k)| \mc E) = E(\phi(X_1, X_2, \dots, X_k)).$
\begin{lemma} If $EX^2 < \infty$, $E(X|\mc G) \in \mc F$ with $X$ independent of $\mc F$, then $E(X|\mc G) = E(X)$.
\end{lemma}
\begin{proof}
$$E(E(X|\mc G)X) = E(E(X|\mc G)^2) = E(X^2).$$
It follows that 
$$E(E(X|\mc G)^2) = E(X^2),$$
so Jensen's inequality is sharp for the convex function $x \mapsto x^2$.
\end{proof}

We know that $A_n\phi \to E(\phi(X_1, X_2, \dots, X_k)| \mc E).$  We prove that $A_n\phi$ is independent of $X_1$.  All the terms in $A_n\phi$ not containing $X_1$ is independent of $X_1$ and is measurable with respect to $\Sigma(X_2, \dots)$.  Then $A_n(\phi)= o(1/n) + A_n'(\phi)$ independent of $X_1$, and the error goes to $0$.  Repeating this for the other $k$ terms gives the result.  
\end{proof}
\pagebreak
\section{November 19th, 2020}
\subsection{de Finetti's Theorem}

\begin{definition}A sequence $X_1, X_2, \dots$  is exchangeable if for each $n$ and $\pi \in S_n$ $$(X_1, X_2, \dots, X_n) = (X_{\pi(1)}, X_{\pi(2)}, \dots, X_{\pi(n)})$$
in distribution.
\end{definition}

\begin{example} IID sequences are obviously exchangable.  Another example: fix a random $\theta \in [0, 1]$
 uniform and consider $Ber(\theta)$, iid. Each $X_i \sim Ber(1/2)$.  But $E(X_1, X_2) = E_\theta(E(X_1, X_2 | \theta)) = E(\theta^2) = \frac{1}{3}.$   
\end{example}

\begin{theorem} This is the only example of exchangeable sequences. That is, given $X_1, X_2, \dots$ and the exchangeable sigma algebra $\mc E$,  the distribution of $(X_1, X_2, \dots) $ given $\mc E$ is iid.
\end{theorem}
\begin{proof}
That the distribution of $X_i, X_j$ given $\mc E$ are the same follows that $E(\phi(X_i)| \mc E) = E(\phi(X_j)| \mc E)$ for all $i, j$(by the change of measure formula).  It suffices to show that for any $f, g$, 
$$E(f(X_i)g(X_j)| \mc E) = E(f(X_i)|\mc E) E(g(X_j)| \mc E)$$

If will be enough to show for any $f: \R^k \to \R$ and $g: \R \to \R$, 
$$E(f(X_1, X_2, \dots, X_k)g(X_{k+1})| \mc E) = E(f(X_1, \dots, X_k)|\mc E) E(g(X_{k+1}| \mc E)$$
\end{proof}

\subsection{Symmetric and Asymmetric Random Walks}
Let $\xi_1, \xi_2, \dots$ be iid and let $S_n = S_0 + \sum_{i=1}^n \xi_i$.  Taking $\mc F_n = \sigma(\xi_1, \dots, \xi_n)$, we have some martingales $X_n = S_n - n E\xi_i$ the linear martingale.  For $E\xi = 0$, we have $X_n = S_n^2 - nEX_i^2$, the quadratic martingale.

Consider the Symmetric simple random walk with $P(\xi_i = 1) = P(\xi_i = -1) = 1/2$.  We compute the moment generating function $E(s^{\tau_1})$ for $s < 1$.  We have that $E(\tau_1) = \infty$.  We compute this using exponential martingales.

For $\theta$, let $\phi(\theta) = E(e^{\theta X_1})$ where $X_i \sim Ber(\pm 1)$. This is $\phi(\theta) = \frac{e^{\theta} + e^{\theta}}{2} \ge 1$.  $M_n = \frac{e^{\theta S_n}}{\phi(\theta)^n}$ is a martingale since 
$$E(M_n|\mc F_{n-1}) = \frac{e^{\theta S_{n-1}}}{\phi(\theta)^{n-1}} \frac{e^{\theta X_n} }{\phi(\theta)},$$
so
$$M_n = M_{n-1} \frac{e^{\theta X_n} }{\phi(\theta)}.$$
Consider $\ov{M_n} = M_{n \wedge \tau}$ for $\tau = \tau_1$.  Note that $0 \le \ov M_n \le e^{\theta}$.  We know that $E1 = (\ov M_0) = E(\ov M_n)$ and by BCT, $E(\ov M_n) = E(M_\tau).$ Therefore, $$E(M_\tau) = 1 = E^{\theta}E(\phi(\theta)^{-\tau}).$$

Now, we set $s = \phi(\theta)^{-1}$. Letting $x = e^{theta}$ and doing some algebra, we find that $x^2 + 1 - 2/s x = 0$, so it follows that 
$$x = \frac{2 \pm \sqrt{4 - 4s^2}}{2s} = \frac{1 \pm \sqrt{1-s^2}}{s}.$$

It follows that $$E(s^{\tau}) = \frac{1}{x} = \frac{1 - \sqrt{1 - s^2}}{s}.$$
\pagebreak
\section{November 24th, 2020}
\subsection{Random Walks, Continued}
For the simple random walk, we found the associated hitting time between $a, b$ using the martingale $S_n = \sum_{i=1}^n x_i$.

In the asymmetric case, the closest martingale is $S_n - (2p-1)n$.  The idea is to consider exponential martingales:  Let $M_n = \left (\frac{1-p}{p}\right )^{S_n}$.  It is easy to check that $M_n$ is a martingale.  Consider the stopping time for $\tau = \tau_{\{-a, b\}}.$  The optional stopping theorem(if applicable) is $E(M_\tau) = E(M_0)$.  Notably, we can find similar expressions such as 
$$P(\tau_{-a} < \tau_b).$$
If we let $\phi(y) = [(1-p)/p]^y$, then $\phi(0) = \phi(b)(1-q) + \phi(-a) q$, so it follows that 
$$q = P(\tau_{-a} < \tau_b) = \frac{\phi(b) - \phi(0)}{\phi(b) - \phi(-a)}.$$

Finally, note that the optimal stopping theorem is applicable since $\tau \wedge n$ is a bounded stopping time and $EM_\tau \le EM_{\tau\wedge n}$, we can apply BCT.

\begin{proposition} $P(\tau_b < \infty) = 1$, $P(\tau_{-a} < \infty) < 1$.
\end{proposition}
\begin{proof}
Note that 
$$1-q = \frac{\phi(0) - \phi(-a)}{\phi(b) - \phi(-a)} = \frac{1 - ((1-p)/p)^{-a}}{((1-p)/p)^b - ((1-p)/p)^{-a}}.$$
Then $$P(\tau_b < \infty) = \lim_{a \to \infty} P(\tau_b < \tau_{-a}) = 1.$$

For the other conclusion, we send $b \to \infty$, and $\phi(b) \to 0$, so
$$P(\tau_{-a} = \infty) = 1 - \left (\frac{1-p}{p}\right)^{-a}.$$

To find $E(\tau_b)$, note that $S_n - (2p-1)n$ is a martingale.  By OST, $0 = E(S_{\tau \wedge n}) - (2p-1) E(\tau \wedge n)$, so it follows that 
$$E(\tau \wedge n) = \frac{1}{2p-1}E(S_{\tau \wedge n}) \le \frac{b}{2p-1}.$$  

By MCT, $\tau \wedge n \uparrow \tau$, so it follows that $E(\tau \wedge n) \uparrow E(\tau)\le \frac{b}{2p-1}$, if we apply OST.  

We can apply DCT since 
$$|S^{\tau \wedge n} | \le b + |\inf S_n|$$
and $P(\inf S_n \le -a) = P(\tau_{-a} < \infty) = \frac{(1-p)^a}{p^a}.$
Hence,
$$E(|\inf S_n|) \le \sum_{a=1}^{\infty} P(\tau_{-a} < \infty) = \sum_{a=1}^{\infty} \left (\frac{1-p}{p}\right)^a  = \frac{1}{1 - \left (\frac{1-p}{p}\right )^a}.$$
\end{proof}
\subsection{Combinatorics of SRW}
We consider simple random walks.  

\begin{thm}[Reflection Principle] Let $x, y > 0$.  The number of paths from $(0, x)$ to $(n, y)$ that are $0$ at some time is equal to the number of paths from $(0, -x)$ to $(n, y)$.
\end{thm}
For any path as above, reflecting the path about the x-axis until the first hitting time of $0$ creates a bijection to the set of paths from $(0, -x) \to (n, y)$.  The inverse map is exactly reflecting back.

\begin{thm}[Ballot Theorem] Suppose $A$ and $B$ get $\alpha, \beta$ votes each with $\alpha > \beta$.  The probability that throughout the vote counting process $A$ is ahead of $B$ is $\frac{\alpha - \beta}{\alpha + \beta}$.
\end{thm}
\begin{proof}
Let $V_t(A)$ be the number of votes $A$ has got after the first $t$ votes.  Similarly for $B$.  Note that $V_t(A) + V_t(B) = t$.  We want to consider $V_t(A) - V_t(B)$, specifically the paths that never hit the $x$-axis.  

Let $n = \alpha + \beta$, $x = \alpha - \beta$.  Let $N_{k, r}$ be the number of paths from $0$ to $r$ in $k$ steps.  

We claim that the number of desired paths is 
$$N_{n-1, x-1} - N_{n-1, x-1} = \frac{\alpha - \beta}{\alpha + \beta}$$
\begin{exercise} Prove this result using the reflection principle.
\end{exercise}
\end{proof}
\begin{lemma} $P(S_1 \ne 0, S_2 \ne 0, \dots, S_{2n} \ne 0) = P(S_{2n} = 0)$.  
\end{lemma}
\begin{proof}
$$P(S_1 > 0, \dots, S_{2n} > 0) = \sum_{r=1}^{\infty} P(S_1 > 0, \dots, S_{2n-1} > 0, S_{2n} = 2r).$$

The probability is exactly
$$\frac{N_{2n-1, 2r-1} - N_{2n-1, 2r+1}}{2n-1}.$$
If we let $p_{n, x} = P(S_n = x)$, then
$$\sum_{r=1}^{\infty} P(S_1 > 0, \dots, S_{2n-1} > 0, S_{2n} = 2r) = \frac{1}{2}\sum_{r=1}^{\infty} (p_{2n-1, 2r-1} - p_{2n-1,2r+1 }) = \frac{1}{2}p_{2n-1, 1}= \frac{1}{2}P(S_{2n} = 0).$$
By symmetry, the probability each are less than $0$ is $\frac{1}{2}P(S_{2n} = 0)$, so it follows that the probability is $P(S_{2n} = 0)$, as desired.

Then, 
$$P(S_{2n} = 0) = \binom{2n}{n} \frac{1}{2^{2n}}$$
and using the Stirling Approximation it follows that 
$$P(S_{2n} = 0) \sim \frac{1}{\sqrt{n}}.$$


\end{proof}
\begin{lemma} $P(\tau = 2n) = P(\tau > 2n-2) - P(\tau > 2n) = P(S_{2n-2}) - P(S_{2n} = 0) \sim n^{-3/2}$, plugging in the result from above.
\end{lemma}

\begin{theorem}[Arcsin law]
Let $\pi_{2n}$ be the number of edges that are above the $x$-axis.  Let $u_m = P(S_{m} = 0)$.  Then $P(\pi_{2n} = 2k) = u_{2k} u_{2n-2k}.$
\end{theorem}

\pagebreak
\section{December 1st, 2020}
\subsection{Combinatorics of Simple Random Walks}

\end{document}


